==================== COMMIT: 5807be46770880fc2b1c6dfff9925e60bb8ef1f0 ====================
commit 5807be46770880fc2b1c6dfff9925e60bb8ef1f0
Author: jsboigeEpita <jean-sylvain.boige@epita.fr>
Date:   Mon Jun 16 03:09:04 2025 +0200

    Refactor: Revert imports from missing semantic_kernel_compatibility to semantic_kernel

diff --git a/argumentation_analysis/agents/core/logic/fol_logic_agent.py b/argumentation_analysis/agents/core/logic/fol_logic_agent.py
index a6e8a223..4b551894 100644
--- a/argumentation_analysis/agents/core/logic/fol_logic_agent.py
+++ b/argumentation_analysis/agents/core/logic/fol_logic_agent.py
@@ -31,6 +31,7 @@ from pydantic import Field
 
 # Import de la classe Agent de base depuis l'orchestrateur principal
 # et définition locale de ChatCompletionAgent héritant de celle-ci.
+from semantic_kernel.agents import ChatCompletionAgent
 from semantic_kernel.contents import ChatMessageContent
 
 from argumentation_analysis.agents.core.abc.agent_bases import BaseLogicAgent
diff --git a/argumentation_analysis/orchestration/analysis_runner.py b/argumentation_analysis/orchestration/analysis_runner.py
index c357be94..d4739621 100644
--- a/argumentation_analysis/orchestration/analysis_runner.py
+++ b/argumentation_analysis/orchestration/analysis_runner.py
@@ -31,6 +31,8 @@ from semantic_kernel.kernel import Kernel as SKernel # Alias pour éviter confli
 import semantic_kernel as sk
 from semantic_kernel.contents import ChatMessageContent, ChatRole as AuthorRole
 # CORRECTIF COMPATIBILITÉ: Utilisation du module de compatibilité
+from semantic_kernel.agents import AgentGroupChat, ChatCompletionAgent, Agent
+from semantic_kernel.exceptions import AgentChatException
 from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion, AzureChatCompletion # Pour type hint
 from semantic_kernel.connectors.ai.function_choice_behavior import FunctionChoiceBehavior
 from semantic_kernel.functions.kernel_arguments import KernelArguments
diff --git a/argumentation_analysis/orchestration/enhanced_pm_analysis_runner.py b/argumentation_analysis/orchestration/enhanced_pm_analysis_runner.py
index 841d58d3..497f6678 100644
--- a/argumentation_analysis/orchestration/enhanced_pm_analysis_runner.py
+++ b/argumentation_analysis/orchestration/enhanced_pm_analysis_runner.py
@@ -35,45 +35,8 @@ if project_root not in sys.path:
 # Imports Semantic Kernel
 import semantic_kernel as sk
 from semantic_kernel.contents import ChatMessageContent, ChatRole as AuthorRole
-# Imports de compatibilité remplacés par des définitions locales ou des imports de base
-from argumentation_analysis.agents.core.abc.agent_bases import BaseAgent as Agent
-from semantic_kernel.agents import Agent as SKAgent  # Importation pour l'héritage
-
-class ChatCompletionAgent(SKAgent):
-    """Fallback class for ChatCompletionAgent."""
-    def __init__(self, kernel, service, name, instructions, arguments):
-        # Cette classe doit être revue pour utiliser la nouvelle API SK,
-        # pour l'instant elle sert de placeholder.
-        super().__init__(
-            kernel=kernel,
-            id=name, # Utiliser id au lieu de name
-            description=instructions,
-            # Les arguments ne sont plus passés comme ça, à revoir
-        )
-
-class AgentGroupChat:
-    """Fallback class for AgentGroupChat."""
-    def __init__(self, agents, selection_strategy, termination_strategy):
-        self.agents = agents
-        self.selection_strategy = selection_strategy
-        self.termination_strategy = termination_strategy
-        self.history = []
-
-    async def invoke(self):
-        # Simulation de la logique d'invocation
-        for i in range(10): # Limite de sécurité
-            next_agent = await self.selection_strategy.next(self.agents, self.history)
-            if await self.termination_strategy.should_terminate(next_agent, self.history):
-                break
-            
-            # Message factice
-            message = ChatMessageContent(role=AuthorRole.ASSISTANT, content=f"Message de {next_agent.name}", name=next_agent.name)
-            self.history.append(message)
-            yield message
-
-class AgentChatException(Exception):
-    """Custom exception for chat errors."""
-    pass
+from semantic_kernel.agents import AgentGroupChat, ChatCompletionAgent, Agent
+from semantic_kernel.exceptions import AgentChatException
 from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion, AzureChatCompletion
 from semantic_kernel.functions.kernel_arguments import KernelArguments
 
diff --git a/examples/logique_complexe_demo/demo_einstein_workflow.py b/examples/logique_complexe_demo/demo_einstein_workflow.py
index 777923ef..4536959d 100644
--- a/examples/logique_complexe_demo/demo_einstein_workflow.py
+++ b/examples/logique_complexe_demo/demo_einstein_workflow.py
@@ -22,7 +22,7 @@ sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
 
 from semantic_kernel import Kernel
 from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion
-from semantic_kernel_compatibility import ChatCompletionAgent
+from semantic_kernel.agents import ChatCompletionAgent
 
 # Import des composants spécialisés pour l'énigme complexe
 from argumentation_analysis.orchestration.logique_complexe_orchestrator import LogiqueComplexeOrchestrator
diff --git a/examples/logique_complexe_demo/test_einstein_simple.py b/examples/logique_complexe_demo/test_einstein_simple.py
index f19e148d..b4d33fea 100644
--- a/examples/logique_complexe_demo/test_einstein_simple.py
+++ b/examples/logique_complexe_demo/test_einstein_simple.py
@@ -16,7 +16,7 @@ sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
 
 from semantic_kernel import Kernel
 from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion
-from semantic_kernel_compatibility import ChatCompletionAgent
+from semantic_kernel.agents import ChatCompletionAgent
 
 # Import des composants spécialisés pour l'énigme complexe
 from argumentation_analysis.orchestration.logique_complexe_orchestrator import LogiqueComplexeOrchestrator
diff --git a/tests/unit/argumentation_analysis/test_agent_interaction.py b/tests/unit/argumentation_analysis/test_agent_interaction.py
index 8c728334..e1a1aa5d 100644
--- a/tests/unit/argumentation_analysis/test_agent_interaction.py
+++ b/tests/unit/argumentation_analysis/test_agent_interaction.py
@@ -12,14 +12,7 @@ import pytest
 from unittest.mock import MagicMock, AsyncMock, patch
 import semantic_kernel as sk
 from semantic_kernel.contents import ChatMessageContent, ChatRole as AuthorRole
-# Fallback classes for compatibility after removing semantic_kernel_compatibility
-class Agent:
-    def __init__(self, name=""):
-        self.name = name
-
-class AgentGroupChat:
-    def __init__(self, *args, **kwargs):
-        pass
+from semantic_kernel.agents import Agent, AgentGroupChat
 
 # Utiliser la fonction setup_import_paths pour résoudre les problèmes d'imports relatifs
 # from tests import setup_import_paths # Commenté pour investigation
diff --git a/tests/unit/argumentation_analysis/test_integration_balanced_strategy.py b/tests/unit/argumentation_analysis/test_integration_balanced_strategy.py
index c2ff9a98..c08e45fe 100644
--- a/tests/unit/argumentation_analysis/test_integration_balanced_strategy.py
+++ b/tests/unit/argumentation_analysis/test_integration_balanced_strategy.py
@@ -15,16 +15,7 @@ import logging
 
 import semantic_kernel as sk
 from semantic_kernel.contents import ChatMessageContent, ChatRole as AuthorRole
-# Fallback classes for compatibility after removing semantic_kernel_compatibility
-class Agent:
-    def __init__(self, name=""):
-        self.name = name
-
-class AgentGroupChat:
-    def __init__(self, *args, **kwargs):
-        pass
-        
-from argumentation_analysis.agents.core.abc.agent_bases import BaseAgent
+from semantic_kernel.agents import Agent, AgentGroupChat
 
 # Utiliser la fonction setup_import_paths pour résoudre les problèmes d'imports relatifs
 # from tests import setup_import_paths # Commenté pour investigation
diff --git a/tests/unit/argumentation_analysis/test_integration_end_to_end.py b/tests/unit/argumentation_analysis/test_integration_end_to_end.py
index ba9913a1..755e5ec4 100644
--- a/tests/unit/argumentation_analysis/test_integration_end_to_end.py
+++ b/tests/unit/argumentation_analysis/test_integration_end_to_end.py
@@ -16,14 +16,7 @@ from unittest.mock import MagicMock, AsyncMock, patch
 
 import semantic_kernel as sk
 from semantic_kernel.contents import ChatMessageContent, ChatRole as AuthorRole
-# Fallback classes for compatibility after removing semantic_kernel_compatibility
-class Agent:
-    def __init__(self, name=""):
-        self.name = name
-
-class AgentGroupChat:
-    def __init__(self, *args, **kwargs):
-        pass
+from semantic_kernel.agents import Agent, AgentGroupChat
 
 from argumentation_analysis.core.shared_state import RhetoricalAnalysisState
 from argumentation_analysis.core.state_manager_plugin import StateManagerPlugin
diff --git a/tests/unit/argumentation_analysis/test_strategies.py b/tests/unit/argumentation_analysis/test_strategies.py
index 09e6eb4f..941f6c11 100644
--- a/tests/unit/argumentation_analysis/test_strategies.py
+++ b/tests/unit/argumentation_analysis/test_strategies.py
@@ -6,6 +6,7 @@ Tests unitaires pour le module strategies.
 import unittest
 from unittest.mock import MagicMock, AsyncMock, patch
 import asyncio
+from semantic_kernel.agents import Agent
 from semantic_kernel.contents import ChatMessageContent, ChatRole as AuthorRole
 from argumentation_analysis.core.strategies import SimpleTerminationStrategy, DelegatingSelectionStrategy, BalancedParticipationStrategy
 from argumentation_analysis.core.shared_state import RhetoricalAnalysisState
@@ -14,10 +15,6 @@ from argumentation_analysis.core.shared_state import RhetoricalAnalysisState
 
 import pytest # Ajout de pytest pour les fixtures
 
-# Classe factice pour remplacer semantic_kernel.agents.Agent qui n'est plus disponible
-class Agent:
-    pass
-
 class TestSimpleTerminationStrategy:
     """Tests pour la classe SimpleTerminationStrategy."""
 

==================== COMMIT: 5c155985c38a1e53c39df345eda5b227d36fda5f ====================
commit 5c155985c38a1e53c39df345eda5b227d36fda5f
Author: jsboigeEpita <jean-sylvain.boige@epita.fr>
Date:   Mon Jun 16 03:00:14 2025 +0200

    refactor(cleanup): Remove semantic_kernel_compatibility layer and obsolete conftest
    
    Supprime complètement la couche de compatibilité pour semantic-kernel v0.9.6, qui était une source d'erreurs et de dette technique. Corrige tous les imports dans le code source et les tests pour utiliser les nouvelles APIs ou des fallbacks locaux si nécessaire. Supprime également les fichiers de test obsolètes et le conftest.py racine qui entraient en conflit avec l'architecture de test modulaire.

diff --git a/argumentation_analysis/agents/core/logic/fol_logic_agent.py b/argumentation_analysis/agents/core/logic/fol_logic_agent.py
index 7f3bd93d..a6e8a223 100644
--- a/argumentation_analysis/agents/core/logic/fol_logic_agent.py
+++ b/argumentation_analysis/agents/core/logic/fol_logic_agent.py
@@ -31,7 +31,6 @@ from pydantic import Field
 
 # Import de la classe Agent de base depuis l'orchestrateur principal
 # et définition locale de ChatCompletionAgent héritant de celle-ci.
-from semantic_kernel_compatibility import ChatCompletionAgent
 from semantic_kernel.contents import ChatMessageContent
 
 from argumentation_analysis.agents.core.abc.agent_bases import BaseLogicAgent
diff --git a/argumentation_analysis/core/strategies.py b/argumentation_analysis/core/strategies.py
index 6fed32b9..3779d4bf 100644
--- a/argumentation_analysis/core/strategies.py
+++ b/argumentation_analysis/core/strategies.py
@@ -1,6 +1,5 @@
 ﻿# core/strategies.py
 # CORRECTIF COMPATIBILITÉ: Utilisation du module de compatibilité
-from argumentation_analysis.utils.semantic_kernel_compatibility import Agent, TerminationStrategy, SelectionStrategy
 from semantic_kernel.contents import ChatMessageContent, ChatRole as AuthorRole
 from typing import List, Dict, TYPE_CHECKING
 import logging
diff --git a/argumentation_analysis/orchestration/analysis_runner.py b/argumentation_analysis/orchestration/analysis_runner.py
index 1468f20c..c357be94 100644
--- a/argumentation_analysis/orchestration/analysis_runner.py
+++ b/argumentation_analysis/orchestration/analysis_runner.py
@@ -31,8 +31,6 @@ from semantic_kernel.kernel import Kernel as SKernel # Alias pour éviter confli
 import semantic_kernel as sk
 from semantic_kernel.contents import ChatMessageContent, ChatRole as AuthorRole
 # CORRECTIF COMPATIBILITÉ: Utilisation du module de compatibilité
-from argumentation_analysis.utils.semantic_kernel_compatibility import AgentGroupChat, ChatCompletionAgent, Agent
-from semantic_kernel_compatibility import AgentChatException
 from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion, AzureChatCompletion # Pour type hint
 from semantic_kernel.connectors.ai.function_choice_behavior import FunctionChoiceBehavior
 from semantic_kernel.functions.kernel_arguments import KernelArguments
diff --git a/argumentation_analysis/orchestration/cluedo_extended_orchestrator.py b/argumentation_analysis/orchestration/cluedo_extended_orchestrator.py
index b85513cd..4577aa09 100644
--- a/argumentation_analysis/orchestration/cluedo_extended_orchestrator.py
+++ b/argumentation_analysis/orchestration/cluedo_extended_orchestrator.py
@@ -14,34 +14,16 @@ from datetime import datetime
 import semantic_kernel as sk
 from semantic_kernel.functions import kernel_function
 from semantic_kernel.kernel import Kernel
-# Les modules agents ne sont pas disponibles dans cette version de semantic_kernel
-# Utilisation de fallbacks pour compatibilité
-try:
-    from semantic_kernel_compatibility import Agent, AgentGroupChat
-    from semantic_kernel.agents.strategies.selection.selection_strategy import SelectionStrategy
-    from semantic_kernel.agents.strategies.termination.termination_strategy import TerminationStrategy
-    AGENTS_AVAILABLE = True
-except ImportError:
-    # Fallbacks pour compatibilité
-    class Agent:
-        def __init__(self, name: str, kernel: Kernel = None, **kwargs):
-            self.name = name
-            self.kernel = kernel
-            
-    class AgentGroupChat:
-        def __init__(self, agents: List[Agent] = None, **kwargs):
-            self.agents = agents or []
-            
-    class SelectionStrategy:
-        def select_next_agent(self, agents: List[Agent], last_agent: Agent = None) -> Agent:
-            return agents[0] if agents else None
-            
-    class TerminationStrategy:
-        def should_terminate(self, messages: List[Any]) -> bool:
-            return False
-            
-    AGENTS_AVAILABLE = False
+from argumentation_analysis.orchestration.base import SelectionStrategy, TerminationStrategy
+from argumentation_analysis.agents.core.abc.agent_bases import BaseAgent as Agent
 from semantic_kernel.contents.chat_message_content import ChatMessageContent
+
+class AgentGroupChat:
+    """Fallback class for compatibility."""
+    def __init__(self, agents: List[Agent] = None, **kwargs):
+        self.agents = agents or []
+        
+AGENTS_AVAILABLE = True
 from semantic_kernel.functions.kernel_arguments import KernelArguments
 
 # Import conditionnel pour les modules filters qui peuvent ne pas exister
diff --git a/argumentation_analysis/orchestration/enhanced_pm_analysis_runner.py b/argumentation_analysis/orchestration/enhanced_pm_analysis_runner.py
index 76932110..841d58d3 100644
--- a/argumentation_analysis/orchestration/enhanced_pm_analysis_runner.py
+++ b/argumentation_analysis/orchestration/enhanced_pm_analysis_runner.py
@@ -35,8 +35,45 @@ if project_root not in sys.path:
 # Imports Semantic Kernel
 import semantic_kernel as sk
 from semantic_kernel.contents import ChatMessageContent, ChatRole as AuthorRole
-from semantic_kernel_compatibility import AgentGroupChat, ChatCompletionAgent, Agent
-from semantic_kernel_compatibility import AgentChatException
+# Imports de compatibilité remplacés par des définitions locales ou des imports de base
+from argumentation_analysis.agents.core.abc.agent_bases import BaseAgent as Agent
+from semantic_kernel.agents import Agent as SKAgent  # Importation pour l'héritage
+
+class ChatCompletionAgent(SKAgent):
+    """Fallback class for ChatCompletionAgent."""
+    def __init__(self, kernel, service, name, instructions, arguments):
+        # Cette classe doit être revue pour utiliser la nouvelle API SK,
+        # pour l'instant elle sert de placeholder.
+        super().__init__(
+            kernel=kernel,
+            id=name, # Utiliser id au lieu de name
+            description=instructions,
+            # Les arguments ne sont plus passés comme ça, à revoir
+        )
+
+class AgentGroupChat:
+    """Fallback class for AgentGroupChat."""
+    def __init__(self, agents, selection_strategy, termination_strategy):
+        self.agents = agents
+        self.selection_strategy = selection_strategy
+        self.termination_strategy = termination_strategy
+        self.history = []
+
+    async def invoke(self):
+        # Simulation de la logique d'invocation
+        for i in range(10): # Limite de sécurité
+            next_agent = await self.selection_strategy.next(self.agents, self.history)
+            if await self.termination_strategy.should_terminate(next_agent, self.history):
+                break
+            
+            # Message factice
+            message = ChatMessageContent(role=AuthorRole.ASSISTANT, content=f"Message de {next_agent.name}", name=next_agent.name)
+            self.history.append(message)
+            yield message
+
+class AgentChatException(Exception):
+    """Custom exception for chat errors."""
+    pass
 from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion, AzureChatCompletion
 from semantic_kernel.functions.kernel_arguments import KernelArguments
 
diff --git a/conftest.py b/conftest.py
deleted file mode 100644
index 936d0dc1..00000000
--- a/conftest.py
+++ /dev/null
@@ -1,177 +0,0 @@
-print("INFO: conftest.py (RACINE): Fichier en cours de lecture par pytest.")
-
-import sys
-import os
-import pytest # Importé plus haut pour être disponible globalement
-import jpype
-import jpype.imports # Assurer que jpype.imports est disponible
-
-# Ajout précoce du chemin pour trouver argumentation_analysis
-current_script_dir_for_path = os.path.dirname(os.path.abspath(__file__))
-project_root_for_path = current_script_dir_for_path
-if project_root_for_path not in sys.path:
-    sys.path.insert(0, str(project_root_for_path))
-    print(f"INFO: conftest.py (RACINE): Ajout de {project_root_for_path} à sys.path.")
-else:
-    print(f"INFO: conftest.py (RACINE): {project_root_for_path} déjà dans sys.path.")
-
-initialize_jvm_func = None
-try:
-    from argumentation_analysis.core.jvm_setup import initialize_jvm as init_jvm_actual
-    initialize_jvm_func = init_jvm_actual
-    print("INFO: conftest.py (RACINE): Import de 'initialize_jvm' réussi.")
-except Exception as e_import_jvm_setup:
-    print(f"ERREUR CRITIQUE: conftest.py (RACINE): Échec de l'import de 'initialize_jvm': {e_import_jvm_setup}")
-
-USE_REAL_JVM = True
-jpype_real_jvm_initialized_value = "0"
-
-if initialize_jvm_func is None:
-    print("ERREUR CRITIQUE: conftest.py (RACINE): initialize_jvm_func non disponible. La JVM ne sera pas démarrée par ce conftest.")
-elif USE_REAL_JVM:
-    print("INFO: conftest.py (RACINE): Tentative d'initialisation de la VRAIE JVM...")
-    try:
-        if initialize_jvm_func():
-            print("INFO: conftest.py (RACINE): VRAIE JVM initialisée avec succès par initialize_jvm_func().")
-            jpype_real_jvm_initialized_value = "1"
-        else:
-            print("ERREUR: conftest.py (RACINE): initialize_jvm_func() a retourné False (échec de l'initialisation).")
-    except Exception as e_init_jvm_conftest:
-        print(f"ERREUR CRITIQUE: conftest.py (RACINE) lors de l'appel à initialize_jvm_func: {e_init_jvm_conftest}")
-else:
-    print("INFO: conftest.py (RACINE): USE_REAL_JVM est False. Initialisation de la vraie JVM sautée.")
-
-os.environ["JPYPE_REAL_JVM_INITIALIZED"] = jpype_real_jvm_initialized_value
-print(f"INFO: conftest.py (RACINE): os.environ['JPYPE_REAL_JVM_INITIALIZED'] défini à '{jpype_real_jvm_initialized_value}'.")
-
-print("INFO: conftest.py (RACINE): Initialisation minimale terminée.")
-
-if jpype.isJVMStarted():
-    print("INFO: conftest.py (RACINE): Vérification jpype.isJVMStarted() = True. L'enregistrement des domaines est géré par jvm_setup.py.")
-    jpype.imports.registerDomain("org")
-    print("INFO: conftest.py (RACINE): Domaine 'org' enregistré pour les imports.")
-    pass
-else:
-    print("INFO: conftest.py (RACINE): jpype.isJVMStarted() = False. L'enregistrement des domaines sera géré par jvm_setup.py lors du démarrage.")
-
-@pytest.fixture(scope="module")
-def dung_classes():
-    import jpype
-    print(f"DEBUG: ROOT conftest - dung_classes fixture: jpype.isJVMStarted() = {jpype.isJVMStarted()}")
-    if not jpype.isJVMStarted():
-        pytest.skip("JVM non démarrée, skip dung_classes fixture.")
-    
-    context_class_loader = None
-    try:
-        JavaThread = jpype.JClass("java.lang.Thread")
-        current_thread = JavaThread.currentThread()
-        context_class_loader = current_thread.getContextClassLoader()
-    except Exception as e_cl:
-        print(f"WARNING: ROOT conftest - Erreur obtention ContextClassLoader: {e_cl}")
-    
-    loader_to_use = context_class_loader if context_class_loader else jpype.java.lang.ClassLoader.getSystemClassLoader()
-
-    classes_to_load = {
-        "DungTheory": "org.tweetyproject.arg.dung.syntax.DungTheory",
-        "Argument": "org.tweetyproject.arg.dung.syntax.Argument",
-        "Attack": "org.tweetyproject.arg.dung.syntax.Attack",
-        "PreferredReasoner": "org.tweetyproject.arg.dung.reasoner.SimplePreferredReasoner",
-        "GroundedReasoner": "org.tweetyproject.arg.dung.reasoner.SimpleGroundedReasoner",
-        "CompleteReasoner": "org.tweetyproject.arg.dung.reasoner.SimpleCompleteReasoner",
-        "StableReasoner": "org.tweetyproject.arg.dung.reasoner.SimpleStableReasoner"
-    }
-    loaded_classes = {}
-    try:
-        for name, class_str in classes_to_load.items():
-            print(f"INFO: ROOT conftest - Chargement de {name} ({class_str}) avec loader={loader_to_use}")
-            loaded_classes[name] = jpype.JClass(class_str, loader=loader_to_use)
-        print("INFO: ROOT conftest - dung_classes: Toutes les classes Dung ont été chargées.")
-        return loaded_classes
-    except jpype.JException as e:
-        print(f"ERREUR CRITIQUE JPYPE dans dung_classes: {type(e).__name__}: {e}")
-        if hasattr(e, 'stacktrace') and callable(e.stacktrace): print(f"Stacktrace Java:\n{e.stacktrace()}")
-        pytest.fail(f"Échec JPype dans dung_classes (ROOT conftest): {e}")
-    except Exception as e_gen:
-        print(f"ERREUR CRITIQUE GENERALE dans dung_classes: {type(e_gen).__name__}: {e_gen}")
-        import traceback
-        print(f"Stacktrace Python:\n{traceback.format_exc()}")
-        pytest.fail(f"Erreur générale inattendue dans dung_classes (ROOT conftest): {e_gen}")
-
-
-@pytest.fixture(scope="module")
-def qbf_classes():
-    import jpype
-    if not jpype.isJVMStarted(): pytest.skip("JVM non démarrée")
-    print(f"DEBUG QBF_CLASSES: Classpath: {jpype.getClassPath(True)}") # Ajout du log classpath
-    loader_to_use = jpype.JClass("java.lang.Thread").currentThread().getContextClassLoader() or jpype.java.lang.ClassLoader.getSystemClassLoader()
-    try:
-        return {
-            "QuantifiedBooleanFormula": jpype.JClass("org.tweetyproject.logics.qbf.syntax.QuantifiedBooleanFormula", loader=loader_to_use),
-            "Quantifier": jpype.JClass("org.tweetyproject.logics.qbf.syntax.Quantifier", loader=loader_to_use),
-            "QbfParser": jpype.JClass("org.tweetyproject.logics.qbf.parser.QbfParser", loader=loader_to_use),
-            "Variable": jpype.JClass("org.tweetyproject.logics.commons.syntax.Variable", loader=loader_to_use),
-        }
-    except Exception as e: pytest.fail(f"Erreur chargement qbf_classes: {e}")
-
-@pytest.fixture(scope="module")
-def belief_revision_classes():
-    import jpype
-    if not jpype.isJVMStarted(): pytest.skip("JVM non démarrée")
-    print(f"DEBUG BELIEF_REVISION_CLASSES: Classpath: {jpype.getClassPath(True)}") # Ajout du log classpath
-    loader_to_use = jpype.JClass("java.lang.Thread").currentThread().getContextClassLoader() or jpype.java.lang.ClassLoader.getSystemClassLoader()
-    try:
-        return {
-            "PlFormula": jpype.JClass("org.tweetyproject.logics.pl.syntax.PlFormula", loader=loader_to_use),
-            "PlBeliefSet": jpype.JClass("org.tweetyproject.logics.pl.syntax.PlBeliefSet", loader=loader_to_use),
-            "PlParser": jpype.JClass("org.tweetyproject.logics.pl.parser.PlParser", loader=loader_to_use),
-            # ... (ajouter toutes les autres classes avec org.tweetyproject et loader_to_use)
-            "SimplePlReasoner": jpype.JClass("org.tweetyproject.logics.pl.reasoner.SimplePlReasoner", loader=loader_to_use),
-            "Negation": jpype.JClass("org.tweetyproject.logics.pl.syntax.Negation", loader=loader_to_use),
-            "KernelContractionOperator": jpype.JClass("org.tweetyproject.beliefdynamics.kernels.KernelContractionOperator", loader=loader_to_use),
-            "RandomIncisionFunction": jpype.JClass("org.tweetyproject.beliefdynamics.kernels.RandomIncisionFunction", loader=loader_to_use),
-            "DefaultMultipleBaseExpansionOperator": jpype.JClass("org.tweetyproject.beliefdynamics.DefaultMultipleBaseExpansionOperator", loader=loader_to_use), # Correction du chemin
-            "LeviMultipleBaseRevisionOperator": jpype.JClass("org.tweetyproject.beliefdynamics.LeviMultipleBaseRevisionOperator", loader=loader_to_use), # Correction du chemin
-            "CrMasBeliefSet": jpype.JClass("org.tweetyproject.beliefdynamics.mas.CrMasBeliefSet", loader=loader_to_use),
-            "InformationObject": jpype.JClass("org.tweetyproject.beliefdynamics.mas.InformationObject", loader=loader_to_use),
-            "CrMasRevisionWrapper": jpype.JClass("org.tweetyproject.beliefdynamics.mas.CrMasRevisionWrapper", loader=loader_to_use),
-            "CrMasSimpleRevisionOperator": jpype.JClass("org.tweetyproject.beliefdynamics.operators.CrMasSimpleRevisionOperator", loader=loader_to_use), # Correction du chemin
-            "CrMasArgumentativeRevisionOperator": jpype.JClass("org.tweetyproject.beliefdynamics.operators.CrMasArgumentativeRevisionOperator", loader=loader_to_use), # Correction du chemin
-            "DummyAgent": jpype.JClass("org.tweetyproject.agents.DummyAgent", loader=loader_to_use),
-            "Order": jpype.JClass("org.tweetyproject.commons.util.Order", loader=loader_to_use),
-            "PlSignature": jpype.JClass("org.tweetyproject.logics.pl.syntax.PlSignature", loader=loader_to_use),
-            "ContensionInconsistencyMeasure": jpype.JClass("org.tweetyproject.logics.pl.analysis.ContensionInconsistencyMeasure", loader=loader_to_use),
-            "NaiveMusEnumerator": jpype.JClass("org.tweetyproject.logics.pl.analysis.NaiveMusEnumerator", loader=loader_to_use),
-            "SatSolver": jpype.JClass("org.tweetyproject.logics.pl.sat.SatSolver", loader=loader_to_use),
-            "MaInconsistencyMeasure": jpype.JClass("org.tweetyproject.logics.pl.analysis.MaInconsistencyMeasure", loader=loader_to_use),
-            "McscInconsistencyMeasure": jpype.JClass("org.tweetyproject.logics.pl.analysis.McscInconsistencyMeasure", loader=loader_to_use),
-            "PossibleWorldIterator": jpype.JClass("org.tweetyproject.logics.pl.syntax.PossibleWorldIterator", loader=loader_to_use),
-            "DalalDistance": jpype.JClass("org.tweetyproject.logics.pl.util.DalalDistance", loader=loader_to_use),
-            "DSumInconsistencyMeasure": jpype.JClass("org.tweetyproject.logics.pl.analysis.DSumInconsistencyMeasure", loader=loader_to_use),
-            "DMaxInconsistencyMeasure": jpype.JClass("org.tweetyproject.logics.pl.analysis.DMaxInconsistencyMeasure", loader=loader_to_use),
-            "DHitInconsistencyMeasure": jpype.JClass("org.tweetyproject.logics.pl.analysis.DHitInconsistencyMeasure", loader=loader_to_use),
-            "ProductNorm": jpype.JClass("org.tweetyproject.math.tnorms.ProductNorm", loader=loader_to_use),
-            "FuzzyInconsistencyMeasure": jpype.JClass("org.tweetyproject.logics.pl.analysis.FuzzyInconsistencyMeasure", loader=loader_to_use),
-            "PriorityIncisionFunction": jpype.JClass("org.tweetyproject.beliefdynamics.kernels.PriorityIncisionFunction", loader=loader_to_use),
-        }
-    except Exception as e: pytest.fail(f"Erreur chargement belief_revision_classes: {e}")
-
-@pytest.fixture(scope="module")
-def dialogue_classes():
-    import jpype
-    if not jpype.isJVMStarted(): pytest.skip("JVM non démarrée")
-    print(f"DEBUG DIALOGUE_CLASSES: Classpath: {jpype.getClassPath(True)}") # Ajout du log classpath
-    loader_to_use = jpype.JClass("java.lang.Thread").currentThread().getContextClassLoader() or jpype.java.lang.ClassLoader.getSystemClassLoader()
-    try:
-        return {
-            "ArgumentationAgent": jpype.JClass("org.tweetyproject.agents.dialogues.ArgumentationAgent", loader=loader_to_use),
-            "GroundedAgent": jpype.JClass("org.tweetyproject.agents.dialogues.GroundedAgent", loader=loader_to_use),
-            "OpponentModel": jpype.JClass("org.tweetyproject.agents.dialogues.OpponentModel", loader=loader_to_use),
-            "Dialogue": jpype.JClass("org.tweetyproject.agents.dialogues.Dialogue", loader=loader_to_use),
-            "DialogueTrace": jpype.JClass("org.tweetyproject.agents.dialogues.DialogueTrace", loader=loader_to_use),
-            "DialogueResult": jpype.JClass("org.tweetyproject.agents.dialogues.DialogueResult", loader=loader_to_use),
-            "PersuasionProtocol": jpype.JClass("org.tweetyproject.agents.dialogues.PersuasionProtocol", loader=loader_to_use),
-            "Position": jpype.JClass("org.tweetyproject.agents.dialogues.Position", loader=loader_to_use),
-            "SimpleBeliefSet": jpype.JClass("org.tweetyproject.logics.commons.syntax.SimpleBeliefSet", loader=loader_to_use),
-            "DefaultStrategy": jpype.JClass("org.tweetyproject.agents.dialogues.strategies.DefaultStrategy", loader=loader_to_use),
-        }
-    except Exception as e: pytest.fail(f"Erreur chargement dialogue_classes: {e}")
diff --git a/scripts/diagnostic/test_compatibility_fixes.py b/scripts/diagnostic/test_compatibility_fixes.py
deleted file mode 100644
index dc49d830..00000000
--- a/scripts/diagnostic/test_compatibility_fixes.py
+++ /dev/null
@@ -1,46 +0,0 @@
-import project_core.core_from_scripts.auto_env
-#!/usr/bin/env python3
-"""
-Test de compatibilité pour les corrections d'imports semantic_kernel
-
-Ce script teste que tous les imports problématiques ont été correctement
-corrigés et que le module de compatibilité fonctionne.
-"""
-
-import sys
-import traceback
-from typing import List, Tuple
-
-def test_import(module_path: str, description: str) -> Tuple[bool, str]:
-    """
-    Teste un import et retourne le résultat.
-    
-    Args:
-        module_path: Chemin du module à importer
-        description: Description du test
-        
-    Returns:
-        Tuple (succès, message)
-    """
-    try:
-        exec(f"import {module_path}")
-        return True, f"[OK] {description}: Import réussi"
-    except Exception as e:
-        return False, f"[ERREUR] {description}: {str(e)}"
-
-def test_specific_imports() -> List[Tuple[bool, str]]:
-    """
-    Teste des imports spécifiques depuis les modules corrigés.
-    
-    Returns:
-        Liste des résultats de test
-    """
-    results = []
-    
-    # Test du module de compatibilité
-    try:
-        from semantic_kernel.agents import Agent, ChatCompletionAgent, AgentGroupChat
-        from argumentation_analysis.utils.semantic_kernel_compatibility import SelectionStrategy, TerminationStrategy
-        results.append((True, "[OK] Importations de compatibilité de base"))
-    except ImportError as e:
-        results.append((False, f"[ERREUR] Importations de compatibilité: {e}"))
\ No newline at end of file
diff --git a/tests/phase2_validation/test_authentic_llm_validation.py b/tests/phase2_validation/test_authentic_llm_validation.py
index 48a91b71..ffc13676 100644
--- a/tests/phase2_validation/test_authentic_llm_validation.py
+++ b/tests/phase2_validation/test_authentic_llm_validation.py
@@ -167,27 +167,6 @@ class TestPhase2AuthenticLLMValidation:
         
         logger.info("✅ Validation rejet automatique mock_level != NONE réussie")
 
-    def test_semantic_kernel_compatibility_authentic(self):
-        """Test 6: Validation Semantic Kernel compatibility sans mocks."""
-        logger.info("🔍 Test 6: Validation Semantic Kernel compatibility authentique")
-        
-        # Test AuthorRole
-        role = AuthorRole.USER
-        assert role.value == "user"
-        assert role.name == "user"
-        
-        # Test FunctionChoiceBehavior
-        auto_behavior = FunctionChoiceBehavior.Auto()
-        assert auto_behavior.auto_invoke_kernel_functions is True
-        
-        manual_behavior = FunctionChoiceBehavior.Manual()
-        assert manual_behavior.auto_invoke_kernel_functions is False
-        
-        # Test AgentChatException
-        with pytest.raises(AgentChatException):
-            raise AgentChatException("Test exception")
-        
-        logger.info("✅ Semantic Kernel compatibility layer authentique validé")
 
     @pytest.mark.asyncio
     async def test_informal_agent_authentic_integration(self):
diff --git a/tests/unit/argumentation_analysis/test_agent_interaction.py b/tests/unit/argumentation_analysis/test_agent_interaction.py
index 0fafcbb9..8c728334 100644
--- a/tests/unit/argumentation_analysis/test_agent_interaction.py
+++ b/tests/unit/argumentation_analysis/test_agent_interaction.py
@@ -12,7 +12,14 @@ import pytest
 from unittest.mock import MagicMock, AsyncMock, patch
 import semantic_kernel as sk
 from semantic_kernel.contents import ChatMessageContent, ChatRole as AuthorRole
-from semantic_kernel_compatibility import Agent, AgentGroupChat
+# Fallback classes for compatibility after removing semantic_kernel_compatibility
+class Agent:
+    def __init__(self, name=""):
+        self.name = name
+
+class AgentGroupChat:
+    def __init__(self, *args, **kwargs):
+        pass
 
 # Utiliser la fonction setup_import_paths pour résoudre les problèmes d'imports relatifs
 # from tests import setup_import_paths # Commenté pour investigation
diff --git a/tests/unit/argumentation_analysis/test_integration_balanced_strategy.py b/tests/unit/argumentation_analysis/test_integration_balanced_strategy.py
index 6f9bc087..c2ff9a98 100644
--- a/tests/unit/argumentation_analysis/test_integration_balanced_strategy.py
+++ b/tests/unit/argumentation_analysis/test_integration_balanced_strategy.py
@@ -15,7 +15,16 @@ import logging
 
 import semantic_kernel as sk
 from semantic_kernel.contents import ChatMessageContent, ChatRole as AuthorRole
-from semantic_kernel_compatibility import Agent, AgentGroupChat
+# Fallback classes for compatibility after removing semantic_kernel_compatibility
+class Agent:
+    def __init__(self, name=""):
+        self.name = name
+
+class AgentGroupChat:
+    def __init__(self, *args, **kwargs):
+        pass
+        
+from argumentation_analysis.agents.core.abc.agent_bases import BaseAgent
 
 # Utiliser la fonction setup_import_paths pour résoudre les problèmes d'imports relatifs
 # from tests import setup_import_paths # Commenté pour investigation
diff --git a/tests/unit/argumentation_analysis/test_integration_end_to_end.py b/tests/unit/argumentation_analysis/test_integration_end_to_end.py
index cf0c8c4e..ba9913a1 100644
--- a/tests/unit/argumentation_analysis/test_integration_end_to_end.py
+++ b/tests/unit/argumentation_analysis/test_integration_end_to_end.py
@@ -16,7 +16,14 @@ from unittest.mock import MagicMock, AsyncMock, patch
 
 import semantic_kernel as sk
 from semantic_kernel.contents import ChatMessageContent, ChatRole as AuthorRole
-from semantic_kernel_compatibility import Agent, AgentGroupChat
+# Fallback classes for compatibility after removing semantic_kernel_compatibility
+class Agent:
+    def __init__(self, name=""):
+        self.name = name
+
+class AgentGroupChat:
+    def __init__(self, *args, **kwargs):
+        pass
 
 from argumentation_analysis.core.shared_state import RhetoricalAnalysisState
 from argumentation_analysis.core.state_manager_plugin import StateManagerPlugin
diff --git a/tests/unit/argumentation_analysis/test_strategies.py b/tests/unit/argumentation_analysis/test_strategies.py
index c048ff0d..09e6eb4f 100644
--- a/tests/unit/argumentation_analysis/test_strategies.py
+++ b/tests/unit/argumentation_analysis/test_strategies.py
@@ -6,7 +6,6 @@ Tests unitaires pour le module strategies.
 import unittest
 from unittest.mock import MagicMock, AsyncMock, patch
 import asyncio
-from semantic_kernel_compatibility import Agent
 from semantic_kernel.contents import ChatMessageContent, ChatRole as AuthorRole
 from argumentation_analysis.core.strategies import SimpleTerminationStrategy, DelegatingSelectionStrategy, BalancedParticipationStrategy
 from argumentation_analysis.core.shared_state import RhetoricalAnalysisState
diff --git a/tests/validation_sherlock_watson/test_import.py b/tests/validation_sherlock_watson/test_import.py
deleted file mode 100644
index 6e3952f9..00000000
--- a/tests/validation_sherlock_watson/test_import.py
+++ /dev/null
@@ -1,41 +0,0 @@
-#!/usr/bin/env python3
-try:
-    from semantic_kernel.agents.chat_completion.chat_completion_agent import ChatCompletionAgent
-    print("Import works - ChatCompletionAgent found")
-except ImportError as e:
-    print(f"Import failed: {e}")
-    
-    # Test avec une version alternative
-    try:
-        from semantic_kernel_compatibility import ChatCompletionAgent
-        print("Alternative import works - ChatCompletionAgent found in agents module")
-    except ImportError as e2:
-        print(f"Alternative import failed: {e2}")
-        
-        # Test pour voir ce qui est disponible dans semantic_kernel
-        try:
-            import semantic_kernel
-            print("Available in semantic_kernel:", dir(semantic_kernel))
-            
-            # Essaie de voir si agents existe
-            if hasattr(semantic_kernel, 'agents'):
-                print("Agents module exists:", dir(semantic_kernel.agents))
-            else:
-                print("No agents module found")
-                
-            # Regarde dans les autres agents existants
-            import os
-            script_dir = os.path.dirname(os.path.abspath(__file__))
-            sherlock_path = os.path.join(script_dir, "argumentation_analysis", "agents", "core", "pm", "sherlock_enquete_agent.py")
-            if os.path.exists(sherlock_path):
-                with open(sherlock_path, 'r') as f:
-                    content = f.read()
-                    if "ChatCompletionAgent" in content:
-                        print("ChatCompletionAgent is used in sherlock_enquete_agent.py")
-                        # Extraire l'import line
-                        for line in content.split('\n'):
-                            if 'ChatCompletionAgent' in line and 'import' in line:
-                                print(f"Import line found: {line.strip()}")
-                    
-        except Exception as e3:
-            print(f"Failed to analyze semantic_kernel: {e3}")

==================== COMMIT: 452fe723baf4afd81f6103cb6923ec702fb6eb9e ====================
commit 452fe723baf4afd81f6103cb6923ec702fb6eb9e
Merge: d03f2d32 ebaa5647
Author: jsboigeEpita <jean-sylvain.boige@epita.fr>
Date:   Mon Jun 16 02:52:51 2025 +0200

    Merge branch 'origin/main' into main

diff --cc argumentation_analysis/agents/core/extract/extract_definitions.py
index 47520166,af44925c..aa5a08cc
--- a/argumentation_analysis/agents/core/extract/extract_definitions.py
+++ b/argumentation_analysis/agents/core/extract/extract_definitions.py
@@@ -1,321 -1,443 +1,444 @@@
- """
- Définitions et structures de données pour l'agent d'extraction.
- 
- Ce module contient les classes et structures de données utilisées par l'agent d'extraction
- pour gérer les extraits et leurs métadonnées.
- """
- 
- import re
- import logging
- from pathlib import Path # Ajout de l'import pour Path
- from typing import List, Dict, Any, Tuple, Optional, Union
- 
- # Importer PROJECT_ROOT depuis la configuration centrale
- try:
-     from argumentation_analysis.ui.config import PROJECT_ROOT
- except ImportError:
-     # Fallback si le script est exécuté dans un contexte où l'import direct n'est pas possible
-     PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent.parent.parent
- 
- # Configuration du logging
- logging.basicConfig(
-     level=logging.INFO,
-     format='%(asctime)s [%(levelname)s] [%(name)s] %(message)s',
-     datefmt='%H:%M:%S'
- )
- logger = logging.getLogger("ExtractAgent.Definitions")
- 
- # Création d'un handler pour écrire les logs dans un fichier
- # Définition du chemin du fichier de log
- log_dir = PROJECT_ROOT / "_temp" / "logs"
- log_dir.mkdir(parents=True, exist_ok=True)
- log_file_path = log_dir / "extract_agent.log"
- 
- file_handler = logging.FileHandler(log_file_path, mode='a', encoding='utf-8')
- file_handler.setFormatter(logging.Formatter('%(asctime)s [%(levelname)s] [%(name)s] %(message)s', datefmt='%H:%M:%S'))
- logger.addHandler(file_handler)
- 
- 
- class ExtractResult:
-     """Classe représentant le résultat d'une extraction."""
-     
-     def __init__(
-         self,
-         source_name: str,
-         extract_name: str,
-         status: str,
-         message: str,
-         start_marker: str = "",
-         end_marker: str = "",
-         template_start: str = "",
-         explanation: str = "",
-         extracted_text: str = ""
-     ):
-         """
-         Initialise un résultat d'extraction.
-         
-         Args:
-             source_name: Nom de la source
-             extract_name: Nom de l'extrait
-             status: Statut de l'extraction (valid, rejected, error)
-             message: Message explicatif
-             start_marker: Marqueur de début
-             end_marker: Marqueur de fin
-             template_start: Template pour le marqueur de début
-             explanation: Explication de l'extraction
-             extracted_text: Texte extrait
-         """
-         self.source_name = source_name
-         self.extract_name = extract_name
-         self.status = status
-         self.message = message
-         self.start_marker = start_marker
-         self.end_marker = end_marker
-         self.template_start = template_start
-         self.explanation = explanation
-         self.extracted_text = extracted_text
-     
-     def to_dict(self) -> Dict[str, Any]:
-         """Convertit le résultat en dictionnaire."""
-         return {
-             "source_name": self.source_name,
-             "extract_name": self.extract_name,
-             "status": self.status,
-             "message": self.message,
-             "start_marker": self.start_marker,
-             "end_marker": self.end_marker,
-             "template_start": self.template_start,
-             "explanation": self.explanation,
-             "extracted_text": self.extracted_text
-         }
-     
-     @classmethod
-     def from_dict(cls, data: Dict[str, Any]) -> 'ExtractResult':
-         """Crée un résultat à partir d'un dictionnaire."""
-         return cls(
-             source_name=data.get("source_name", ""),
-             extract_name=data.get("extract_name", ""),
-             status=data.get("status", ""),
-             message=data.get("message", ""),
-             start_marker=data.get("start_marker", ""),
-             end_marker=data.get("end_marker", ""),
-             template_start=data.get("template_start", ""),
-             explanation=data.get("explanation", ""),
-             extracted_text=data.get("extracted_text", "")
-         )
- 
- 
- class ExtractAgentPlugin:
-     """Plugin pour les fonctions natives de l'extracteur agentique."""
-     
-     def __init__(self):
-         """Initialise le plugin d'extraction."""
-         self.extract_results = []
-     
-     def find_similar_markers(
-         self, 
-         text: str, 
-         marker: str, 
-         max_results: int = 5,
-         find_similar_text_func=None
-     ) -> List[Dict[str, Any]]:
-         """
-         Trouve des marqueurs similaires dans le texte source.
-         
-         Args:
-             text: Texte source complet
-             marker: Marqueur à rechercher
-             max_results: Nombre maximum de résultats à retourner
-             find_similar_text_func: Fonction pour trouver du texte similaire
-             
-         Returns:
-             Liste de dictionnaires contenant les marqueurs similaires
-         """
-         if not text or not marker:
-             return []
-         
-         if find_similar_text_func is None:
-             # Implémentation par défaut si la fonction n'est pas fournie
-             logger.warning("Fonction find_similar_text non fournie, utilisation d'une implémentation basique")
-             
-             similar_markers = []
-             try:
-                 # Recherche simple avec regex
-                 pattern = re.escape(marker[:min(10, len(marker))])
-                 matches = list(re.finditer(pattern, text, re.IGNORECASE))
-                 
-                 for match in matches[:max_results]:
-                     start_pos = max(0, match.start() - 50)
-                     end_pos = min(len(text), match.end() + 50)
-                     context = text[start_pos:end_pos]
-                     
-                     similar_markers.append({
-                         "marker": match.group(),
-                         "position": match.start(),
-                         "context": context
-                     })
-                 
-                 return similar_markers
-             except Exception as e:
-                 logger.error(f"Erreur lors de la recherche de marqueurs similaires: {e}")
-                 return []
-         else:
-             # Utiliser la fonction fournie
-             similar_markers = []
-             results = find_similar_text_func(text, marker, context_size=50, max_results=max_results)
-             
-             for context, position, found_text in results:
-                 similar_markers.append({
-                     "marker": found_text,
-                     "position": position,
-                     "context": context
-                 })
-             
-             return similar_markers
-     
-     def search_text_dichotomically(
-         self, 
-         text: str, 
-         search_term: str, 
-         block_size: int = 500, 
-         overlap: int = 50
-     ) -> List[Dict[str, Any]]:
-         """
-         Recherche un terme dans le texte en utilisant une approche dichotomique.
-         
-         Args:
-             text: Texte source complet
-             search_term: Terme à rechercher
-             block_size: Taille des blocs de texte à analyser
-             overlap: Chevauchement entre les blocs
-             
-         Returns:
-             Liste de dictionnaires contenant les résultats de recherche
-         """
-         if not text or not search_term:
-             return []
-         
-         results = []
-         text_length = len(text)
-         
-         # Diviser le texte en blocs avec chevauchement
-         for i in range(0, text_length, block_size - overlap):
-             start_pos = i
-             end_pos = min(i + block_size, text_length)
-             block = text[start_pos:end_pos]
-             
-             # Rechercher le terme dans le bloc
-             if search_term.lower() in block.lower():
-                 # Trouver toutes les occurrences
-                 for match in re.finditer(re.escape(search_term), block, re.IGNORECASE):
-                     match_start = start_pos + match.start()
-                     match_end = start_pos + match.end()
-                     
-                     # Extraire le contexte
-                     context_start = max(0, match_start - 50)
-                     context_end = min(text_length, match_end + 50)
-                     context = text[context_start:context_end]
-                     
-                     results.append({
-                         "match": match.group(),
-                         "position": match_start,
-                         "context": context,
-                         "block_start": start_pos,
-                         "block_end": end_pos
-                     })
-         
-         return results
-     
-     def extract_blocks(
-         self, 
-         text: str, 
-         block_size: int = 500, 
-         overlap: int = 50
-     ) -> List[Dict[str, Any]]:
-         """
-         Extrait des blocs de texte avec chevauchement pour l'analyse.
-         
-         Args:
-             text: Texte source complet
-             block_size: Taille des blocs de texte à extraire
-             overlap: Chevauchement entre les blocs
-             
-         Returns:
-             Liste de dictionnaires contenant les blocs de texte
-         """
-         if not text:
-             return []
-         
-         blocks = []
-         text_length = len(text)
-         
-         for i in range(0, text_length, block_size - overlap):
-             start_pos = i
-             end_pos = min(i + block_size, text_length)
-             block = text[start_pos:end_pos]
-             
-             blocks.append({
-                 "block": block,
-                 "start_pos": start_pos,
-                 "end_pos": end_pos
-             })
-         
-         return blocks
-     
-     def get_extract_results(self) -> List[Dict[str, Any]]:
-         """Récupère les résultats des extractions effectuées."""
-         return self.extract_results
- 
- 
- class ExtractDefinition:
-     """Classe représentant la définition d'une extraction."""
-     
-     def __init__(
-         self,
-         source_name: str,
-         extract_name: str,
-         start_marker: str,
-         end_marker: str,
-         template_start: str = "",
-         description: str = ""
-     ):
-         """
-         Initialise une définition d'extraction.
-         
-         Args:
-             source_name: Nom de la source
-             extract_name: Nom de l'extrait
-             start_marker: Marqueur de début
-             end_marker: Marqueur de fin
-             template_start: Template pour le marqueur de début
-             description: Description de l'extraction
-         """
-         self.source_name = source_name
-         self.extract_name = extract_name
-         self.start_marker = start_marker
-         self.end_marker = end_marker
-         self.template_start = template_start
-         self.description = description
-     
-     def to_dict(self) -> Dict[str, Any]:
-         """Convertit la définition en dictionnaire."""
-         return {
-             "source_name": self.source_name,
-             "extract_name": self.extract_name,
-             "start_marker": self.start_marker,
-             "end_marker": self.end_marker,
-             "template_start": self.template_start,
-             "description": self.description
-         }
-     
-     @classmethod
-     def from_dict(cls, data: Dict[str, Any]) -> 'ExtractDefinition':
-         """Crée une définition à partir d'un dictionnaire."""
-         return cls(
-             source_name=data.get("source_name", ""),
-             extract_name=data.get("extract_name", ""),
-             start_marker=data.get("start_marker", ""),
-             end_marker=data.get("end_marker", ""),
-             template_start=data.get("template_start", ""),
-             description=data.get("description", "")
+ # argumentation_analysis/agents/core/extract/extract_definitions.py
+ """
+ Définitions et structures de données pour l'agent d'extraction.
+ 
+ Ce module contient les classes Pydantic (ou similaires) et les structures de données
+ utilisées par `ExtractAgent` et ses composants. Il définit :
+     - `ExtractResult`: Pour encapsuler le résultat d'une opération d'extraction.
+     - `ExtractAgentPlugin`: Un plugin contenant des fonctions natives utiles
+       pour le traitement de texte dans le contexte de l'extraction.
+     - `ExtractDefinition`: Pour représenter la définition d'un extrait spécifique
+       à rechercher dans un texte source.
+ """
+ 
+ import re
+ import logging
+ from pathlib import Path # De la version stashed
+ from typing import List, Dict, Any, Tuple, Optional, Union
++import json
+ 
+ # Importer PROJECT_ROOT depuis la configuration centrale (de la version stashed)
+ try:
+     from argumentation_analysis.ui.config import PROJECT_ROOT
+ except ImportError:
+     # Fallback si le script est exécuté dans un contexte où l'import direct n'est pas possible
+     PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent.parent.parent
+ 
+ # Configuration du logging (de la version stashed)
+ logging.basicConfig(
+     level=logging.INFO,
+     format='%(asctime)s [%(levelname)s] [%(name)s] %(message)s',
+     datefmt='%H:%M:%S'
+ )
+ logger = logging.getLogger("ExtractAgent.Definitions")
+ 
+ # Création d'un handler pour écrire les logs dans un fichier (de la version stashed)
+ log_dir = PROJECT_ROOT / "_temp" / "logs"
+ log_dir.mkdir(parents=True, exist_ok=True)
+ log_file_path = log_dir / "extract_agent.log"
+ 
+ file_handler = logging.FileHandler(log_file_path, mode='a', encoding='utf-8')
+ file_handler.setFormatter(logging.Formatter('%(asctime)s [%(levelname)s] [%(name)s] %(message)s', datefmt='%H:%M:%S'))
+ logger.addHandler(file_handler)
+ 
+ 
+ class ExtractResult: # De la version HEAD (Updated upstream)
+     """
+     Classe représentant le résultat d'une opération d'extraction.
+ 
+     Cette classe encapsule toutes les informations pertinentes suite à une tentative
+     d'extraction, y compris le statut, les marqueurs, le texte extrait et
+     toute explication ou message d'erreur.
+ 
+     Attributes:
+         source_name (str): Nom de la source du texte.
+         extract_name (str): Nom de l'extrait.
+         status (str): Statut de l'extraction (ex: "valid", "rejected", "error").
+         message (str): Message descriptif concernant le résultat.
+         start_marker (str): Marqueur de début utilisé ou proposé.
+         end_marker (str): Marqueur de fin utilisé ou proposé.
+         template_start (str): Template de début utilisé ou proposé.
+         explanation (str): Explication fournie par l'agent pour l'extraction.
+         extracted_text (str): Le texte effectivement extrait.
+     """
 -    
++
+     def __init__(
+         self,
+         source_name: str,
+         extract_name: str,
+         status: str,
+         message: str,
+         start_marker: str = "",
+         end_marker: str = "",
+         template_start: str = "",
+         explanation: str = "",
+         extracted_text: str = ""
+     ):
+         """
+         Initialise un objet `ExtractResult`.
+ 
+         :param source_name: Nom de la source du texte.
+         :type source_name: str
+         :param extract_name: Nom de l'extrait.
+         :type extract_name: str
+         :param status: Statut de l'extraction (par exemple, "valid", "rejected", "error").
+         :type status: str
+         :param message: Message descriptif concernant le résultat de l'extraction.
+         :type message: str
+         :param start_marker: Marqueur de début utilisé ou proposé. Par défaut "".
+         :type start_marker: str
+         :param end_marker: Marqueur de fin utilisé ou proposé. Par défaut "".
+         :type end_marker: str
+         :param template_start: Template de début utilisé ou proposé. Par défaut "".
+         :type template_start: str
+         :param explanation: Explication fournie par l'agent pour l'extraction. Par défaut "".
+         :type explanation: str
+         :param extracted_text: Le texte effectivement extrait. Par défaut "".
+         :type extracted_text: str
+         """
+         self.source_name = source_name
+         self.extract_name = extract_name
+         self.status = status
+         self.message = message
+         self.start_marker = start_marker
+         self.end_marker = end_marker
+         self.template_start = template_start
+         self.explanation = explanation
+         self.extracted_text = extracted_text
 -    
++
+     def to_dict(self) -> Dict[str, Any]:
+         """Convertit l'instance `ExtractResult` en un dictionnaire.
+ 
+         :return: Un dictionnaire représentant l'objet.
+         :rtype: Dict[str, Any]
+         """
+         return {
+             "source_name": self.source_name,
+             "extract_name": self.extract_name,
+             "status": self.status,
+             "message": self.message,
+             "start_marker": self.start_marker,
+             "end_marker": self.end_marker,
+             "template_start": self.template_start,
+             "explanation": self.explanation,
+             "extracted_text": self.extracted_text
+         }
 -    
++
+     def to_json(self) -> str:
+         """Convertit l'instance `ExtractResult` en une chaîne JSON.
+ 
+         :return: Une chaîne JSON représentant l'objet.
+         :rtype: str
+         """
+         return json.dumps(self.to_dict(), indent=2)
+ 
+     @classmethod
+     def from_dict(cls, data: Dict[str, Any]) -> 'ExtractResult':
+         """Crée une instance de `ExtractResult` à partir d'un dictionnaire.
+ 
+         :param data: Dictionnaire contenant les données pour initialiser l'objet.
+         :type data: Dict[str, Any]
+         :return: Une nouvelle instance de `ExtractResult`.
+         :rtype: ExtractResult
+         """
+         return cls(
+             source_name=data.get("source_name", ""),
+             extract_name=data.get("extract_name", ""),
+             status=data.get("status", ""),
+             message=data.get("message", ""),
+             start_marker=data.get("start_marker", ""),
+             end_marker=data.get("end_marker", ""),
+             template_start=data.get("template_start", ""),
+             explanation=data.get("explanation", ""),
+             extracted_text=data.get("extracted_text", "")
+         )
+ 
+ 
+ class ExtractAgentPlugin: # De la version HEAD (Updated upstream)
+     """
+     Plugin contenant des fonctions natives utiles pour l'agent d'extraction.
+ 
+     Ce plugin regroupe des méthodes de traitement de texte qui ne nécessitent pas
+     d'appel à un LLM mais sont utiles pour préparer les données ou analyser
+     les textes sources dans le cadre du processus d'extraction.
+ 
+     Attributes:
+         extract_results (List[Dict[str, Any]]): Une liste pour stocker les résultats
+             des opérations d'extraction effectuées, à des fins de journalisation ou de suivi.
+             (Note: L'utilisation de cette liste pourrait être revue pour une meilleure gestion d'état).
+     """
 -    
++
+     def __init__(self):
+         """Initialise le plugin `ExtractAgentPlugin`.
+ 
+         Initialise une liste vide `extract_results` pour stocker les résultats
+         des opérations d'extraction effectuées par ce plugin.
+         """
+         self.extract_results: List[Dict[str, Any]] = []
 -    
++
+     def find_similar_markers(
 -        self, 
 -        text: str, 
 -        marker: str, 
++        self,
++        text: str,
++        marker: str,
+         max_results: int = 5,
+         find_similar_text_func=None
+     ) -> List[Dict[str, Any]]:
+         """
+         Trouve des marqueurs textuels similaires à un marqueur donné dans un texte source.
+ 
+         Utilise soit une fonction `find_similar_text_func` fournie, soit une
+         implémentation basique par défaut basée sur des regex simples.
+ 
+         :param text: Le texte source complet dans lequel rechercher.
+         :type text: str
+         :param marker: Le marqueur (chaîne de caractères) à rechercher.
+         :type marker: str
+         :param max_results: Le nombre maximum de résultats similaires à retourner.
+         :type max_results: int
+         :param find_similar_text_func: Fonction optionnelle à utiliser pour trouver
+                                        du texte similaire. Si None, une recherche
+                                        basique est effectuée.
+         :type find_similar_text_func: Optional[Callable]
+         :return: Une liste de dictionnaires, chaque dictionnaire représentant un marqueur
+                  similaire trouvé et contenant "marker", "position", et "context".
+                  Retourne une liste vide si aucun marqueur similaire n'est trouvé ou
+                  si `text` ou `marker` sont vides.
+         :rtype: List[Dict[str, Any]]
+         """
+         if not text or not marker:
+             return []
 -        
++
+         if find_similar_text_func is None:
+             # Implémentation par défaut si la fonction n'est pas fournie
+             logger.warning("Fonction find_similar_text non fournie, utilisation d'une implémentation basique")
 -            
++
+             similar_markers = []
+             try:
+                 # Recherche simple avec regex
+                 pattern = re.escape(marker[:min(10, len(marker))])
+                 matches = list(re.finditer(pattern, text, re.IGNORECASE))
 -                
++
+                 for match in matches[:max_results]:
+                     start_pos = max(0, match.start() - 50)
+                     end_pos = min(len(text), match.end() + 50)
+                     context = text[start_pos:end_pos]
 -                    
++
+                     similar_markers.append({
+                         "marker": match.group(),
+                         "position": match.start(),
+                         "context": context
+                     })
 -                
++
+                 return similar_markers
+             except Exception as e:
+                 logger.error(f"Erreur lors de la recherche de marqueurs similaires: {e}")
+                 return []
+         else:
+             # Utiliser la fonction fournie
+             similar_markers = []
+             results = find_similar_text_func(text, marker, context_size=50, max_results=max_results)
 -            
++
+             for context, position, found_text in results:
+                 similar_markers.append({
+                     "marker": found_text,
+                     "position": position,
+                     "context": context
+                 })
 -            
++
+             return similar_markers
 -    
++
+     def search_text_dichotomically(
 -        self, 
 -        text: str, 
 -        search_term: str, 
 -        block_size: int = 500, 
++        self,
++        text: str,
++        search_term: str,
++        block_size: int = 500,
+         overlap: int = 50
+     ) -> List[Dict[str, Any]]:
+         """
+         Recherche un terme dans un texte en le divisant d'abord en blocs.
+ 
+         Cette méthode est une simplification et ne réalise pas une recherche
+         dichotomique au sens strict algorithmique, mais plutôt une recherche
+         par blocs. Elle divise le texte en blocs avec chevauchement et recherche
+         le terme (insensible à la casse) dans chaque bloc.
+ 
+         :param text: Le texte source complet dans lequel rechercher.
+         :type text: str
+         :param search_term: Le terme à rechercher.
+         :type search_term: str
+         :param block_size: La taille des blocs dans lesquels diviser le texte.
+         :type block_size: int
+         :param overlap: Le chevauchement entre les blocs consécutifs.
+         :type overlap: int
+         :return: Une liste de dictionnaires. Chaque dictionnaire représente une
+                  correspondance trouvée et contient "match", "position", "context",
+                  "block_start", et "block_end".
+                  Retourne une liste vide si `text` ou `search_term` sont vides.
+         :rtype: List[Dict[str, Any]]
+         """
+         if not text or not search_term:
+             return []
 -        
++
+         results = []
+         text_length = len(text)
 -        
++
+         # Diviser le texte en blocs avec chevauchement
+         for i in range(0, text_length, block_size - overlap):
+             start_pos = i
+             end_pos = min(i + block_size, text_length)
+             block = text[start_pos:end_pos]
 -            
++
+             # Rechercher le terme dans le bloc
+             if search_term.lower() in block.lower():
+                 # Trouver toutes les occurrences
+                 for match in re.finditer(re.escape(search_term), block, re.IGNORECASE):
+                     match_start = start_pos + match.start()
+                     match_end = start_pos + match.end()
 -                    
++
+                     # Extraire le contexte
+                     context_start = max(0, match_start - 50)
+                     context_end = min(text_length, match_end + 50)
+                     context = text[context_start:context_end]
 -                    
++
+                     results.append({
+                         "match": match.group(),
+                         "position": match_start,
+                         "context": context,
+                         "block_start": start_pos,
+                         "block_end": end_pos
+                     })
 -        
++
+         return results
 -    
++
+     def extract_blocks(
 -        self, 
 -        text: str, 
 -        block_size: int = 500, 
++        self,
++        text: str,
++        block_size: int = 500,
+         overlap: int = 50
+     ) -> List[Dict[str, Any]]:
+         """
+         Divise un texte en blocs de taille spécifiée avec un chevauchement défini.
+ 
+         Utile pour traiter de grands textes par morceaux.
+ 
+         :param text: Le texte source complet à diviser en blocs.
+         :type text: str
+         :param block_size: La taille souhaitée pour chaque bloc de texte.
+         :type block_size: int
+         :param overlap: Le nombre de caractères de chevauchement entre les blocs consécutifs.
+         :type overlap: int
+         :return: Une liste de dictionnaires. Chaque dictionnaire représente un bloc et
+                  contient "block", "start_pos", et "end_pos".
+                  Retourne une liste vide si le texte d'entrée est vide.
+         :rtype: List[Dict[str, Any]]
+         """
+         if not text:
+             return []
 -        
++
+         blocks = []
+         text_length = len(text)
 -        
++
+         for i in range(0, text_length, block_size - overlap):
+             start_pos = i
+             end_pos = min(i + block_size, text_length)
+             block = text[start_pos:end_pos]
 -            
++
+             blocks.append({
+                 "block": block,
+                 "start_pos": start_pos,
+                 "end_pos": end_pos
+             })
 -        
++
+         return blocks
 -    
++
+     def get_extract_results(self) -> List[Dict[str, Any]]:
+         """Récupère la liste des résultats des opérations d'extraction stockées.
+ 
+         :return: Une liste de dictionnaires, chaque dictionnaire représentant
+                  le résultat d'une opération d'extraction.
+         :rtype: List[Dict[str, Any]]
+         """
+         return self.extract_results
+ 
+ 
+ class ExtractDefinition: # De la version HEAD (Updated upstream)
+     """
+     Classe représentant la définition d'un extrait à rechercher ou à gérer.
+ 
+     Cette structure de données contient les informations nécessaires pour identifier
+     et localiser un segment de texte spécifique (un "extrait") au sein d'un
+     document source plus large.
+ 
+     Attributes:
+         source_name (str): Nom de la source du texte.
+         extract_name (str): Nom ou description de l'extrait.
+         start_marker (str): Le marqueur textuel indiquant le début de l'extrait.
+         end_marker (str): Le marqueur textuel indiquant la fin de l'extrait.
+         template_start (str): Un template optionnel qui peut précéder le `start_marker`.
+         description (str): Une description optionnelle de ce que représente l'extrait.
+     """
 -    
++
+     def __init__(
+         self,
+         source_name: str,
+         extract_name: str,
+         start_marker: str,
+         end_marker: str,
+         template_start: str = "",
+         description: str = ""
+     ):
+         """
+         Initialise un objet `ExtractDefinition`.
+ 
+         :param source_name: Nom de la source du texte.
+         :type source_name: str
+         :param extract_name: Nom de l'extrait.
+         :type extract_name: str
+         :param start_marker: Marqueur de début pour l'extrait.
+         :type start_marker: str
+         :param end_marker: Marqueur de fin pour l'extrait.
+         :type end_marker: str
+         :param template_start: Template optionnel pour le marqueur de début. Par défaut "".
+         :type template_start: str
+         :param description: Description optionnelle de l'extraction. Par défaut "".
+         :type description: str
+         """
+         self.source_name = source_name
+         self.extract_name = extract_name
+         self.start_marker = start_marker
+         self.end_marker = end_marker
+         self.template_start = template_start
+         self.description = description
 -    
++
+     def to_dict(self) -> Dict[str, Any]:
+         """Convertit l'instance `ExtractDefinition` en un dictionnaire.
+ 
+         :return: Un dictionnaire représentant l'objet.
+         :rtype: Dict[str, Any]
+         """
+         return {
+             "source_name": self.source_name,
+             "extract_name": self.extract_name,
+             "start_marker": self.start_marker,
+             "end_marker": self.end_marker,
+             "template_start": self.template_start,
+             "description": self.description
+         }
 -    
++
+     @classmethod
+     def from_dict(cls, data: Dict[str, Any]) -> 'ExtractDefinition':
+         """Crée une instance de `ExtractDefinition` à partir d'un dictionnaire.
+ 
+         :param data: Dictionnaire contenant les données pour initialiser l'objet.
+         :type data: Dict[str, Any]
+         :return: Une nouvelle instance de `ExtractDefinition`.
+         :rtype: ExtractDefinition
+         """
+         return cls(
+             source_name=data.get("source_name", ""),
+             extract_name=data.get("extract_name", ""),
+             start_marker=data.get("start_marker", ""),
+             end_marker=data.get("end_marker", ""),
+             template_start=data.get("template_start", ""),
+             description=data.get("description", "")
          )
diff --cc argumentation_analysis/orchestration/cluedo_extended_orchestrator.py
index c6278399,22e5b9b0..b85513cd
--- a/argumentation_analysis/orchestration/cluedo_extended_orchestrator.py
+++ b/argumentation_analysis/orchestration/cluedo_extended_orchestrator.py
@@@ -85,28 -329,39 +329,39 @@@ class CluedoExtendedOrchestrator
          self.watson_agent: Optional[WatsonLogicAssistant] = None
          self.moriarty_agent: Optional[MoriartyInterrogatorAgent] = None
          self.orchestration: Optional[GroupChatOrchestration] = None
+         # self.runtime: Optional[InProcessRuntime] = None  # Module non disponible
          
-         self.selection_strategy = None
-         self.termination_strategy = None
-         self.suggestion_handler = None
-         
+         # Métriques de performance
          self.start_time: Optional[datetime] = None
          self.end_time: Optional[datetime] = None
- 
+         self.execution_metrics: Dict[str, Any] = {}
+         
+         self._logger = logging.getLogger(self.__class__.__name__)
+     
      async def setup_workflow(self,
                             nom_enquete: str = "Le Mystère du Manoir Tudor",
-                            elements_jeu: Optional[Dict[str, List[str]]] = None,
-                            initial_cards: Dict[str, List[str]] = None):
-         """Configure le workflow, l'état Oracle, les agents et les composants logiques."""
-         logger.info(f"Configuration du workflow 3-agents - Stratégie: {self.oracle_strategy}")
- 
+                            elements_jeu: Optional[Dict[str, List[str]]] = None) -> CluedoOracleState:
+         """
+         Configure le workflow 3-agents avec état Oracle.
+         
+         Args:
+             nom_enquete: Nom de l'enquête
+             elements_jeu: Éléments du jeu Cluedo (optionnel)
+             
+         Returns:
+             État Oracle configuré
+         """
+         self._logger.info(f"Configuration du workflow 3-agents - Stratégie: {self.oracle_strategy}")
+         
+         # Configuration des éléments par défaut
          if elements_jeu is None:
              elements_jeu = {
 -                "suspects": ["Colonel Moutarde", "Professeur Violet", "Mademoiselle Rose", "Docteur Orchidée"],
 -                "armes": ["Poignard", "Chandelier", "Revolver", "Corde"],
 -                "lieux": ["Salon", "Cuisine", "Bureau", "Bibliothèque"]
 +                "suspects": ["Colonel Moutarde", "Professeur Violet", "Mademoiselle Rose"],
 +                "armes": ["Poignard", "Chandelier", "Revolver"],
 +                "lieux": ["Salon", "Cuisine", "Bureau"]
              }
          
+         # Création de l'état Oracle étendu
          self.oracle_state = CluedoOracleState(
              nom_enquete_cluedo=nom_enquete,
              elements_jeu_cluedo=elements_jeu,
diff --cc argumentation_analysis/ui/file_operations.py
index 3cab4076,0e7e6ae1..903d6240
--- a/argumentation_analysis/ui/file_operations.py
+++ b/argumentation_analysis/ui/file_operations.py
@@@ -1,144 -1,156 +1,156 @@@
- # argumentation_analysis/ui/file_operations.py
- import json
- import gzip
- import logging
- from pathlib import Path
- from typing import Optional, List, Dict, Any
- from cryptography.fernet import Fernet, InvalidToken
- from cryptography.exceptions import InvalidSignature
- 
- # Importer les éléments nécessaires depuis config et utils
- # Attention à ne pas recréer de cycle.
- # On importe 'config as ui_config_module' pour accéder aux constantes.
- from . import config as ui_config_module
- # On importe les fonctions de utils qui ne dépendent pas de config de manière cyclique.
- from .utils import encrypt_data, decrypt_data, get_full_text_for_source, utils_logger # utils_logger est déjà configuré dans utils.py
- 
- # Logger spécifique pour les opérations sur fichiers si besoin, ou utiliser utils_logger
- file_ops_logger = utils_logger # Ou logging.getLogger("App.UI.FileOps")
- 
- 
- def load_extract_definitions(
-     config_file: Path, 
-     key: bytes,
-     # app_config est utilisé par get_full_text_for_source, mais load_extract_definitions
-     # lui-même ne l'utilise pas directement pour le chargement/déchiffrement.
-     # Cependant, si on voulait que load_extract_definitions peuple les full_text au chargement,
-     # il faudrait le passer. Pour l'instant, on le garde optionnel et non utilisé ici.
-     app_config: Optional[Dict[str, Any]] = None 
- ) -> list:
-     """Charge, déchiffre et décompresse les définitions depuis le fichier chiffré."""
-     fallback_definitions = ui_config_module.EXTRACT_SOURCES if ui_config_module.EXTRACT_SOURCES else ui_config_module.DEFAULT_EXTRACT_SOURCES
- 
-     if not config_file.exists():
-         file_ops_logger.info(f"Fichier config chiffré '{config_file}' non trouvé. Utilisation définitions par défaut.")
-         return [item.copy() for item in fallback_definitions]
-     if not key:
-         file_ops_logger.warning("Clé chiffrement absente. Chargement config impossible. Utilisation définitions par défaut.")
-         return [item.copy() for item in fallback_definitions]
- 
-     file_ops_logger.info(f"Chargement et déchiffrement de '{config_file}'...")
-     try:
-         with open(config_file, 'rb') as f: encrypted_data = f.read()
-         decrypted_compressed_data = decrypt_data(encrypted_data, key) # Utilise decrypt_data de utils.py
-         if not decrypted_compressed_data:
-             file_ops_logger.warning("Échec déchiffrement. Utilisation définitions par défaut.")
-             return [item.copy() for item in fallback_definitions]
-         decompressed_data = gzip.decompress(decrypted_compressed_data)
-         definitions = json.loads(decompressed_data.decode('utf-8'))
-         file_ops_logger.info("✅ Définitions chargées et déchiffrées.")
- 
-         if not isinstance(definitions, list) or not all(
-             isinstance(item, dict) and
-             "source_name" in item and "source_type" in item and "schema" in item and
-             "host_parts" in item and "path" in item and isinstance(item.get("extracts"), list)
-             for item in definitions
-         ):
-             file_ops_logger.warning("⚠️ Format définitions invalide après chargement. Utilisation définitions par défaut.")
-             return [item.copy() for item in fallback_definitions]
- 
-         file_ops_logger.info(f"-> {len(definitions)} définitions chargées depuis fichier.")
-         return definitions
-     except (InvalidToken, InvalidSignature) as e: # Intercepter spécifiquement et relancer
-         file_ops_logger.error(f"❌ Erreur déchiffrement/validation token pour '{config_file}': {e}. L'exception sera relancée.", exc_info=True)
-         raise # Relancer l'exception InvalidToken ou InvalidSignature
-     except Exception as e:
-         file_ops_logger.error(f"❌ Erreur chargement/traitement général '{config_file}': {e}. Utilisation définitions par défaut.", exc_info=True)
-         return [item.copy() for item in fallback_definitions]
- 
- def save_extract_definitions(
-     extract_definitions: List[Dict[str, Any]],
-     config_file: Path, # Renommé de config_path pour correspondre à l'usage dans embed_all_sources.py
-     encryption_key: bytes, # Renommé de passphrase pour refléter qu'une clé Fernet est attendue par encrypt_data
-     embed_full_text: bool = False,
-     config: Optional[Dict[str, Any]] = None # 'config' est le app_config passé à get_full_text_for_source
- ) -> bool:
-     """Sauvegarde, compresse et chiffre les définitions dans le fichier.
-     Peut optionnellement embarquer le texte complet des sources.
-     """
-     if not encryption_key:
-         file_ops_logger.error("Clé chiffrement absente. Sauvegarde annulée.")
-         return False
-     if not isinstance(extract_definitions, list):
-         file_ops_logger.error("Erreur sauvegarde: définitions non valides (doit être une liste).")
-         return False
- 
-     file_ops_logger.info(f"Préparation sauvegarde vers '{config_file}'...")
- 
-     # Copie profonde pour éviter de modifier la liste originale en dehors de cette fonction
-     # lors du traitement de embed_full_text
-     definitions_to_process = [dict(d) for d in extract_definitions]
- 
- 
-     if embed_full_text:
-         file_ops_logger.info("Option embed_full_text activée. Tentative de récupération des textes complets manquants...")
-         for source_info in definitions_to_process: # Utiliser la copie
-             if not isinstance(source_info, dict):
-                 file_ops_logger.warning(f"Élément non-dictionnaire ignoré dans extract_definitions: {type(source_info)}")
-                 continue
- 
-             current_full_text = source_info.get("full_text")
-             if not current_full_text:
-                 source_name = source_info.get('source_name', 'Source inconnue')
-                 file_ops_logger.info(f"Texte complet manquant pour '{source_name}'. Récupération...")
-                 try:
-                     # Utilise get_full_text_for_source de utils.py
-                     retrieved_text = get_full_text_for_source(source_info, app_config=config)
-                     if retrieved_text is not None:
-                         source_info["full_text"] = retrieved_text
-                         file_ops_logger.info(f"Texte complet récupéré et ajouté pour '{source_name}'.")
-                     else:
-                         file_ops_logger.warning(f"Échec de la récupération du texte complet (texte vide retourné) pour '{source_name}'. Champ 'full_text' non peuplé.")
-                         source_info["full_text"] = None
-                 except ConnectionError as e_conn:
-                     file_ops_logger.warning(f"Erreur de connexion lors de la récupération du texte pour '{source_name}': {e_conn}. Champ 'full_text' non peuplé.")
-                     source_info["full_text"] = None
-                 except Exception as e_get_text:
-                     file_ops_logger.error(f"Erreur inattendue lors de la récupération du texte pour '{source_name}': {e_get_text}. Champ 'full_text' non peuplé.", exc_info=True)
-                     source_info["full_text"] = None
-     else:
-         file_ops_logger.info("Option embed_full_text désactivée. Suppression des textes complets des définitions...")
-         for source_info in definitions_to_process: # Utiliser la copie
-             if not isinstance(source_info, dict):
-                 continue
-             if "full_text" in source_info:
-                 source_info.pop("full_text", None)
-                 file_ops_logger.debug(f"Champ 'full_text' retiré pour '{source_info.get('source_name', 'Source inconnue')}'.")
- 
-     try:
-         json_data = json.dumps(definitions_to_process, indent=2, ensure_ascii=False).encode('utf-8') # Utiliser la copie traitée
-         compressed_data = gzip.compress(json_data)
-         encrypted_data_to_save = encrypt_data(compressed_data, encryption_key) # Utilise encrypt_data de utils.py
-         if not encrypted_data_to_save:
-             raise ValueError("Échec du chiffrement des données.")
- 
-         config_file.parent.mkdir(parents=True, exist_ok=True)
-         with open(config_file, 'wb') as f:
-             f.write(encrypted_data_to_save)
-         file_ops_logger.info(f"✅ Définitions sauvegardées dans '{config_file}'.")
-         return True
-     except Exception as e:
-         file_ops_logger.error(f"❌ Erreur lors de la sauvegarde chiffrée vers '{config_file}': {e}", exc_info=True)
-         return False
- 
+ # argumentation_analysis/ui/file_operations.py
+ from typing import Optional, Union, List, Dict, Any
+ import json
+ import gzip
+ import logging
+ import base64 
+ from pathlib import Path
+ # from typing import Optional, List, Dict, Any # Redondant avec la première ligne
+ from cryptography.fernet import InvalidToken 
+ 
+ from . import config as ui_config_module
+ from .utils import get_full_text_for_source, utils_logger 
+ from argumentation_analysis.utils.core_utils.crypto_utils import encrypt_data_with_fernet, decrypt_data_with_fernet
+ 
+ file_ops_logger = utils_logger
+ 
+ 
+ def load_extract_definitions(
+     config_file: Path,
+     b64_derived_key: Optional[str],
+     app_config: Optional[Dict[str, Any]] = None,
+     raise_on_decrypt_error: bool = False
+ ) -> list:
+     """Charge, déchiffre et décompresse les définitions depuis le fichier chiffré."""
+     # Utiliser uniquement DEFAULT_EXTRACT_SOURCES comme fallback pour éviter le cycle avec EXTRACT_SOURCES
+     # qui est en cours de définition par l'appelant (config.py)
+     fallback_definitions = ui_config_module.DEFAULT_EXTRACT_SOURCES
+ 
+     if not config_file.exists():
+         file_ops_logger.info(f"Fichier config '{config_file}' non trouvé. Utilisation définitions par défaut.")
+         return [item.copy() for item in fallback_definitions]
+ 
+     if b64_derived_key: # Clé fournie, tenter le déchiffrement
+         file_ops_logger.info(f"Chargement et déchiffrement de '{config_file}' avec clé...")
+         try:
+             with open(config_file, 'rb') as f: encrypted_data = f.read()
+             decrypted_compressed_data = decrypt_data_with_fernet(encrypted_data, b64_derived_key)
+             
+             if not decrypted_compressed_data:
+                 file_ops_logger.error(f"Échec du déchiffrement pour '{config_file}'. Le token est peut-être invalide.")
+                 raise InvalidToken(f"Échec du déchiffrement pour '{config_file}'.")
+ 
+             decompressed_data = gzip.decompress(decrypted_compressed_data)
+             definitions = json.loads(decompressed_data.decode('utf-8'))
+             file_ops_logger.info("✅ Définitions chargées et déchiffrées.")
+ 
+         except InvalidToken:
+             # Ce bloc est spécifiquement pour quand decrypt_data_with_fernet lève InvalidToken
+             file_ops_logger.error(f"❌ Token invalide (InvalidToken) lors du déchiffrement de '{config_file}'.", exc_info=True)
+             if raise_on_decrypt_error:
+                 raise
+             return [item.copy() for item in fallback_definitions]
+         except Exception as e:
+             file_ops_logger.error(f"[FAIL] Erreur chargement/dechiffrement '{config_file}': {e}. Utilisation definitions par defaut.", exc_info=True)
+             return [item.copy() for item in fallback_definitions]
+     
+     else: # Pas de clé, essayer de lire comme JSON simple
+         file_ops_logger.info(f"Aucune clé fournie. Tentative de chargement de '{config_file}' comme JSON simple...")
+         try:
+             with open(config_file, 'r', encoding='utf-8') as f:
+                 definitions = json.load(f)
+             file_ops_logger.info(f"[OK] Définitions chargées comme JSON simple depuis '{config_file}'.")
+         
+         except json.JSONDecodeError as e_json:
+             file_ops_logger.error(f"[FAIL] Erreur decodage JSON pour '{config_file}': {e_json}. L'exception sera relancee.", exc_info=False)
+             raise
+         except Exception as e:
+             file_ops_logger.error(f"[FAIL] Erreur chargement JSON simple '{config_file}': {e}. Utilisation definitions par defaut.", exc_info=True)
+             return [item.copy() for item in fallback_definitions]
+ 
+     # Validation du format (commun aux deux chemins)
+     if not isinstance(definitions, list) or not all(
+         isinstance(item, dict) and
+         "source_name" in item and "source_type" in item and "schema" in item and
+         "host_parts" in item and "path" in item and isinstance(item.get("extracts"), list)
+         for item in definitions
+     ):
+         file_ops_logger.warning(f"[WARN] Format definitions invalide apres chargement de '{config_file}'. Utilisation definitions par defaut.")
+         return [item.copy() for item in fallback_definitions]
+ 
+     file_ops_logger.info(f"-> {len(definitions)} définitions chargées depuis '{config_file}'.")
+     return definitions
+ 
+ def save_extract_definitions(
+     extract_definitions: List[Dict[str, Any]],
+     config_file: Path,
+     b64_derived_key: Optional[Union[str, bytes]], 
+     embed_full_text: bool = False,
+     config: Optional[Dict[str, Any]] = None 
+ ) -> bool:
+     """Sauvegarde, compresse et chiffre les définitions dans le fichier.
+     Peut optionnellement embarquer le texte complet des sources.
+     """
+     if not b64_derived_key: 
+         file_ops_logger.error("Clé chiffrement (b64_derived_key) absente ou vide. Sauvegarde annulée.")
+         return False
+     if not isinstance(extract_definitions, list):
+         file_ops_logger.error("Erreur sauvegarde: définitions non valides (doit être une liste).")
+         return False
+ 
+     file_ops_logger.info(f"Préparation sauvegarde vers '{config_file}'...")
+ 
+     definitions_to_process = [dict(d) for d in extract_definitions]
+ 
+ 
+     if embed_full_text:
+         file_ops_logger.info("Option embed_full_text activée. Tentative de récupération des textes complets manquants...")
+         for source_info in definitions_to_process: 
+             if not isinstance(source_info, dict):
+                 file_ops_logger.warning(f"Élément non-dictionnaire ignoré dans extract_definitions: {type(source_info)}")
+                 continue
+ 
+             current_full_text = source_info.get("full_text")
+             if not current_full_text:
+                 source_name = source_info.get('source_name', 'Source inconnue')
+                 file_ops_logger.info(f"Texte complet manquant pour '{source_name}'. Récupération...")
+                 try:
+                     retrieved_text = get_full_text_for_source(source_info, app_config=config)
+                     if retrieved_text is not None:
+                         source_info["full_text"] = retrieved_text
+                         file_ops_logger.info(f"Texte complet récupéré et ajouté pour '{source_name}'.")
+                     else:
+                         file_ops_logger.warning(f"Échec de la récupération du texte complet (texte vide retourné) pour '{source_name}'. Champ 'full_text' non peuplé.")
+                         source_info["full_text"] = None
+                 except ConnectionError as e_conn:
+                     file_ops_logger.warning(f"Erreur de connexion lors de la récupération du texte pour '{source_name}': {e_conn}. Champ 'full_text' non peuplé.")
+                     source_info["full_text"] = None
+                 except Exception as e_get_text:
+                     file_ops_logger.error(f"Erreur inattendue lors de la récupération du texte pour '{source_name}': {e_get_text}. Champ 'full_text' non peuplé.", exc_info=True)
+                     source_info["full_text"] = None
+     else:
+         file_ops_logger.info("Option embed_full_text désactivée. Suppression des textes complets des définitions...")
+         for source_info in definitions_to_process: 
+             if not isinstance(source_info, dict):
+                 continue
+             if "full_text" in source_info:
+                 source_info.pop("full_text", None)
+                 file_ops_logger.debug(f"Champ 'full_text' retiré pour '{source_info.get('source_name', 'Source inconnue')}'.")
+ 
+     try:
+         json_data = json.dumps(definitions_to_process, indent=2, ensure_ascii=False).encode('utf-8')
+         compressed_data = gzip.compress(json_data)
+         encrypted_data_to_save = encrypt_data_with_fernet(compressed_data, b64_derived_key)
+         if not encrypted_data_to_save:
+             raise ValueError("Échec du chiffrement des données (encrypt_data_with_fernet a retourné None).")
+ 
+         config_file.parent.mkdir(parents=True, exist_ok=True)
+         with open(config_file, 'wb') as f:
+             f.write(encrypted_data_to_save)
+         file_ops_logger.info(f"[OK] Définitions sauvegardées dans '{config_file}'.")
+         return True
+     except Exception as e:
 -        file_ops_logger.error(f"[FAIL] Erreur lors de la sauvegarde chiffree vers '{config_file}': {e}", exc_info=True)
++        file_ops_logger.error(f"[FAIL] Erreur lors de la sauvegarde chiffrée vers '{config_file}': {e}", exc_info=True)
+         return False
+ 
  file_ops_logger.info("Fonctions d'opérations sur fichiers UI définies.")
diff --cc tests/mocks/jpype_setup.py
index a615299e,768af7e9..0292eb79
--- a/tests/mocks/jpype_setup.py
+++ b/tests/mocks/jpype_setup.py
@@@ -1,382 -7,124 +7,228 @@@ et de la JVM, permettant de basculer en
  import sys
  import os
  import pytest
- from unittest.mock import MagicMock
+ from unittest.mock import MagicMock, patch
  import importlib.util
- from argumentation_analysis.core.jvm_setup import shutdown_jvm_if_needed
+ from argumentation_analysis.core.jvm_setup import shutdown_jvm_if_needed, start_jvm_if_needed, is_jvm_started
  import logging
  
- # --- Configuration du Logger ---
+ # Configuration du logging
  logger = logging.getLogger(__name__)
- # Configuration basique si le logger n'est pas déjà configuré par pytest ou autre
- if not logger.handlers:
-     handler = logging.StreamHandler(sys.stdout)
-     formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
-     handler.setFormatter(formatter)
-     logger.addHandler(handler)
-     logger.setLevel(logging.INFO) # Ou logging.DEBUG pour plus de détails
-     logger.propagate = False
  
- # --- Détermination de la disponibilité du vrai JPype via variable d'environnement ---
- # Cette variable est utilisée par les décorateurs skipif dans les fichiers de test.
- logger.info(f"jpype_setup.py: Évaluation de _REAL_JPYPE_AVAILABLE...")
- logger.info(f"jpype_setup.py: Valeur brute de os.environ.get('USE_REAL_JPYPE', 'false'): '{os.environ.get('USE_REAL_JPYPE', 'false')}'")
- _REAL_JPYPE_AVAILABLE = os.environ.get('USE_REAL_JPYPE', 'false').lower() in ('true', '1')
- logger.info(f"jpype_setup.py: _REAL_JPYPE_AVAILABLE évalué à: {_REAL_JPYPE_AVAILABLE}")
- # Les prints de débogage précédents ont confirmé que _REAL_JPYPE_AVAILABLE est correctement évalué.
- # La cause du skip était une erreur dans la fixture integration_jvm (chemin des libs).
+ # --- Configuration Globale ---
  
+ _use_real_jpype = os.environ.get('USE_REAL_JPYPE', 'false').lower() in ('true', '1')
+ _jpype_mock = None
+ _jpype_patcher = None
  
 -# --- Fonctions de Hook Pytest (si utilisées directement dans ce module) ---
 -
 -def pytest_sessionstart(session):
 -    """
 -    Démarre la JVM si USE_REAL_JPYPE est activé, sinon prépare le mock.
 -    """
 -    global _jpype_mock, _jpype_patcher
 +# --- Sauvegarde du module JPype potentiellement pré-importé ou import frais ---
 +_REAL_JPYPE_MODULE = None
 +_PRE_EXISTING_JPYPE_IN_SYS_MODULES = sys.modules.get('jpype')
 +
 +if _PRE_EXISTING_JPYPE_IN_SYS_MODULES:
 +    _REAL_JPYPE_MODULE = _PRE_EXISTING_JPYPE_IN_SYS_MODULES
 +    logger.info(f"jpype_setup.py: _REAL_JPYPE_MODULE initialisé à partir de _PRE_EXISTING_JPYPE_IN_SYS_MODULES (ID: {id(_REAL_JPYPE_MODULE)}).")
 +else:
 +    logger.info("jpype_setup.py: JPype non préchargé, tentative d'import frais.")
 +    try:
 +        import jpype as r_jpype_fresh_import
 +        _REAL_JPYPE_MODULE = r_jpype_fresh_import
 +        logger.info(f"jpype_setup.py: Vrai module JPype importé fraîchement (ID: {id(_REAL_JPYPE_MODULE)}).")
 +    except ImportError as e_fresh_import:
 +        logger.warning(f"jpype_setup.py: Le vrai module JPype n'a pas pu être importé fraîchement: {e_fresh_import}")
 +        _REAL_JPYPE_MODULE = None
 +    except NameError as e_name_error_fresh_import:
 +        logger.error(f"jpype_setup.py: NameError lors de l'import frais de JPype: {e_name_error_fresh_import}.")
 +        _REAL_JPYPE_MODULE = None
 +
 +if _REAL_JPYPE_MODULE is None:
 +    logger.error("jpype_setup.py: _REAL_JPYPE_MODULE EST NONE après la tentative d'initialisation.")
 +else:
 +    logger.info(f"jpype_setup.py: _REAL_JPYPE_MODULE est initialisé (ID: {id(_REAL_JPYPE_MODULE)}) avant la définition des fixtures.")
 +# Initialisation des globales pour les objets mock, au cas où ils ne seraient pas créés
 +_JPYPE_MODULE_MOCK_OBJ_GLOBAL = None
 +_MOCK_DOT_JPYPE_MODULE_GLOBAL = None
 +
 +# --- Mock JPype ---
++_REAL_JPYPE_AVAILABLE = _REAL_JPYPE_MODULE is not None
++
 +if not _REAL_JPYPE_AVAILABLE:
 +    try:
 +        import jpype_mock # Importer le module directement
 +        # Importer le vrai module mock d'imports depuis le sous-package jpype_components
 +        from jpype_components.imports import imports as actual_mock_jpype_imports_module
 +
 +        jpype_module_mock_obj = MagicMock(name="jpype_module_mock")
 +        jpype_module_mock_obj.__path__ = []
 +        jpype_module_mock_obj.isJVMStarted = jpype_mock.isJVMStarted
 +        jpype_module_mock_obj.startJVM = jpype_mock.startJVM
 +        jpype_module_mock_obj.getJVMPath = jpype_mock.getJVMPath
 +        jpype_module_mock_obj.getJVMVersion = jpype_mock.getJVMVersion
 +        jpype_module_mock_obj.getDefaultJVMPath = jpype_mock.getDefaultJVMPath
 +        jpype_module_mock_obj.JClass = jpype_mock.JClass
 +        jpype_module_mock_obj.JException = jpype_mock.JException
 +        jpype_module_mock_obj.JObject = jpype_mock.JObject
 +        jpype_module_mock_obj.JVMNotFoundException = jpype_mock.JVMNotFoundException
 +        jpype_module_mock_obj.__version__ = '1.4.1.mock' # ou jpype_mock.__version__ si défini
 +        jpype_module_mock_obj.imports = actual_mock_jpype_imports_module
 +        _JPYPE_MODULE_MOCK_OBJ_GLOBAL = jpype_module_mock_obj
 +        _MOCK_DOT_JPYPE_MODULE_GLOBAL = jpype_mock._jpype # Accéder à _jpype depuis le module jpype_mock importé
 +        logger.info("jpype_setup.py: Mock JPype préparé.")
 +    except ImportError as e_jpype:
 +        logger.error(f"jpype_setup.py: ERREUR CRITIQUE lors de l'import de jpype_mock ou ses composants: {e_jpype}. Utilisation de mocks de fallback pour JPype.")
 +        _fb_jpype_mock = MagicMock(name="jpype_fallback_mock")
 +        _fb_jpype_mock.imports = MagicMock(name="jpype.imports_fallback_mock")
 +        _fb_dot_jpype_mock = MagicMock(name="_jpype_fallback_mock")
 +
 +        _JPYPE_MODULE_MOCK_OBJ_GLOBAL = _fb_jpype_mock
 +        _MOCK_DOT_JPYPE_MODULE_GLOBAL = _fb_dot_jpype_mock
 +        logger.info("jpype_setup.py: Mock JPype de FALLBACK préparé et assigné aux variables globales de mock.")
 +
 +
 +@pytest.fixture(scope="function", autouse=True)
 +def activate_jpype_mock_if_needed(request):
 +    global _JPYPE_MODULE_MOCK_OBJ_GLOBAL, _MOCK_DOT_JPYPE_MODULE_GLOBAL, _REAL_JPYPE_MODULE
 +
 +    # Déterminer si le vrai JPype doit être utilisé
 +    env_use_real_jpype = os.environ.get('USE_REAL_JPYPE', 'false').lower() in ('true', '1')
      
 -    if _use_real_jpype:
 -        logger.info("Démarrage de la JVM pour la session de test.")
 -        try:
 -            start_jvm_if_needed()
 -        except Exception as e:
 -            logger.error(f"Erreur lors du démarrage de la JVM: {e}")
 -            pytest.exit(f"Impossible de démarrer la JVM: {e}")
 -    else:
 -        logger.info("Configuration du mock JPype pour la session de test.")
 -        _jpype_mock = MagicMock()
 +    use_real_jpype_marker = False
 +    if request.node.get_closest_marker("real_jpype"):
 +        use_real_jpype_marker = True
          
 -        # Simuler les comportements de base du module jpype
 -        _jpype_mock.isJVMStarted.return_value = True
 -        _jpype_mock.JClass.return_value = MagicMock()
 -        _jpype_mock.JException = Exception # Simuler les exceptions Java
 +    use_real_jpype_path = False
 +    path_str = str(request.node.fspath).replace(os.sep, '/')
 +    if 'tests/integration/' in path_str or 'tests/minimal_jpype_tweety_tests/' in path_str:
 +        use_real_jpype_path = True
          
 -        # Patcher le module jpype dans le système
 -        _jpype_patcher = patch('sys.modules', {**sys.modules, 'jpype': _jpype_mock})
 -        _jpype_patcher.start()
 -
 -
 -def pytest_sessionfinish(session, exitstatus):
 -    """
 -    Arrête la JVM si elle a été démarrée pour les tests.
 -    """
 -    global _jpype_patcher
 -    
 -    if _use_real_jpype:
 -        logger.info("Arrêt de la JVM à la fin de la session de test.")
 -        shutdown_jvm_if_needed()
 -    elif _jpype_patcher:
 -        logger.info("Arrêt du patcher JPype.")
 -        _jpype_patcher.stop()
 -
 -# --- Fixtures Pytest ---
 -
 -@pytest.fixture(scope="session")
 -def real_jpype_fixture():
 -    """
 -    Fixture qui gère le cycle de vie de la vraie JVM pour toute la session de test.
 -    Ne fait rien si USE_REAL_JPYPE est désactivé.
 -    """
 -    if not _use_real_jpype:
 -        yield None
 -        return
 -
 -    # La gestion est faite par les hooks pytest_sessionstart/finish
 -    yield
 -    
 -
 -@pytest.fixture(scope="function")
 -def mock_jpype(request):
 -    """
 -    Fixture pour mocker JPype au niveau de la fonction, permettant une configuration
 -    spécifique au test.
 -    """
 -    if _use_real_jpype:
 -        pytest.skip("Test incompatible avec la vraie JVM. Utilise un mock.")
 -
 -    start_path = 'sys.modules'
 -    
 -    # Création du mock
 -    jpype_mock = MagicMock()
 -    jpype_mock.isJVMStarted.return_value = True
 -    jpype_mock.JClass.return_value = MagicMock()
 -    
 -    # Patcher le module jpype
 -    patcher = patch.dict(start_path, {'jpype': jpype_mock})
 -    patcher.start()
 -    
 -    yield jpype_mock
 -    
 -    # Nettoyage après le test
 -    patcher.stop()
 -
 -
 -@pytest.fixture(scope="session", autouse=True)
 -def manage_global_jpype_mock():
 -    """
 -    Fixture auto-utilisée pour gérer le mock JPype global pour la session.
 -    """
 -    if _use_real_jpype:
 +    final_use_real_jpype = False
 +    if env_use_real_jpype:
 +        final_use_real_jpype = True
 +        logger.info(f"Test {request.node.name}: REAL JPype forcé par la variable d'environnement USE_REAL_JPYPE.")
 +    elif use_real_jpype_marker:
 +        final_use_real_jpype = True
 +        logger.info(f"Test {request.node.name}: REAL JPype demandé par le marqueur 'real_jpype'.")
 +    elif use_real_jpype_path:
 +        final_use_real_jpype = True
 +        logger.info(f"Test {request.node.name}: REAL JPype activé par chemin ({path_str}).")
 +    # else: final_use_real_jpype reste False
 +
 +    if final_use_real_jpype:
 +        logger.info(f"Test {request.node.name} demande REAL JPype. Configuration de sys.modules pour utiliser le vrai JPype.")
 +        if _REAL_JPYPE_MODULE:
 +            sys.modules['jpype'] = _REAL_JPYPE_MODULE
 +            if hasattr(_REAL_JPYPE_MODULE, '_jpype'):
 +                sys.modules['_jpype'] = _REAL_JPYPE_MODULE._jpype
 +            elif '_jpype' in sys.modules and sys.modules.get('_jpype') is not getattr(_REAL_JPYPE_MODULE, '_jpype', None) :
 +                del sys.modules['_jpype']
 +            if hasattr(_REAL_JPYPE_MODULE, 'imports'):
 +                sys.modules['jpype.imports'] = _REAL_JPYPE_MODULE.imports
 +            elif 'jpype.imports' in sys.modules and sys.modules.get('jpype.imports') is not getattr(_REAL_JPYPE_MODULE, 'imports', None):
 +                del sys.modules['jpype.imports']
 +            logger.debug(f"REAL JPype (ID: {id(_REAL_JPYPE_MODULE)}) est maintenant sys.modules['jpype'].")
 +        else:
 +            logger.error(f"Test {request.node.name} demande REAL JPype, mais _REAL_JPYPE_MODULE n'est pas disponible. Test échouera probablement.")
          yield
 -        return
 -
 -    # Création et configuration du mock global
 -    global_jpype_mock = MagicMock(name="GlobalJPypeMock")
 -    global_jpype_mock.isJVMStarted.return_value = True
 -    global_jpype_mock.JClass.return_value = MagicMock()
 -    global_jpype_mock.JException = Exception
 -
 -    # Patcher sys.modules pour que toute importation de 'jpype' retourne notre mock
 -    patcher = patch.dict(sys.modules, {'jpype': global_jpype_mock})
 -    patcher.start()
 -
 -    yield global_jpype_mock
 -
 -    # Nettoyage à la fin de la session
 -    patcher.stop()
 +    else:
 +        logger.info(f"Test {request.node.name} utilise MOCK JPype.")
 +        try:
 +            jpype_components_jvm_module = sys.modules.get('tests.mocks.jpype_components.jvm')
 +            if jpype_components_jvm_module:
 +                if hasattr(jpype_components_jvm_module, '_jvm_started'):
 +                    jpype_components_jvm_module._jvm_started = False
 +                if hasattr(jpype_components_jvm_module, '_jvm_path'):
 +                    jpype_components_jvm_module._jvm_path = None
 +                if hasattr(_JPYPE_MODULE_MOCK_OBJ_GLOBAL, 'config') and hasattr(_JPYPE_MODULE_MOCK_OBJ_GLOBAL.config, 'jvm_path'):
 +                    _JPYPE_MODULE_MOCK_OBJ_GLOBAL.config.jvm_path = None
 +                logger.info("État (_jvm_started, _jvm_path, config.jvm_path) du mock JPype réinitialisé pour le test.")
 +            else:
 +                logger.warning("Impossible de réinitialiser l'état du mock JPype: module 'tests.mocks.jpype_components.jvm' non trouvé.")
 +        except Exception as e_reset_mock:
 +            logger.error(f"Erreur lors de la réinitialisation de l'état du mock JPype: {e_reset_mock}")
 +
 +        original_modules = {}
 +        modules_to_handle = ['jpype', '_jpype', 'jpype._core', 'jpype.imports', 'jpype.types', 'jpype.config', 'jpype.JProxy']
 +
 +        if 'jpype.imports' in sys.modules and \
 +           hasattr(sys.modules['jpype.imports'], '_jpype') and \
 +           _MOCK_DOT_JPYPE_MODULE_GLOBAL is not None and \
 +           hasattr(_MOCK_DOT_JPYPE_MODULE_GLOBAL, 'isStarted'):
 +            if sys.modules['jpype.imports']._jpype is not _MOCK_DOT_JPYPE_MODULE_GLOBAL:
 +                if 'jpype.imports._jpype_original' not in original_modules:
 +                     original_modules['jpype.imports._jpype_original'] = sys.modules['jpype.imports']._jpype
 +                logger.debug(f"Patch direct de sys.modules['jpype.imports']._jpype avec notre mock _jpype.")
 +                sys.modules['jpype.imports']._jpype = _MOCK_DOT_JPYPE_MODULE_GLOBAL
 +            else:
 +                logger.debug("sys.modules['jpype.imports']._jpype est déjà notre mock.")
 +
 +        for module_name in modules_to_handle:
 +            if module_name in sys.modules:
 +                is_current_module_our_mock = False
 +                if module_name == 'jpype' and sys.modules[module_name] is _JPYPE_MODULE_MOCK_OBJ_GLOBAL: is_current_module_our_mock = True
 +                elif module_name in ['_jpype', 'jpype._core'] and sys.modules[module_name] is _MOCK_DOT_JPYPE_MODULE_GLOBAL: is_current_module_our_mock = True
 +                elif module_name == 'jpype.imports' and hasattr(_JPYPE_MODULE_MOCK_OBJ_GLOBAL, 'imports') and sys.modules[module_name] is _JPYPE_MODULE_MOCK_OBJ_GLOBAL.imports: is_current_module_our_mock = True
 +                elif module_name == 'jpype.config' and hasattr(_JPYPE_MODULE_MOCK_OBJ_GLOBAL, 'config') and sys.modules[module_name] is _JPYPE_MODULE_MOCK_OBJ_GLOBAL.config: is_current_module_our_mock = True
 +
 +                if not is_current_module_our_mock and module_name not in original_modules:
 +                    original_modules[module_name] = sys.modules.pop(module_name)
 +                    logger.debug(f"Supprimé et sauvegardé sys.modules['{module_name}']")
 +                elif module_name in sys.modules and is_current_module_our_mock:
 +                    del sys.modules[module_name]
 +                    logger.debug(f"Supprimé notre mock préexistant pour sys.modules['{module_name}'].")
 +                elif module_name in sys.modules:
 +                    del sys.modules[module_name]
 +                    logger.debug(f"Supprimé sys.modules['{module_name}'] (sauvegarde prioritaire existante).")
 +
 +        sys.modules['jpype'] = _JPYPE_MODULE_MOCK_OBJ_GLOBAL
 +        sys.modules['_jpype'] = _MOCK_DOT_JPYPE_MODULE_GLOBAL
 +        sys.modules['jpype._core'] = _MOCK_DOT_JPYPE_MODULE_GLOBAL
 +        if hasattr(_JPYPE_MODULE_MOCK_OBJ_GLOBAL, 'imports'):
 +            sys.modules['jpype.imports'] = _JPYPE_MODULE_MOCK_OBJ_GLOBAL.imports
 +        else:
 +            sys.modules['jpype.imports'] = MagicMock(name="jpype.imports_fallback_in_fixture")
 +
 +        if hasattr(_JPYPE_MODULE_MOCK_OBJ_GLOBAL, 'config'):
 +            sys.modules['jpype.config'] = _JPYPE_MODULE_MOCK_OBJ_GLOBAL.config
 +        else:
 +            sys.modules['jpype.config'] = MagicMock(name="jpype.config_fallback_in_fixture")
 +
 +        mock_types_module = MagicMock(name="jpype.types_mock_module_dynamic_in_fixture")
 +        for type_name in ["JString", "JArray", "JObject", "JBoolean", "JInt", "JDouble", "JLong", "JFloat", "JShort", "JByte", "JChar"]:
 +            if hasattr(_JPYPE_MODULE_MOCK_OBJ_GLOBAL, type_name):
 +                setattr(mock_types_module, type_name, getattr(_JPYPE_MODULE_MOCK_OBJ_GLOBAL, type_name))
 +            else:
 +                setattr(mock_types_module, type_name, MagicMock(name=f"Mock{type_name}_in_fixture"))
 +        sys.modules['jpype.types'] = mock_types_module
 +
 +        sys.modules['jpype.JProxy'] = MagicMock(name="jpype.JProxy_mock_module_dynamic_in_fixture")
 +        logger.debug(f"Mocks JPype (principal, _jpype/_core, imports, config, types, JProxy) mis en place.")
 +        yield
 +        logger.debug(f"Nettoyage après test {request.node.name} (utilisation du mock).")
 +
 +        if 'jpype.imports._jpype_original' in original_modules:
 +            if 'jpype.imports' in sys.modules and hasattr(sys.modules['jpype.imports'], '_jpype'):
 +                sys.modules['jpype.imports']._jpype = original_modules['jpype.imports._jpype_original']
 +                logger.debug("Restauré jpype.imports._jpype à sa valeur originale.")
 +            del original_modules['jpype.imports._jpype_original']
 +
 +        modules_we_set_up_in_fixture = ['jpype', '_jpype', 'jpype._core', 'jpype.imports', 'jpype.config', 'jpype.types', 'jpype.JProxy']
 +        for module_name in modules_we_set_up_in_fixture:
 +            current_module_in_sys = sys.modules.get(module_name)
 +            is_our_specific_mock_from_fixture = False
 +            if module_name == 'jpype' and current_module_in_sys is _JPYPE_MODULE_MOCK_OBJ_GLOBAL: is_our_specific_mock_from_fixture = True
 +            elif module_name in ['_jpype', 'jpype._core'] and current_module_in_sys is _MOCK_DOT_JPYPE_MODULE_GLOBAL: is_our_specific_mock_from_fixture = True
 +            elif module_name == 'jpype.imports' and hasattr(_JPYPE_MODULE_MOCK_OBJ_GLOBAL, 'imports') and current_module_in_sys is _JPYPE_MODULE_MOCK_OBJ_GLOBAL.imports: is_our_specific_mock_from_fixture = True
 +            elif module_name == 'jpype.config' and hasattr(_JPYPE_MODULE_MOCK_OBJ_GLOBAL, 'config') and current_module_in_sys is _JPYPE_MODULE_MOCK_OBJ_GLOBAL.config: is_our_specific_mock_from_fixture = True
 +            elif module_name == 'jpype.types' and current_module_in_sys is mock_types_module: is_our_specific_mock_from_fixture = True
 +            elif module_name == 'jpype.JProxy' and isinstance(current_module_in_sys, MagicMock) and hasattr(current_module_in_sys, 'name') and "jpype.JProxy_mock_module_dynamic_in_fixture" in current_module_in_sys.name : is_our_specific_mock_from_fixture = True
 +
 +            if is_our_specific_mock_from_fixture:
 +                if module_name in sys.modules:
 +                    del sys.modules[module_name]
 +                    logger.debug(f"Supprimé notre mock pour sys.modules['{module_name}']")
 +
 +        for module_name, original_module in original_modules.items():
 +            sys.modules[module_name] = original_module
 +            logger.debug(f"Restauré sys.modules['{module_name}'] à {original_module}")
 +
-         logger.info(f"État de JPype restauré après test {request.node.name} (utilisation du mock).")
- 
- def pytest_sessionstart(session):
-     global _REAL_JPYPE_MODULE, _JPYPE_MODULE_MOCK_OBJ_GLOBAL, logger
-     logger.info("jpype_setup.py: pytest_sessionstart hook triggered.")
-     if not hasattr(logger, 'info'):
-         import logging
-         logger = logging.getLogger(__name__)
- 
-     if _REAL_JPYPE_MODULE and _REAL_JPYPE_MODULE is not _JPYPE_MODULE_MOCK_OBJ_GLOBAL:
-         logger.info("   pytest_sessionstart: Real JPype module is available.")
-         # try:
-             # La logique de configuration de destroy_jvm et l'import de jpype.config
-             # sont maintenant gérés de manière centralisée par initialize_jvm lors du premier démarrage réel.
-             # Commenter cette section pour éviter les conflits ou les configurations prématurées.
-             # original_sys_jpype_module = sys.modules.get('jpype')
-             # if sys.modules.get('jpype') is not _REAL_JPYPE_MODULE:
-             #     sys.modules['jpype'] = _REAL_JPYPE_MODULE
-             #     logger.info("   pytest_sessionstart: Temporarily set sys.modules['jpype'] to _REAL_JPYPE_MODULE for config import.")
- 
-             # if not hasattr(_REAL_JPYPE_MODULE, 'config') or _REAL_JPYPE_MODULE.config is None:
-             #     logger.info("   pytest_sessionstart: Attempting to import jpype.config explicitly.")
-             #     import jpype.config # This might be problematic if called before JVM start or with wrong classpath context
-             
-             # if original_sys_jpype_module is not None and sys.modules.get('jpype') is not original_sys_jpype_module:
-             #     sys.modules['jpype'] = original_sys_jpype_module
-             #     logger.info("   pytest_sessionstart: Restored original sys.modules['jpype'].")
-             # elif original_sys_jpype_module is None and 'jpype' in sys.modules and sys.modules['jpype'] is _REAL_JPYPE_MODULE:
-             #     pass # It was correctly set
-             
-             # Tentative d'assurer que jpype.config est le vrai config, si possible.
-             # initialize_jvm s'occupera de mettre destroy_jvm à False.
-             # Bloc try/except correctement indenté :
-         try:
-             if not hasattr(_REAL_JPYPE_MODULE, 'config') or _REAL_JPYPE_MODULE.config is None:
-                 logger.info("   pytest_sessionstart: _REAL_JPYPE_MODULE.config non trouvé, tentative d'import de jpype.config.")
-                 _current_sys_jpype = sys.modules.get('jpype')
-                 sys.modules['jpype'] = _REAL_JPYPE_MODULE
-                 import jpype.config
-                 sys.modules['jpype'] = _current_sys_jpype
-                 logger.info(f"   pytest_sessionstart: Import de jpype.config tenté. hasattr(config): {hasattr(_REAL_JPYPE_MODULE, 'config')}")
- 
-             if hasattr(_REAL_JPYPE_MODULE, 'config') and _REAL_JPYPE_MODULE.config is not None:
-                 if 'jpype.config' not in sys.modules or sys.modules.get('jpype.config') is not _REAL_JPYPE_MODULE.config:
-                     sys.modules['jpype.config'] = _REAL_JPYPE_MODULE.config
-                     logger.info("   pytest_sessionstart: Assuré que sys.modules['jpype.config'] est _REAL_JPYPE_MODULE.config.")
-             else:
-                 logger.warning("   pytest_sessionstart: _REAL_JPYPE_MODULE.config toujours non disponible après tentative d'import.")
- 
-         except ImportError as e_cfg_imp_sess_start:
-             logger.error(f"   pytest_sessionstart: ImportError lors de la tentative d'import de jpype.config: {e_cfg_imp_sess_start}")
-         except Exception as e_sess_start_cfg:
-             logger.error(f"   pytest_sessionstart: Erreur inattendue lors de la manipulation de jpype.config: {e_sess_start_cfg}", exc_info=True)
- 
-         logger.info("   pytest_sessionstart: La configuration de jpype.config.destroy_jvm est gérée par initialize_jvm.")
-     elif _JPYPE_MODULE_MOCK_OBJ_GLOBAL and _REAL_JPYPE_MODULE is _JPYPE_MODULE_MOCK_OBJ_GLOBAL:
-         logger.info("   pytest_sessionstart: JPype module is the MOCK. No changes to destroy_jvm needed for the mock.")
-     else:
-         logger.info("   pytest_sessionstart: Real JPype module not definitively available or identified as mock. La configuration de jpype.config est gérée par initialize_jvm.")
- 
- def pytest_sessionfinish(session, exitstatus):
-     global _REAL_JPYPE_MODULE, _JPYPE_MODULE_MOCK_OBJ_GLOBAL, logger
-     logger.info(f"jpype_setup.py: pytest_sessionfinish hook triggered. Exit status: {exitstatus}")
- 
-     # Déterminer si le vrai JPype a été utilisé pour la session ou le dernier test
-     # Cela est une heuristique. Idéalement, on saurait si la JVM a été démarrée par notre code.
-     real_jpype_was_potentially_used = False
-     if _REAL_JPYPE_MODULE and sys.modules.get('jpype') is _REAL_JPYPE_MODULE:
-         logger.info("   pytest_sessionfinish: sys.modules['jpype'] IS _REAL_JPYPE_MODULE. Le vrai JPype a potentiellement été utilisé.")
-         real_jpype_was_potentially_used = True
-     elif _REAL_JPYPE_MODULE and _REAL_JPYPE_MODULE is not _JPYPE_MODULE_MOCK_OBJ_GLOBAL:
-         logger.info("   pytest_sessionfinish: _REAL_JPYPE_MODULE est disponible et n'est pas le mock global. Le vrai JPype a potentiellement été utilisé.")
-         real_jpype_was_potentially_used = True
-     else:
-         logger.info("   pytest_sessionfinish: sys.modules['jpype'] n'est pas _REAL_JPYPE_MODULE ou _REAL_JPYPE_MODULE est le mock. Le mock JPype a probablement été utilisé.")
- 
-     if real_jpype_was_potentially_used:
-         logger.info("   pytest_sessionfinish: Tentative d'arrêt de la JVM via shutdown_jvm_if_needed() car le vrai JPype a potentiellement été utilisé.")
-         try:
-             # S'assurer que le vrai jpype est dans sys.modules pour que shutdown_jvm_if_needed fonctionne correctement
-             original_jpype_in_sys = sys.modules.get('jpype')
-             if original_jpype_in_sys is not _REAL_JPYPE_MODULE and _REAL_JPYPE_MODULE is not None:
-                 logger.info(f"   pytest_sessionfinish: Temporairement, sys.modules['jpype'] = _REAL_JPYPE_MODULE (ID: {id(_REAL_JPYPE_MODULE)}) pour shutdown.")
-                 sys.modules['jpype'] = _REAL_JPYPE_MODULE
-             
-             shutdown_jvm_if_needed() # Appel de notre fonction centralisée
-             
-             # Restaurer l'état précédent de sys.modules['jpype'] si modifié
-             if original_jpype_in_sys is not None and sys.modules.get('jpype') is not original_jpype_in_sys:
-                 logger.info(f"   pytest_sessionfinish: Restauration de sys.modules['jpype'] à son état original (ID: {id(original_jpype_in_sys)}).")
-                 sys.modules['jpype'] = original_jpype_in_sys
-             elif original_jpype_in_sys is None and 'jpype' in sys.modules: # Si on l'a ajouté et qu'il n'y était pas
-                 del sys.modules['jpype']
-                 logger.info("   pytest_sessionfinish: sys.modules['jpype'] supprimé car il n'était pas là initialement.")
- 
-         except Exception as e_shutdown:
-             logger.error(f"   pytest_sessionfinish: Erreur lors de l'appel à shutdown_jvm_if_needed(): {e_shutdown}", exc_info=True)
-         
-         # La logique ci-dessous pour restaurer sys.modules['jpype'] et sys.modules['jpype.config']
-         # est importante si la JVM n'est PAS arrêtée par JPype via atexit (destroy_jvm=False).
-         # Si shutdown_jvm_if_needed() a bien arrêté la JVM, cette partie est moins critique mais ne fait pas de mal.
-         logger.info("   pytest_sessionfinish: Vérification de l'état de la JVM après tentative d'arrêt.")
-         try:
-             jvm_still_started_after_shutdown_attempt = False
-             if _REAL_JPYPE_MODULE and hasattr(_REAL_JPYPE_MODULE, 'isJVMStarted'):
-                  # Assurer que _REAL_JPYPE_MODULE est utilisé pour la vérification
-                 _current_jpype_for_check = sys.modules.get('jpype')
-                 if _current_jpype_for_check is not _REAL_JPYPE_MODULE and _REAL_JPYPE_MODULE is not None:
-                     sys.modules['jpype'] = _REAL_JPYPE_MODULE
-                 jvm_still_started_after_shutdown_attempt = _REAL_JPYPE_MODULE.isJVMStarted()
-                 if _current_jpype_for_check is not None and _current_jpype_for_check is not _REAL_JPYPE_MODULE: # restaurer
-                     sys.modules['jpype'] = _current_jpype_for_check
-                 elif _current_jpype_for_check is None and 'jpype' in sys.modules and sys.modules['jpype'] is _REAL_JPYPE_MODULE:
-                     del sys.modules['jpype']
- 
- 
-             logger.info(f"   pytest_sessionfinish: JVM encore démarrée après tentative d'arrêt: {jvm_still_started_after_shutdown_attempt}")
- 
-             destroy_jvm_is_false = False # Valeur par défaut si config non accessible
-             if _REAL_JPYPE_MODULE and hasattr(_REAL_JPYPE_MODULE, 'config') and _REAL_JPYPE_MODULE.config is not None and hasattr(_REAL_JPYPE_MODULE.config, 'destroy_jvm'):
-                 destroy_jvm_is_false = not _REAL_JPYPE_MODULE.config.destroy_jvm
-             logger.info(f"   pytest_sessionfinish: destroy_jvm est False (selon config): {destroy_jvm_is_false}")
- 
-             if jvm_still_started_after_shutdown_attempt and destroy_jvm_is_false:
-                 logger.info("   pytest_sessionfinish: JVM est toujours active et destroy_jvm est False. Assurer la présence des modules jpype pour atexit.")
-                 current_sys_jpype = sys.modules.get('jpype')
-                 if current_sys_jpype is not _REAL_JPYPE_MODULE and _REAL_JPYPE_MODULE is not None:
-                     logger.warning(f"   pytest_sessionfinish: sys.modules['jpype'] (ID: {id(current_sys_jpype)}) n'est pas _REAL_JPYPE_MODULE (ID: {id(_REAL_JPYPE_MODULE)}). Restauration de _REAL_JPYPE_MODULE.")
-                     sys.modules['jpype'] = _REAL_JPYPE_MODULE
-                 
-                 if _REAL_JPYPE_MODULE and hasattr(_REAL_JPYPE_MODULE, 'config') and _REAL_JPYPE_MODULE.config is not None:
-                     current_sys_jpype_config = sys.modules.get('jpype.config')
-                     if current_sys_jpype_config is not _REAL_JPYPE_MODULE.config:
-                         logger.warning(f"   pytest_sessionfinish: sys.modules['jpype.config'] (ID: {id(current_sys_jpype_config)}) n'est pas _REAL_JPYPE_MODULE.config (ID: {id(_REAL_JPYPE_MODULE.config)}). Restauration.")
-                         sys.modules['jpype.config'] = _REAL_JPYPE_MODULE.config
-                 else:
-                     logger.warning("   pytest_sessionfinish: _REAL_JPYPE_MODULE.config non disponible, ne peut pas assurer sys.modules['jpype.config'].")
-             else:
-                 logger.info("   pytest_sessionfinish: JVM non démarrée ou destroy_jvm est True. Pas de gestion spéciale de sys.modules pour atexit depuis ici.")
-         except AttributeError as ae:
-              logger.error(f"   pytest_sessionfinish: AttributeError (vérification post-arrêt): {ae}.", exc_info=True)
-         except Exception as e:
-             logger.error(f"   pytest_sessionfinish: Erreur inattendue (vérification post-arrêt): {type(e).__name__}: {e}", exc_info=True)
-     else:
-         logger.info("   pytest_sessionfinish: Le mock JPype a probablement été utilisé. Aucun arrêt de JVM nécessaire depuis ici.")
++        logger.info(f"État de JPype restauré après test {request.node.name} (utilisation du mock).")
diff --cc tests/ui/test_utils.py
index 2811029c,86b0da3d..01c49fb4
--- a/tests/ui/test_utils.py
+++ b/tests/ui/test_utils.py
@@@ -36,7 -36,7 +36,7 @@@ import base64 # Ajouté pour la fixtur
  def mock_logger():
      # Crée un mock partagé
      shared_mock_log = MagicMock()
--    
++
      # Liste des patchers à appliquer
      patchers = [
          patch('argumentation_analysis.ui.utils.utils_logger', shared_mock_log),
@@@ -45,13 -45,13 +45,13 @@@
          patch('argumentation_analysis.ui.fetch_utils.fetch_logger', shared_mock_log), # Nouveau
          patch('argumentation_analysis.ui.cache_utils.cache_logger', shared_mock_log)  # Nouveau
      ]
--    
++
      # Démarrer tous les patchers
      for p in patchers:
          p.start()
--    
++
      yield shared_mock_log # Le mock partagé est utilisé pour les assertions
--    
++
      # Arrêter tous les patchers
      for p in patchers:
          p.stop()
@@@ -250,7 -241,7 +250,7 @@@ def test_get_full_text_fetch_error
      # Le message de log réel est "Erreur de connexion lors de la récupération de '{url}' ({source_name}, méthode: {fetch_method}): {e}"
      expected_log_message_part_url = aa_utils.reconstruct_url(sample_source_info_direct['schema'], sample_source_info_direct['host_parts'], sample_source_info_direct['path'])
      expected_log_message_part_source = sample_source_info_direct['source_name']
--    
++
      error_found = False
      for call_args_tuple in mock_logger.error.call_args_list:
          logged_message = call_args_tuple[0][0] # Premier argument positionnel
@@@ -322,20 -312,20 +322,20 @@@ def test_save_extract_definitions_embed
  
      assert success is True
      assert config_file_path.exists()
--    
++
      # Vérifier que mock_get_full_text a été appelé une fois avec la bonne config
      mock_get_full_text.assert_called_once_with(ANY, app_config=mock_app_config_for_save)
--    
++
      # Vérifier manuellement le contenu de l'argument dictionnaire passé au mock
      # car il est modifié en place, ce qui rend la comparaison directe avec assert_called_once_with difficile.
      actual_call_arg_dict = mock_get_full_text.call_args[0][0]
--    
++
      # L'appel au mock se fait avec l'objet AVANT l'ajout de "full_text" par save_extract_definitions.
      # Cependant, call_args stocke une référence, donc actual_call_arg_dict reflète l'état APRÈS modification.
      expected_dict_state_after_modification = sample_definitions[1].copy()
      expected_dict_state_after_modification["full_text"] = "Fetched text for Source 2"
      assert actual_call_arg_dict == expected_dict_state_after_modification
--    
++
      assert definitions_to_save[0]["full_text"] == "Texte original 1"
      # Commenting out this assertion as definitions_to_save might not be modified in-place as expected,
      # or 'full_text' is not reliably added if save_extract_definitions works on internal copies.
@@@ -404,9 -394,13 +404,9 @@@ def test_save_extract_definitions_encry
          sample_definitions, config_file_path, test_key, embed_full_text=True, config=mock_app_config_for_save
      )
      mock_encrypt_data_with_fernet_in_file_ops.assert_called_once()
 -    assert success is False # Car mock_encrypt_data_with_fernet retourne None
 -    # encrypt_data_with_fernet loggue déjà, mais save_extract_definitions loggue aussi l'erreur globale
 -    # Le message exact peut varier si encrypt_data_with_fernet retourne None sans exception spécifique attrapée par save_extract_definitions
 -    # On s'attend à ce que save_extract_definitions logue un échec.
 -    # Le message exact inclura la ValueError levée.
 +    assert success is False
      expected_error_message_part = f"❌ Erreur lors de la sauvegarde chiffrée vers '{config_file_path}': Échec du chiffrement des données (encrypt_data_with_fernet a retourné None)."
--    
++
      error_call_found = False
      for call_args_tuple in mock_logger.error.call_args_list:
          args = call_args_tuple[0] # Les arguments positionnels
@@@ -430,16 -424,23 +430,16 @@@ def test_save_extract_definitions_embed
      success = save_extract_definitions(
          definitions_to_save, config_file_path, test_key, embed_full_text=True, config=mock_app_config_for_save
      )
 -    assert success is True # La sauvegarde doit réussir même si la récupération de texte échoue pour une source
 +    assert success is True
  
 -    # Vérifier que get_full_text_for_source a été appelé pour la source sans texte
      mock_get_full_text.assert_called_once_with(ANY, app_config=mock_app_config_for_save)
 -
 -    # Vérifier manuellement l'argument passé au mock, car il est modifié en place.
      actual_call_arg_dict = mock_get_full_text.call_args[0][0]
--    
 -    # Construire l'état attendu de l'argument APRÈS la tentative de fetch et l'ajout de full_text = None
 -    # definitions_to_save[1] est l'état avant l'appel à save_extract_definitions,
 -    # et il a déjà eu "full_text" supprimé si présent.
++
      expected_dict_after_failed_fetch = definitions_to_save[1].copy()
 -    expected_dict_after_failed_fetch["full_text"] = None # Car le fetch échoue et la clé est mise à None
 -    
 +    expected_dict_after_failed_fetch["full_text"] = None
-     
++
      assert actual_call_arg_dict == expected_dict_after_failed_fetch
--    
++
      # Vérifier que le logger a été appelé avec le message d'erreur de connexion
      mock_logger.warning.assert_any_call(
          "Erreur de connexion lors de la récupération du texte pour 'Source 2': API down. Champ 'full_text' non peuplé."
@@@ -471,8 -481,9 +471,8 @@@ def test_load_extract_definitions_no_ke
      assert definitions_no_file == [{"default": True}]
      mock_logger.info.assert_any_call(f"Fichier config '{config_file_path}' non trouvé. Utilisation définitions par défaut.")
  
 -    # Maintenant, créer un fichier avec du contenu non-JSON et vérifier JSONDecodeError
      config_file_path.write_text("dummy non-json content for key test")
--    
++
      with patch('argumentation_analysis.ui.file_operations.ui_config_module.EXTRACT_SOURCES', None), \
           patch('argumentation_analysis.ui.file_operations.ui_config_module.DEFAULT_EXTRACT_SOURCES', [{"default_key_test_2": True}]):
          with pytest.raises(json.JSONDecodeError):
@@@ -486,17 -498,23 +486,18 @@@
              break
      assert error_call_found, "Le message d'erreur de décodage JSON attendu n'a pas été loggué."
  
 -# Patches pour les dépendances de load_extract_definitions
 - # Cible corrigée
 -def test_load_extract_definitions_decryption_fails(mock_decrypt_data_with_fernet_in_file_ops, config_file_path, test_key, mock_logger): # mock_decrypt renommé
 +@patch('argumentation_analysis.ui.file_operations.decrypt_data_with_fernet', side_effect=InvalidToken("Test InvalidToken from mock"))
 +def test_load_extract_definitions_decryption_fails(mock_decrypt_data_with_fernet_in_file_ops, config_file_path, test_key, mock_logger):
      config_file_path.write_text("dummy encrypted data")
 -    # b64_key_str = test_key.decode('utf-8') # test_key est déjà une str
      with patch('argumentation_analysis.ui.file_operations.ui_config_module.EXTRACT_SOURCES', None), \
           patch('argumentation_analysis.ui.file_operations.ui_config_module.DEFAULT_EXTRACT_SOURCES', [{"default": True}]):
 -        # load_extract_definitions ne relance plus InvalidToken, elle logue et retourne les définitions par défaut
 -        definitions = load_extract_definitions(config_file_path, test_key) # Utiliser test_key directement
 +        definitions = load_extract_definitions(config_file_path, test_key)
          assert definitions == [{"default": True}]
  
 -    # Vérifier que le logger a été appelé avec un message d'erreur approprié
 -    # Le message vient de load_extract_definitions quand InvalidToken est attrapée
      error_logged = False
-     expected_log_part = f"❌ InvalidToken explicitement levée lors du déchiffrement de '{config_file_path}'"
-     for call_args_tuple in mock_logger.error.call_args_list:
+     # Le message exact loggué par load_extract_definitions pour InvalidToken attrapée
+     expected_log_part = f"❌ Token invalide (InvalidToken) lors du déchiffrement de '{config_file_path}'"
+     for call_args_tuple in mock_logger.error.call_args_list: # C'est une erreur maintenant
          args, kwargs = call_args_tuple
          if args and isinstance(args[0], str) and expected_log_part in args[0] and kwargs.get('exc_info') is True:
              error_logged = True
@@@ -511,10 -528,10 +512,10 @@@ def test_load_extract_definitions_decom
      expected_default_defs = [{"default_decomp_fail": True}]
      with patch('argumentation_analysis.ui.file_operations.ui_config_module.EXTRACT_SOURCES', None), \
           patch('argumentation_analysis.ui.file_operations.ui_config_module.DEFAULT_EXTRACT_SOURCES', expected_default_defs):
--        
 -        definitions = load_extract_definitions(config_file_path, test_key) # Utiliser test_key directement
++
 +        definitions = load_extract_definitions(config_file_path, test_key)
          assert definitions == expected_default_defs
--    
++
      error_logged = False
      for call_args_tuple in mock_logger.error.call_args_list:
          logged_message = call_args_tuple[0][0]
@@@ -533,10 -552,10 +534,10 @@@ def test_load_extract_definitions_inval
      expected_default_defs = [{"default_invalid_json": True}]
      with patch('argumentation_analysis.ui.file_operations.ui_config_module.EXTRACT_SOURCES', None), \
           patch('argumentation_analysis.ui.file_operations.ui_config_module.DEFAULT_EXTRACT_SOURCES', expected_default_defs):
--        
 -        definitions = load_extract_definitions(config_file_path, test_key) # Utiliser test_key directement
++
 +        definitions = load_extract_definitions(config_file_path, test_key)
          assert definitions == expected_default_defs
--            
++
      error_logged = False
      for call_args_tuple in mock_logger.error.call_args_list:
          logged_message = call_args_tuple[0][0]
@@@ -556,10 -577,10 +557,10 @@@ def test_load_extract_definitions_inval
      expected_default_defs = [{"default_invalid_format": True}]
      with patch('argumentation_analysis.ui.file_operations.ui_config_module.EXTRACT_SOURCES', None), \
           patch('argumentation_analysis.ui.file_operations.ui_config_module.DEFAULT_EXTRACT_SOURCES', expected_default_defs):
--        
 -        definitions = load_extract_definitions(config_file_path, test_key) # Utiliser test_key directement
++
 +        definitions = load_extract_definitions(config_file_path, test_key)
          assert definitions == expected_default_defs
--            
++
      warning_logged = False
      for call_args_tuple in mock_logger.warning.call_args_list:
          logged_message = call_args_tuple[0][0]
@@@ -655,11 -679,16 +656,11 @@@ def test_decrypt_data_no_key(mock_logge
      assert decrypt_data_with_fernet(b"encrypted", None) is None
      mock_logger.error.assert_any_call("Erreur déchiffrement Fernet: Clé (str b64 ou bytes) manquante.")
  
 -def test_decrypt_data_invalid_token(test_key, mock_logger): # test_key est str b64
 -    # decrypt_data_with_fernet retourne None en cas d'InvalidToken et logue l'erreur.
 -    # Utiliser des données plus longues pour le test pour éviter certaines erreurs Fernet avant InvalidToken
 -    result = decrypt_data_with_fernet(b"not_really_encrypted_data_longer_than_key", test_key) 
 +def test_decrypt_data_invalid_token(test_key, mock_logger):
 +    result = decrypt_data_with_fernet(b"not_really_encrypted_data_longer_than_key", test_key)
      assert result is None
--    
 -    # Vérifier que le logger (maintenant celui de crypto_utils, mocké par mock_logger) a été appelé.
++
      error_found = False
 -    # Le message exact loggué par decrypt_data_with_fernet pour InvalidToken
      expected_log_start = "Erreur déchiffrement Fernet (InvalidToken/Signature):"
      for call_args_tuple in mock_logger.error.call_args_list:
          args, _ = call_args_tuple

==================== COMMIT: 3c96b15da5a90d1f76643c6b2f0cc7ceb700e5e5 ====================
commit 3c96b15da5a90d1f76643c6b2f0cc7ceb700e5e5
Merge: 966d11c1 ebaa5647
Author: jsboigeEpita <jean-sylvain.boige@epita.fr>
Date:   Mon Jun 16 02:51:35 2025 +0200

    Resolve merge conflicts after pull

diff --cc argumentation_analysis/core/jvm_setup.py
index 77ec0dff,6df648d3..3a0dc591
--- a/argumentation_analysis/core/jvm_setup.py
+++ b/argumentation_analysis/core/jvm_setup.py
@@@ -1,574 -1,379 +1,441 @@@
- # argumentation_analysis/core/jvm_setup.py
+ # core/jvm_setup.py
+ import os
+ import sys
  import jpype
- import jpype.imports
  import logging
- import os
- from pathlib import Path
- from typing import Optional, List
- 
  import platform
- from typing import Optional, List
+ import shutil
+ import subprocess
+ import zipfile
  import requests
+ from pathlib import Path
+ from typing import Optional, List, Dict
 -from tqdm import tqdm
 -
 -
 -# --- Configuration et Constantes ---
 -logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
 -
 -# --- Constantes de Configuration ---
 -# Répertoires (utilisant pathlib pour la robustesse multi-plateforme)
 -PROJ_ROOT = Path(__file__).resolve().parents[3]
 -LIBS_DIR = PROJ_ROOT / "libs"
 -TWEETY_VERSION = "1.24" # Mettre à jour au besoin
 -# TODO: Lire depuis un fichier de config centralisé (par ex. pyproject.toml ou un .conf)
 -# Au lieu de TWEETY_VERSION = "1.24", on pourrait avoir get_config("tweety.version")
 -
 -# Configuration des URLs des dépendances
 -TWEETY_BASE_URL = "https://repo.maven.apache.org/maven2"
 -TWEETY_ARTIFACTS: Dict[str, Dict[str, str]] = {
 -    # Core
 -    "tweety-arg": {"group": "net.sf.tweety", "version": TWEETY_VERSION},
 -    # Modules principaux (à adapter selon les besoins du projet)
 -    "tweety-lp": {"group": "net.sf.tweety.lp", "version": TWEETY_VERSION},
 -    "tweety-log": {"group": "net.sf.tweety.log", "version": TWEETY_VERSION},
 -    "tweety-math": {"group": "net.sf.tweety.math", "version": TWEETY_VERSION},
 -    # Natives (exemple ; peuvent ne pas exister pour toutes les versions)
 -    "tweety-native-maxsat": {"group": "net.sf.tweety.native", "version": TWEETY_VERSION, "classifier": f"maxsat-{platform.system().lower()}"}
 -}
 -
 -# Configuration JDK portable
 -MIN_JAVA_VERSION = 11
 -JDK_VERSION = "17.0.2" # Exemple, choisir une version LTS stable
 -JDK_BUILD = "8"
 -JDK_URL_TEMPLATE = "https://github.com/adoptium/temurin{maj_v}-binaries/releases/download/jdk-{v}%2B{b}/OpenJDK{maj_v}U-jdk_{arch}_{os}_hotspot_{v}_{b_flat}.zip"
 -# Windows: x64_windows, aarch64_windows | Linux: x64_linux, aarch64_linux | macOS: x64_mac, aarch64_mac
 -
 -# --- Fonctions Utilitaires ---
 -def get_os_arch_for_jdk() -> Dict[str, str]:
 -    """Détermine l'OS et l'architecture pour l'URL de téléchargement du JDK."""
 -    system = platform.system().lower()
 -    arch = platform.machine().lower()
 -
 -    os_map = {"windows": "windows", "linux": "linux", "darwin": "mac"}
 -    arch_map = {"amd64": "x64", "x86_64": "x64", "aarch64": "aarch64", "arm64": "aarch64"}
 -
 -    if system not in os_map:
 -        raise OSError(f"Système d'exploitation non supporté pour le JDK portable : {platform.system()}")
 -    if arch not in arch_map:
 -        raise OSError(f"Architecture non supportée pour le JDK portable : {arch}")
 -
 -    return {"os": os_map[system], "arch": arch_map[arch]}
 -
 -
 -def download_file(url: str, dest_path: Path):
 -    """Télécharge un fichier avec une barre de progression."""
 -    logging.info(f"Téléchargement de {url} vers {dest_path}...")
 -    try:
 -        response = requests.get(url, stream=True, timeout=30)
 -        response.raise_for_status()
 +from tqdm.auto import tqdm
 +import stat
- import shutil
- import zipfile
  
 -        total_size = int(response.headers.get("content-length", 0))
 -        with open(dest_path, "wb") as f, tqdm(
 -            desc=dest_path.name,
 -            total=total_size,
 -            unit="iB",
 -            unit_scale=True,
 -            unit_divisor=1024,
 -        ) as bar:
 -            for chunk in response.iter_content(chunk_size=8192):
 -                size = f.write(chunk)
 -                bar.update(size)
 -    except requests.RequestException as e:
 -        logging.error(f"Erreur de téléchargement pour {url}: {e}")
 -        if dest_path.exists():
 -            dest_path.unlink() # Nettoyer le fichier partiel
 -        raise
 -    except IOError as e:
 -        logging.error(f"Erreur d'écriture du fichier {dest_path}: {e}")
 -        if dest_path.exists():
 -            dest_path.unlink()
 -        raise
 -
 -
 -def unzip_file(zip_path: Path, dest_dir: Path):
 -    """Décompresse un fichier ZIP."""
 -    logging.info(f"Décompression de {zip_path} vers {dest_dir}...")
 -    try:
 -        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
 -            # Pour éviter les problèmes de "répertoire dans un répertoire"
 -            # On vérifie si tout le contenu est dans un seul dossier
 -            file_list = zip_ref.namelist()
 -            top_level_dirs = {Path(f).parts[0] for f in file_list}
 -            
 -            if len(top_level_dirs) == 1:
 -                 # Cas où le contenu est dans un sous-répertoire (ex: jdk-17.0.2+8/...)
 -                 # On extrait directement le contenu de ce sous-répertoire
 -                temp_extract_dir = dest_dir / "temp_extract"
 -                zip_ref.extractall(temp_extract_dir)
 -                
 -                source_dir = temp_extract_dir / top_level_dirs.pop()
 -                for item in source_dir.iterdir():
 -                    shutil.move(str(item), str(dest_dir / item.name))
 -                temp_extract_dir.rmdir() # rm -r
 -            else:
 -                 # Le contenu est déjà à la racine du zip
 -                 zip_ref.extractall(dest_dir)
 +# Configuration du logger pour ce module
 +logger = logging.getLogger("Orchestration.JPype")
  
- # --- Fonctions de téléchargement et de provisioning (issues du stash) ---
 -        zip_path.unlink() # Nettoyer l'archive
 -        logging.info("Décompression terminée.")
 -    except (zipfile.BadZipFile, IOError) as e:
 -        logging.error(f"Erreur lors de la décompression de {zip_path}: {e}")
 -        raise
++# --- Fonctions de téléchargement et de provisioning (issues du stash de HEAD) ---
  
 -# --- Fonctions de Gestion des Dépendances ---
 +class TqdmUpTo(tqdm):
 +    """Provides `update_to(block_num, block_size, total_size)`."""
 +    def update_to(self, b=1, bsize=1, tsize=None):
 +         if tsize is not None: self.total = tsize
 +         self.update(b * bsize - self.n)
 +
 +def _download_file_with_progress(file_url: str, target_path: Path, description: str):
 +    """Télécharge un fichier depuis une URL vers un chemin cible avec une barre de progression."""
 +    try:
 +        if target_path.exists() and target_path.stat().st_size > 0:
 +            logger.debug(f"Fichier '{target_path.name}' déjà présent et non vide. Skip.")
 +            return True, False
 +        logger.info(f"Tentative de téléchargement: {file_url} vers {target_path}")
 +        headers = {'User-Agent': 'Mozilla/5.0'}
 +        response = requests.get(file_url, stream=True, timeout=15, headers=headers, allow_redirects=True)
 +        if response.status_code == 404:
 +             logger.error(f"❌ Fichier non trouvé (404) à l'URL: {file_url}")
 +             return False, False
 +        response.raise_for_status()
 +        total_size = int(response.headers.get('content-length', 0))
 +        with TqdmUpTo(unit='B', unit_scale=True, unit_divisor=1024, total=total_size, miniters=1, desc=description[:40]) as t:
 +            with open(target_path, 'wb') as f:
 +                for chunk in response.iter_content(chunk_size=8192):
 +                    if chunk:
 +                        f.write(chunk)
 +                        t.update(len(chunk))
 +        if target_path.exists() and target_path.stat().st_size > 0:
 +            logger.info(f" -> Téléchargement de '{target_path.name}' réussi.")
 +            return True, True
 +        else:
 +            logger.error(f"❓ Téléchargement de '{target_path.name}' semblait terminé mais fichier vide ou absent.")
 +            if target_path.exists(): target_path.unlink(missing_ok=True)
 +            return False, False
 +    except requests.exceptions.RequestException as e:
 +        logger.error(f"❌ Échec connexion/téléchargement pour '{target_path.name}': {e}")
 +        if target_path.exists(): target_path.unlink(missing_ok=True)
 +        return False, False
 +    except Exception as e_other:
 +        logger.error(f"❌ Erreur inattendue pour '{target_path.name}': {e_other}", exc_info=True)
 +        if target_path.exists(): target_path.unlink(missing_ok=True)
 +        return False, False
++
++def get_project_root_for_libs() -> Path: # Renamed to avoid conflict if get_project_root is defined elsewhere
++    return Path(__file__).resolve().parents[3]
++
++def find_libs_dir() -> Optional[Path]:
++    proj_root_temp = get_project_root_for_libs()
++    libs_dir_temp = proj_root_temp / "libs"
++    libs_dir_temp.mkdir(parents=True, exist_ok=True)
++    return libs_dir_temp
+ 
 -# --- Fonction Principale de Téléchargement Tweety ---
  def download_tweety_jars(
 -    version: str = TWEETY_VERSION,
 -    target_dir: str = LIBS_DIR,
 +    version: str = "1.28",
 +    target_dir: str = None,
      native_subdir: str = "native"
      ) -> bool:
      """
      Vérifie et télécharge les JARs Tweety (Core + Modules) et les binaires natifs nécessaires.
 -
 -    Returns:
 -        bool: True si des téléchargements ont eu lieu, False sinon.
      """
 -    LIBS_DIR.mkdir(exist_ok=True)
 -    (LIBS_DIR / native_subdir).mkdir(exist_ok=True)
 -    
 -    downloaded = False
 -    for name, a_info in TWEETY_ARTIFACTS.items():
 -        group_path = a_info["group"].replace('.', '/')
 -        a_version = a_info["version"]
 -        
 -        jar_name_parts = [name, a_version]
 -        if "classifier" in a_info:
 -            jar_name_parts.append(a_info['classifier'])
 -
 -        jar_filename = f"{'-'.join(jar_name_parts)}.jar"
 -        jar_path = LIBS_DIR / jar_filename
 -
 -        if not jar_path.exists():
 -            downloaded = True
 -            url = f"{TWEETY_BASE_URL}/{group_path}/{name}/{a_version}/{jar_filename}"
 -            try:
 -                download_file(url, jar_path)
 -            except Exception:
 -                logging.error(f"Échec du téléchargement pour {name}. Le projet pourrait ne pas fonctionner.")
 -                return False # On arrête si un JAR critique manque
 +    if target_dir is None:
 +        target_dir_path = find_libs_dir()
 +        if not target_dir_path:
 +            logger.critical("Impossible de trouver le répertoire des bibliothèques pour y télécharger les JARs.")
 +            return False
 +    else:
 +        target_dir_path = Path(target_dir)
 +
 +    logger.info(f"\n--- Vérification/Téléchargement des JARs Tweety v{version} ---")
 +    BASE_URL = f"https://tweetyproject.org/builds/{version}/"
 +    LIB_DIR = target_dir_path
 +    NATIVE_LIBS_DIR = LIB_DIR / native_subdir
 +    LIB_DIR.mkdir(exist_ok=True)
-     NATIVE_LIBS_DIR.mkdir(exist_ok=True) 
++    NATIVE_LIBS_DIR.mkdir(exist_ok=True)
 +
 +    CORE_JAR_NAME = f"org.tweetyproject.tweety-full-{version}-with-dependencies.jar"
 +    REQUIRED_MODULES = sorted([
 +        "arg.adf", "arg.aba", "arg.bipolar", "arg.aspic", "arg.dung", "arg.weighted",
 +        "arg.social", "arg.setaf", "arg.rankings", "arg.prob", "arg.extended",
 +        "arg.delp", "arg.deductive", "arg.caf",
 +        "beliefdynamics", "agents.dialogues", "action",
 +        "logics.pl", "logics.fol", "logics.ml", "logics.dl", "logics.cl",
 +        "logics.qbf", "logics.pcl", "logics.rcl", "logics.rpcl", "logics.mln", "logics.bpm",
 +        "lp.asp",
 +        "math", "commons", "agents"
 +    ])
 +    system = platform.system()
 +    native_binaries_repo_path = "https://raw.githubusercontent.com/TweetyProjectTeam/TweetyProject/main/org-tweetyproject-arg-adf/src/main/resources/"
 +    native_binaries = {
 +        "Windows": ["picosat.dll", "lingeling.dll", "minisat.dll"],
 +        "Linux":   ["picosat.so", "lingeling.so", "minisat.so"],
 +        "Darwin":  ["picosat.dylib", "lingeling.dylib", "minisat.dylib"]
 +    }.get(system, [])
 +
 +    logger.info(f"Vérification de l'accès à {BASE_URL}...")
 +    url_accessible = False
 +    try:
 +        response = requests.head(BASE_URL, timeout=10)
 +        response.raise_for_status()
 +        logger.info(f"✔️ URL de base Tweety v{version} accessible.")
 +        url_accessible = True
 +    except requests.exceptions.RequestException as e:
 +        logger.error(f"❌ Impossible d'accéder à l'URL de base {BASE_URL}. Erreur : {e}")
 +        logger.warning("   Le téléchargement des JARs/binaires manquants échouera. Seuls les fichiers locaux seront utilisables.")
 +
 +    logger.info(f"\n--- Vérification/Téléchargement JAR Core ---")
 +    core_present, core_new = _download_file_with_progress(BASE_URL + CORE_JAR_NAME, LIB_DIR / CORE_JAR_NAME, CORE_JAR_NAME)
 +    status_core = "téléchargé" if core_new else ("déjà présent" if core_present else "MANQUANT")
 +    logger.info(f"✔️ JAR Core '{CORE_JAR_NAME}': {status_core}.")
 +    if not core_present:
 +        logger.critical(f"❌ ERREUR CRITIQUE : Le JAR core est manquant et n'a pas pu être téléchargé.")
-         return False 
++        return False
  
 -    if downloaded:
 -        logging.info("Téléchargement des bibliothèques Tweety terminé.")
 +    logger.info(f"\n--- Vérification/Téléchargement des {len(REQUIRED_MODULES)} JARs de modules ---")
 +    modules_present_count = 0
 +    modules_downloaded_count = 0
 +    modules_missing = []
-     for module in tqdm(REQUIRED_MODULES, desc="Modules JARs"):
-         module_jar_name = f"org.tweetyproject.{module}-{version}-with-dependencies.jar"
++    for module_name in tqdm(REQUIRED_MODULES, desc="Modules JARs"):
++        module_jar_name = f"org.tweetyproject.{module_name}-{version}-with-dependencies.jar"
 +        present, new_dl = _download_file_with_progress(BASE_URL + module_jar_name, LIB_DIR / module_jar_name, module_jar_name)
 +        if present:
 +            modules_present_count += 1
 +            if new_dl: modules_downloaded_count += 1
-         elif url_accessible: 
-              modules_missing.append(module)
++        elif url_accessible:
++             modules_missing.append(module_name)
 +    logger.info(f"-> Modules: {modules_downloaded_count} téléchargés, {modules_present_count}/{len(REQUIRED_MODULES)} présents.")
 +    if modules_missing:
 +        logger.warning(f"   Modules potentiellement manquants (non trouvés ou erreur DL): {', '.join(modules_missing)}")
 +
 +    logger.info(f"\n--- Vérification/Téléchargement des {len(native_binaries)} binaires natifs ({system}) ---")
 +    native_present_count = 0
 +    native_downloaded_count = 0
 +    native_missing = []
 +    if not native_binaries:
 +         logger.info(f"   (Aucun binaire natif connu pour {system})")
      else:
 -        logging.info("Toutes les bibliothèques Tweety sont déjà à jour.")
 -        
 -    return downloaded
 +        for name in tqdm(native_binaries, desc="Binaires Natifs"):
 +             present, new_dl = _download_file_with_progress(native_binaries_repo_path + name, NATIVE_LIBS_DIR / name, name)
 +             if present:
 +                 native_present_count += 1
 +                 if new_dl: native_downloaded_count += 1
 +                 if new_dl and system != "Windows":
 +                     try:
-                          target_path = NATIVE_LIBS_DIR / name
-                          current_permissions = target_path.stat().st_mode
-                          target_path.chmod(current_permissions | stat.S_IXUSR | stat.S_IXGRP | stat.S_IXOTH) 
++                         target_path_native = NATIVE_LIBS_DIR / name
++                         current_permissions = target_path_native.stat().st_mode
++                         target_path_native.chmod(current_permissions | stat.S_IXUSR | stat.S_IXGRP | stat.S_IXOTH)
 +                         logger.debug(f"      Permissions d'exécution ajoutées à {name}")
 +                     except Exception as e_chmod:
 +                         logger.warning(f"      Impossible d'ajouter les permissions d'exécution à {name}: {e_chmod}")
-              elif url_accessible: 
++             elif url_accessible:
 +                  native_missing.append(name)
 +        logger.info(f"-> Binaires natifs: {native_downloaded_count} téléchargés, {native_present_count}/{len(native_binaries)} présents.")
 +        if native_missing:
 +            logger.warning(f"   Binaires natifs potentiellement manquants: {', '.join(native_missing)}")
 +        if native_present_count > 0:
 +             logger.info(f"   Note: S'assurer que le chemin '{NATIVE_LIBS_DIR.resolve()}' est inclus dans java.library.path lors du démarrage JVM.")
 +    logger.info("--- Fin Vérification/Téléchargement Tweety ---")
 +    return core_present and modules_present_count > 0
 +
 +
 +PORTABLE_JDK_DIR_NAME = "portable_jdk"
 +PORTABLE_JDK_ZIP_NAME = "OpenJDK17U-jdk_x64_windows_hotspot_17.0.15_6_new.zip"
 +TEMP_DIR_NAME = "_temp"
 +
++def get_project_root() -> Path:
++    return Path(__file__).resolve().parents[3]
++
++
 +def _extract_portable_jdk(project_root: Path, portable_jdk_parent_dir: Path, portable_jdk_zip_path: Path) -> Optional[Path]:
 +    logger.info(f"Tentative d'extraction du JDK portable depuis '{portable_jdk_zip_path}' vers '{portable_jdk_parent_dir}'...")
 +    try:
 +        with zipfile.ZipFile(portable_jdk_zip_path, 'r') as zip_ref:
 +            zip_ref.extractall(portable_jdk_parent_dir)
 +        logger.info(f"JDK portable extrait avec succès dans '{portable_jdk_parent_dir}'.")
 +        for item in portable_jdk_parent_dir.iterdir():
 +            if item.is_dir() and item.name.startswith("jdk-"):
 +                logger.info(f"Dossier racine du JDK portable détecté : '{item}'")
 +                return item
 +        logger.warning(f"Impossible de déterminer le dossier racine du JDK dans '{portable_jdk_parent_dir}' après extraction.")
 +        extracted_items = [d for d in portable_jdk_parent_dir.iterdir() if d.is_dir()]
 +        if len(extracted_items) == 1:
 +            logger.info(f"Un seul dossier trouvé après extraction: '{extracted_items[0]}', en supposant que c'est le JDK.")
 +            return extracted_items[0]
 +        return None
 +    except Exception as e:
 +        logger.error(f"Erreur lors de l'extraction du JDK portable: {e}", exc_info=True)
 +        return None
  
++def find_jdk_path() -> Optional[Path]:
++    project_root = get_project_root()
++    _PORTABLE_JDK_PATH: Optional[Path] = None
+ 
 -# --- Fonction de détection JAVA_HOME (modifiée pour prioriser Java >= MIN_JAVA_VERSION) ---
 -def find_valid_java_home() -> Optional[str]:
 -    """
 -    Cherche un JAVA_HOME valide ou un JDK portable.
 -    1. Vérifie la variable d'environnement JAVA_HOME.
 -    2. Si invalide, cherche un JDK portable local.
 -    3. Si non trouvé, télécharge et installe un JDK portable.
 -    """
 -    # 1. Vérifier JAVA_HOME
+     java_home_env = os.environ.get("JAVA_HOME")
+     if java_home_env:
 -        logging.info(f"Variable JAVA_HOME trouvée : {java_home_env}")
 -        if is_valid_jdk(Path(java_home_env)):
 -            return java_home_env
 -
 -    # 2. Chercher un JDK portable
 -    portable_jdk_dir = PROJ_ROOT / "jdk"
 -    if portable_jdk_dir.exists() and is_valid_jdk(portable_jdk_dir):
 -        logging.info(f"JDK portable valide trouvé : {portable_jdk_dir}")
 -        return str(portable_jdk_dir)
++        logger.info(f"Variable JAVA_HOME trouvée : {java_home_env}")
++        potential_path = Path(java_home_env)
++        if potential_path.is_dir():
++            java_exe_in_java_home = potential_path / "bin" / f"java{'.exe' if os.name == 'nt' else ''}"
++            if java_exe_in_java_home.is_file():
++                logger.info(f"(OK) JDK détecté via JAVA_HOME et validé : {potential_path}")
++                return potential_path
++            else:
++                logger.warning(f"(ATTENTION) JAVA_HOME pointe vers un répertoire sans java exécutable valide: {potential_path}")
++        else:
++            logger.warning(f"(ATTENTION) JAVA_HOME défini mais répertoire inexistant : {potential_path}")
++
++    portable_jdk_root_dir_check = project_root / PORTABLE_JDK_DIR_NAME
++    if portable_jdk_root_dir_check.is_dir():
++        for item in portable_jdk_root_dir_check.iterdir():
++            if item.is_dir() and item.name.startswith("jdk-"):
++                java_exe_portable = item / "bin" / f"java{'.exe' if os.name == 'nt' else ''}"
++                if java_exe_portable.is_file():
++                    logger.info(f"(OK) JDK portable détecté via chemin par défaut : {item}")
++                    return item
++    
++    logger.warning(f"(ATTENTION) JDK portable non trouvé à l'emplacement par défaut : {portable_jdk_root_dir_check}")
++    return None
+ 
 -    # 3. Télécharger un nouveau JDK portable
 -    logging.warning("Aucun JDK valide trouvé. Tentative de téléchargement d'un JDK portable.")
 -    return download_portable_jdk(portable_jdk_dir)
+ 
 +def find_valid_java_home() -> Optional[str]:
 +    logger.debug("Début recherche répertoire Java Home valide...")
 +    
 +    project_root = get_project_root()
 +    portable_jdk_parent_dir = project_root / PORTABLE_JDK_DIR_NAME
 +    portable_jdk_zip_path = project_root / TEMP_DIR_NAME / PORTABLE_JDK_ZIP_NAME
-     PORTABLE_JDK_DOWNLOAD_URL = "https://github.com/adoptium/temurin17-binaries/releases/download/jdk-17.0.15%2B6/OpenJDK17U-jdk_x64_windows_hotspot_17.0.15_6.zip" # Du Stash
++    PORTABLE_JDK_DOWNLOAD_URL = "https://github.com/adoptium/temurin17-binaries/releases/download/jdk-17.0.15%2B6/OpenJDK17U-jdk_x64_windows_hotspot_17.0.15_6.zip"
 +    
 +    potential_jdk_root_dir = None
 +    if portable_jdk_parent_dir.is_dir():
 +        for item in portable_jdk_parent_dir.iterdir():
 +            if item.is_dir() and item.name.startswith("jdk-"):
 +                java_exe_portable = item / "bin" / f"java{'.exe' if os.name == 'nt' else ''}"
 +                if java_exe_portable.is_file():
 +                    logger.info(f"JDK portable trouvé et valide dans: '{item}'")
 +                    potential_jdk_root_dir = item
 +                    break
 +    
 +    if potential_jdk_root_dir:
 +        logger.info(f"🎉 Utilisation du JDK portable intégré: '{potential_jdk_root_dir}'")
 +        return str(potential_jdk_root_dir.resolve())
 +
 +    if portable_jdk_zip_path.is_file():
 +        extracted_jdk_root = _extract_portable_jdk(project_root, portable_jdk_parent_dir, portable_jdk_zip_path)
 +        if extracted_jdk_root and (extracted_jdk_root / "bin" / f"java{'.exe' if os.name == 'nt' else ''}").is_file():
 +            return str(extracted_jdk_root.resolve())
 +    else:
 +        logger.info(f"Archive ZIP du JDK portable non trouvée. Tentative de téléchargement...")
 +        temp_dir = project_root / TEMP_DIR_NAME
 +        temp_dir.mkdir(parents=True, exist_ok=True)
 +        jdk_downloaded, _ = _download_file_with_progress(PORTABLE_JDK_DOWNLOAD_URL, portable_jdk_zip_path, "JDK Portable")
 +        if jdk_downloaded:
 +            extracted_jdk_root = _extract_portable_jdk(project_root, portable_jdk_parent_dir, portable_jdk_zip_path)
 +            if extracted_jdk_root and (extracted_jdk_root / "bin" / f"java{'.exe' if os.name == 'nt' else ''}").is_file():
 +                return str(extracted_jdk_root.resolve())
 +
-     # Si le portable échoue, on revient à la logique de HEAD.
-     # Je vais simplement appeler `find_jdk_path` de HEAD comme fallback.
 +    logger.info("JDK portable non trouvé/installé. Retour à la détection standard (JAVA_HOME / chemin par défaut).")
-     jdk_path_from_head = find_jdk_path()
-     return str(jdk_path_from_head) if jdk_path_from_head else None
- 
- # --- Fonctions pour une initialisation paresseuse (Lazy Initialization) ---
- 
- _PROJECT_ROOT_DIR = None
- _LIBS_DIR = None
- _PORTABLE_JDK_PATH = None
- _ENV_LOADED = False
- _JVM_WAS_SHUTDOWN = False  # Indicateur pour éviter les tentatives de redémarrage
- _JVM_INITIALIZED_THIS_SESSION = False  # Flag pour la session de test
- _SESSION_FIXTURE_OWNS_JVM = False  # Flag pour indiquer que la fixture de session contrôle la JVM
- 
- def _ensure_env_loaded():
-     """Charge les variables d'environnement une seule fois."""
-     global _ENV_LOADED
-     if _ENV_LOADED:
-         return
-     try:
-         from dotenv import load_dotenv, find_dotenv
-         env_file = find_dotenv()
-         if env_file:
-             load_dotenv(env_file, override=True)
-             logger.info(f"Variables d'environnement chargées depuis: {env_file}")
-             _ENV_LOADED = True
-         else:
-             logger.warning("Fichier .env non trouvé, utilisation des variables système")
-             _ENV_LOADED = True # Marquer comme chargé pour ne pas retenter
-     except ImportError:
-         logger.warning("python-dotenv non disponible, utilisation des variables système uniquement")
-         _ENV_LOADED = True # Marquer comme chargé pour ne pas retenter
++    jdk_path_from_standard_detection = find_jdk_path()
++    return str(jdk_path_from_standard_detection.resolve()) if jdk_path_from_standard_detection else None
 +
- def get_project_root() -> Path:
-     """Retourne le répertoire racine du projet (calculé une seule fois)."""
-     global _PROJECT_ROOT_DIR
-     if _PROJECT_ROOT_DIR is None:
-         _PROJECT_ROOT_DIR = Path(__file__).resolve().parent.parent.parent
-     return _PROJECT_ROOT_DIR
 +
- def find_libs_dir() -> Optional[Path]:
-     """Trouve le répertoire des JARs de Tweety (calculé une seule fois)."""
-     global _LIBS_DIR
-     if _LIBS_DIR is not None:
-         return _LIBS_DIR if _LIBS_DIR else None # Retourne None si la recherche précédente a échoué
++_JVM_INITIALIZED_THIS_SESSION = False
++_JVM_WAS_SHUTDOWN = False
++_SESSION_FIXTURE_OWNS_JVM = False
 +
-     project_root = get_project_root()
-     primary_dir = project_root / "libs" / "tweety"
-     fallback_dir = project_root / "libs"
- 
-     if primary_dir.is_dir() and list(primary_dir.glob("*.jar")):
-         _LIBS_DIR = primary_dir
-         logger.info(f"LIBS_DIR défini sur (primaire): {_LIBS_DIR}")
-     elif fallback_dir.is_dir() and list(fallback_dir.glob("*.jar")):
-         _LIBS_DIR = fallback_dir
-         logger.info(f"LIBS_DIR défini sur (fallback): {_LIBS_DIR}")
-     else:
-         logger.warning(
-             f"Aucun JAR trouvé ni dans {primary_dir} ni dans {fallback_dir}. "
-             f"LIBS_DIR n'est pas défini."
-         )
-         _LIBS_DIR = Path() # Marqueur pour indiquer que la recherche a échoué
-         return None
-     return _LIBS_DIR
- 
- def find_jdk_path() -> Optional[Path]:
-     """Trouve le chemin du JDK portable ou via JAVA_HOME (calculé une seule fois)."""
-     global _PORTABLE_JDK_PATH
-     if _PORTABLE_JDK_PATH is not None:
-          return _PORTABLE_JDK_PATH if _PORTABLE_JDK_PATH else None
- 
-     _ensure_env_loaded()
-     project_root = get_project_root()
- 
-     # Priorité 1: Variable d'environnement JAVA_HOME
-     java_home = os.getenv('JAVA_HOME')
-     if java_home:
-         potential_path = Path(java_home)
-         if not potential_path.is_absolute():
-             potential_path = get_project_root() / potential_path
-             
-         if potential_path.is_dir():
-             _PORTABLE_JDK_PATH = potential_path
-             logger.info(f"(OK) JDK détecté via JAVA_HOME : {_PORTABLE_JDK_PATH}")
-             return _PORTABLE_JDK_PATH
-         else:
-             logger.warning(f"(ATTENTION) JAVA_HOME défini mais répertoire inexistant : {potential_path}")
-     
-     # Priorité 2: Chemin par défaut
-     jdk_subdir = "libs/portable_jdk/jdk-17.0.11+9"
-     potential_path = project_root / jdk_subdir
-     if potential_path.is_dir():
-         _PORTABLE_JDK_PATH = potential_path
-         logger.info(f"(OK) JDK portable détecté via chemin par défaut : {_PORTABLE_JDK_PATH}")
-         return _PORTABLE_JDK_PATH
-     
-     logger.warning(f"(ATTENTION) JDK portable non trouvé à l'emplacement par défaut : {potential_path}")
-     _PORTABLE_JDK_PATH = Path() # Marqueur pour indiquer que la recherche a échoué
-     return None
 +
 +def get_jvm_options() -> List[str]:
-     """Prépare les options pour le démarrage de la JVM, incluant le chemin du JDK si disponible."""
 +    options = [
-         "-Xms64m",      # Réduit de 128m à 64m pour éviter les access violations
-         "-Xmx256m",     # Réduit de 512m à 256m pour les tests
++        "-Xms64m",
++        "-Xmx256m",
 +        "-Dfile.encoding=UTF-8",
 +        "-Djava.awt.headless=true"
 +    ]
 +    
-     # Options spécifiques Windows pour contourner les access violations JPype
-     if os.name == 'nt':  # Windows
++    if os.name == 'nt':
 +        options.extend([
-             "-XX:+UseG1GC",              # Garbage collector plus stable
-             "-XX:+DisableExplicitGC",    # Évite les GC manuels problématiques
-             "-XX:-UsePerfData",          # Désactive les données de performance
-             "-Djava.awt.headless=true"   # Force mode headless
++            "-XX:+UseG1GC",
++            "-XX:+DisableExplicitGC",
++            "-XX:-UsePerfData",
 +        ])
 +        logger.info("Options JVM Windows spécifiques ajoutées pour contourner les access violations JPype")
 +    
 +    logger.info(f"Options JVM de base définies : {options}")
 +    return options
  
 -def download_portable_jdk(target_dir: Path) -> Optional[str]:
 -    """Télécharge et extrait un JDK portable."""
 -    try:
 -        os_arch = get_os_arch_for_jdk()
 -    except OSError as e:
 -        logging.error(e)
 -        return None
 +def initialize_jvm(lib_dir_path: Optional[str] = None, specific_jar_path: Optional[str] = None) -> bool:
-     """
-     Initialise la JVM avec les JARs de TweetyProject (initialisation paresseuse).
-     
-     ATTENTION: JPype ne permet qu'un seul cycle de vie JVM par processus Python.
-     Une fois jpype.shutdownJVM() appelé, la JVM ne peut plus être redémarrée.
-     """
 +    global _JVM_WAS_SHUTDOWN, _JVM_INITIALIZED_THIS_SESSION, _SESSION_FIXTURE_OWNS_JVM
 +    
 +    logger.info(f"JVM_SETUP: initialize_jvm appelée. isJVMStarted au début: {jpype.isJVMStarted()}")
 +    logger.info(f"JVM_SETUP: _JVM_WAS_SHUTDOWN: {_JVM_WAS_SHUTDOWN}")
 +    logger.info(f"JVM_SETUP: _JVM_INITIALIZED_THIS_SESSION: {_JVM_INITIALIZED_THIS_SESSION}")
 +    logger.info(f"JVM_SETUP: _SESSION_FIXTURE_OWNS_JVM: {_SESSION_FIXTURE_OWNS_JVM}")
 +    
-     # --- Étape de Provisioning (issue du stash) ---
-     logger.info("JVM_SETUP: Lancement de l'étape de vérification/téléchargement des JARs.")
++    logger.info("JVM_SETUP: Lancement de l'étape de vérification/téléchargement des JARs Tweety.")
 +    libs_ok = download_tweety_jars()
 +    if not libs_ok:
 +        logger.error("JVM_SETUP: Échec du provisioning des bibliothèques Tweety. Démarrage de la JVM annulé.")
 +        return False
-     logger.info("JVM_SETUP: Provisioning des bibliothèques terminé.")
++    logger.info("JVM_SETUP: Provisioning des bibliothèques Tweety terminé.")
  
-     # PROTECTION 1: Vérifier si une tentative de redémarrage est en cours
 -    jdk_url = JDK_URL_TEMPLATE.format(
 -        maj_v=JDK_VERSION.split('.')[0],
 -        v=JDK_VERSION,
 -        b=JDK_BUILD,
 -        b_flat=JDK_BUILD, # Le format de l'URL est parfois incohérent
 -        arch=os_arch['arch'],
 -        os=os_arch['os']
 -    )
 +    if _JVM_WAS_SHUTDOWN and not jpype.isJVMStarted():
-         logger.error("JVM_SETUP: ERREUR - Tentative de redémarrage de la JVM détectée. JPype ne supporte qu'un cycle de vie JVM par processus.")
-         logger.error("JVM_SETUP: Veuillez relancer le processus Python pour utiliser la JVM à nouveau.")
++        logger.error("JVM_SETUP: ERREUR - Tentative de redémarrage de la JVM détectée.")
 +        return False
      
-     # PROTECTION 2: Si la fixture de session contrôle la JVM, interdire les appels directs
 -    target_dir.mkdir(exist_ok=True)
 -    zip_path = target_dir / "jdk.zip"
 +    if _SESSION_FIXTURE_OWNS_JVM and jpype.isJVMStarted():
-         logger.info("JVM_SETUP: La JVM est contrôlée par la fixture de session. Utilisation de la JVM existante.")
++        logger.info("JVM_SETUP: La JVM est contrôlée par la fixture de session.")
 +        _JVM_INITIALIZED_THIS_SESSION = True
 +        return True
 +    
-     # PROTECTION 3: Si déjà initialisée dans cette session, ne pas refaire
 +    if _JVM_INITIALIZED_THIS_SESSION and jpype.isJVMStarted():
-         logger.info("JVM_SETUP: JVM déjà initialisée dans cette session. Réutilisation.")
++        logger.info("JVM_SETUP: JVM déjà initialisée dans cette session.")
 +        return True
 +    
 +    if jpype.isJVMStarted():
 +        logger.info("JVM_SETUP: JVM déjà démarrée (sans contrôle de session).")
 +        _JVM_INITIALIZED_THIS_SESSION = True
 +        return True
  
      try:
 -        download_file(jdk_url, zip_path)
 -        # Supprimer le contenu précédent avant de décompresser
 -        for item in target_dir.iterdir():
 -            if item.is_dir():
 -                shutil.rmtree(item)
 -            elif item.is_file() and item.suffix != '.zip':
 -                item.unlink()
 -
 -        unzip_file(zip_path, target_dir)
 -        
 -        # Vérifier que le JDK est maintenant valide
 -        if is_valid_jdk(target_dir):
 -            logging.info(f"JDK portable installé avec succès dans {target_dir}")
 -            return str(target_dir)
 -        else:
 -            logging.error("L'extraction du JDK n'a pas produit une installation valide.")
 -            return None
 -
 -    except (requests.RequestException, IOError, zipfile.BadZipFile, shutil.Error) as e:
 -        logging.error(f"Échec de l'installation du JDK portable : {e}")
 -        shutil.rmtree(target_dir, ignore_errors=True) # Nettoyage complet
 -        return None
 -
 -
 -def is_valid_jdk(path: Path) -> bool:
 -    """Vérifie si un répertoire est un JDK valide et respecte la version minimale."""
 -    if not path.is_dir():
 -        return False
 -        
 -    java_exe = path / "bin" / ("java.exe" if platform.system() == "Windows" else "java")
 -    if not java_exe.exists():
 -        logging.warning(f"Validation JDK échouée: 'java' non trouvé dans {path / 'bin'}")
 -        return False
 +        logger.info(f"JVM_SETUP: Version de JPype: {jpype.__version__}")
 +    except (ImportError, AttributeError):
 +        logger.warning("JVM_SETUP: Impossible d'obtenir la version de JPype.")
  
      try:
-         jars: List[str] = []
 -        # Exécuter `java -version` et capturer la sortie
 -        # stderr est utilisé par Java pour afficher la version
 -        result = subprocess.run(
 -            [str(java_exe), "-version"],
 -            capture_output=True,
 -            text=True,
 -            check=True,
 -            stderr=subprocess.PIPE
 -        )
 -        version_output = result.stderr
 -        
 -        # Parser la version (ex: "openjdk version "11.0.12" 2021-07-20")
 -        first_line = version_output.splitlines()[0]
 -        version_str = first_line.split('"')[1] # "11.0.12"
 -        major_version = int(version_str.split('.')[0])
 -
 -        if major_version >= MIN_JAVA_VERSION:
 -            logging.info(f"Version Java détectée: {version_str} (Majeure: {major_version}) -> Valide.")
 -            return True
++        jars_classpath: List[str] = []
 +        if specific_jar_path:
 +            specific_jar_file = Path(specific_jar_path)
 +            if specific_jar_file.is_file():
-                 jars = [str(specific_jar_file)]
++                jars_classpath = [str(specific_jar_file)]
 +                logger.info(f"Utilisation du JAR spécifique: {specific_jar_path}")
 +            else:
 +                logger.error(f"(ERREUR) Fichier JAR spécifique introuvable: '{specific_jar_path}'.")
 +                return False
          else:
-             jar_directory = Path(lib_dir_path) if lib_dir_path else find_libs_dir()
-             if not jar_directory or not jar_directory.is_dir():
-                 logger.error(f"(ERREUR) Répertoire des JARs '{jar_directory}' invalide.")
 -            logging.warning(f"Version Java {major_version} est inférieure au minimum requis ({MIN_JAVA_VERSION}).")
++            jar_directory_path = Path(lib_dir_path) if lib_dir_path else find_libs_dir()
++            if not jar_directory_path or not jar_directory_path.is_dir():
++                logger.error(f"(ERREUR) Répertoire des JARs '{jar_directory_path}' invalide.")
 +                return False
 +            
-             jars = [str(f) for f in jar_directory.glob("*.jar")]
- # --- DÉBUT DE L'INTÉGRATION DU CHANGEMENT DU STASH ---
-             # Exclure le JAR problématique identifié dans le stash
++            all_jars_in_dir = [str(f) for f in jar_directory_path.glob("*.jar")]
 +            jar_to_exclude = "org.tweetyproject.lp.asp-1.28-with-dependencies.jar"
-             original_jar_count = len(jars)
-             jars = [jar_path for jar_path in jars if jar_to_exclude not in Path(jar_path).name]
-             if len(jars) < original_jar_count:
-                 logger.info(f"Exclusion de débogage: '{jar_to_exclude}' retiré du classpath. Nombre de JARs réduit à {len(jars)}.")
-             # --- FIN DE L'INTÉGRATION DU CHANGEMENT DU STASH ---
-             logger.info(f"Classpath construit avec {len(jars)} JAR(s) depuis '{jar_directory}'.")
-             logger.info(f"Classpath configuré avec {len(jars)} JARs (JPype {jpype.__version__})")
- 
-         if not jars:
++            original_jar_count = len(all_jars_in_dir)
++            jars_classpath = [jp for jp in all_jars_in_dir if jar_to_exclude not in Path(jp).name]
++            if len(jars_classpath) < original_jar_count:
++                logger.info(f"Exclusion de débogage: '{jar_to_exclude}' retiré du classpath. Nombre de JARs réduit à {len(jars_classpath)}.")
++            logger.info(f"Classpath construit avec {len(jars_classpath)} JAR(s) depuis '{jar_directory_path}'.")
++
++        if not jars_classpath:
 +            logger.error("(ERREUR) Aucun JAR trouvé pour le classpath. Démarrage annulé.")
              return False
 -
 -    except (subprocess.CalledProcessError, FileNotFoundError, IndexError, ValueError) as e:
 -        logging.error(f"Erreur lors de la validation de la version de Java à {path}: {e}")
 -        return False
          
-         jvm_options = get_jvm_options()
-         jdk_path_str = find_valid_java_home()
-         jdk_path = Path(jdk_path_str) if jdk_path_str else None
-         jvm_path = None
 -# --- Gestion du cycle de vie de la JVM ---
--
-         # Stratégie de recherche de la JVM
-         try:
-             jvm_path = jpype.getDefaultJVMPath()
-             logger.info(f"JPype a trouvé une JVM par défaut : {jvm_path}")
-         except jpype.JVMNotFoundException:
-             logger.warning("JPype n'a pas trouvé de JVM par défaut. Tentative avec JAVA_HOME.")
-             if jdk_path:
-                 # Construire le chemin vers jvm.dll sur Windows
-                 if os.name == 'nt':
-                     potential_jvm_path = jdk_path / "bin" / "server" / "jvm.dll"
-                 # Construire le chemin vers libjvm.so sur Linux
-                 else:
-                     potential_jvm_path = jdk_path / "lib" / "server" / "libjvm.so"
-                 
-                 if potential_jvm_path.exists():
-                     jvm_path = str(potential_jvm_path)
-                     logger.info(f"Chemin JVM construit manuellement à partir de JAVA_HOME: {jvm_path}")
-                 else:
-                     logger.error(f"Le fichier de la librairie JVM n'a pas été trouvé à l'emplacement attendu: {potential_jvm_path}")
-             else:
-                 logger.error("JAVA_HOME n'est pas défini et la JVM par défaut n'est pas trouvable.")
 -_jvm_started = False
--
-         if not jvm_path:
-             logger.critical("Impossible de localiser la JVM. Le démarrage est annulé.")
-             return False
 -def start_jvm_if_needed(force_restart: bool = False):
 -    """
 -    Démarre la JVM avec le classpath configuré, si elle n'est pas déjà démarrée.
 -    Cette fonction est idempotente par défaut.
 -    """
 -    global _jvm_started
 -    if _jvm_started and not force_restart:
 -        logging.debug("La JVM est déjà démarrée. Aucune action requise.")
 -        return
--
-         logger.info(f"JVM_SETUP: Avant startJVM. isJVMStarted: {jpype.isJVMStarted()}.")
 -    if force_restart and jpype.isJVMStarted():
 -        logging.info("Forçage du redémarrage de la JVM...")
 -        shutdown_jvm()
--
-         try:
-             logger.info(f"Tentative de démarrage de la JVM avec le chemin : {jvm_path}")
-             jpype.startJVM(jvm_path, *jvm_options, classpath=jars)
-         except Exception as e:
-             logger.error(f"Échec final du démarrage de la JVM avec le chemin '{jvm_path}'. Erreur: {e}", exc_info=True)
-             return False
 -    # 1. S'assurer que les dépendances sont présentes
 -    download_tweety_jars()
 -    
 -    # 2. Trouver un JAVA_HOME valide (ou installer un JDK)
 -    java_home = find_valid_java_home()
 -    if not java_home:
 -        raise RuntimeError(
 -            "Impossible de trouver ou d'installer un JDK valide. "
 -            "Veuillez définir JAVA_HOME sur un JDK version 11+ ou assurer une connexion internet."
 -        )
--
-         logger.info(f"JVM démarrée avec succès. isJVMStarted: {jpype.isJVMStarted()}.")
 -    # 3. Construire le Classpath
 -    jar_paths = [str(p) for p in LIBS_DIR.glob("*.jar")]
 -    classpath = os.pathsep.join(jar_paths)
++        jvm_options_list = get_jvm_options()
 +        
-         try:
-             _ = jpype.JClass("org.tweetyproject.logics.pl.syntax.PlSignature")
-             logger.info("(OK) Test de chargement de classe Tweety (PlSignature) réussi.")
-         except Exception as e_test:
-             logger.error(f"(ERREUR) Test de chargement de classe Tweety échoué: {e_test}", exc_info=True)
++        java_home_path_str = find_valid_java_home()
++        if not java_home_path_str:
++            logger.error("Impossible de trouver un JDK valide. JAVA_HOME n'est pas défini ou le JDK portable a échoué.")
++        else:
++            logger.info(f"Utilisation de JAVA_HOME (ou équivalent portable) : {java_home_path_str}")
  
-         # Marquer que la JVM a été initialisée avec succès dans cette session
 -    if not jar_paths:
 -        raise RuntimeError(f"Aucune bibliothèque (.jar) trouvée dans {LIBS_DIR}. Le classpath est vide.")
++        logger.info(f"Tentative de démarrage de la JVM avec classpath: {os.pathsep.join(jars_classpath)}")
++        logger.info(f"Options JVM: {jvm_options_list}")
+         
 -    logging.info(f"Classpath configuré : {classpath}")
 -    
 -    # 4. Démarrer la JVM
 -    try:
 -        logging.info("Démarrage de la JVM...")
++        jvm_dll_path = jpype.getDefaultJVMPath()
++        logger.info(f"Chemin JVM par défaut détecté par JPype: {jvm_dll_path}")
++
+         jpype.startJVM(
 -            #jpype.getDefaultJVMPath(), # Laisser JPype trouver la libjvm
 -            jvmpath=jpype.getDefaultJVMPath(),
 -            classpath=classpath,
++            jvm_dll_path,
++            *jvm_options_list,
++            classpath=jars_classpath,
+             ignoreUnrecognized=True,
 -            convertStrings=False,
 -            # Passer le JAVA_HOME trouvé permet de s'assurer que JPype utilise le bon JDK
 -            # C'est implicite si la libjvm est trouvée via le path, mais c'est plus sûr
++            convertStrings=False
+         )
 -        _jvm_started = True
 -        logging.info("JVM démarrée avec succès.")
 +        _JVM_INITIALIZED_THIS_SESSION = True
-         logger.info("JVM_SETUP: Flag _JVM_INITIALIZED_THIS_SESSION défini à True.")
++        logger.info("JVM démarrée avec succès.")
 +        return True
  
      except Exception as e:
-         logger.critical(f"(ERREUR CRITIQUE) Échec global du démarrage de la JVM: {e}", exc_info=True)
 -        logging.error(f"Erreur fatale lors du démarrage de la JVM : {e}")
 -        logging.error(f"JAVA_HOME utilisé (si trouvé) : {java_home}")
 -        logging.error(f"Chemin JVM par défaut de JPype : {jpype.getDefaultJVMPath()}")
 -        # Tenter d'offrir plus de diagnostics
 -        if sys.platform == "win32" and "Error: Could not find " in str(e):
 -             logging.error("Astuce Windows: Assurez-vous que Microsoft Visual C++ Redistributable est installé.")
 -        elif "No matching overloads found" in str(e):
 -             logging.error("Astuce: Cette erreur peut survenir si le classpath est incorrect ou si une dépendance manque.")
 -        raise
++        logger.error(f"Erreur fatale lors du démarrage de la JVM: {e}", exc_info=True)
++        if " RuntimeError: No matching overloads found." in str(e) or "No matching overloads found" in str(e):
++             logger.error("Astuce: Cette erreur peut survenir si le classpath est incorrect, si une dépendance manque, ou incompatibilité de version JAR/JVM.")
++        elif sys.platform == "win32" and ("java.lang.UnsatisfiedLinkError" in str(e) or "Can't load IA 32-bit .dll on a AMD 64-bit platform" in str(e)):
++             logger.error("Astuce Windows: Vérifiez la cohérence 32/64 bits entre Python, JPype et le JDK. Assurez-vous que Microsoft Visual C++ Redistributable est installé.")
 +        return False
  
- def _safe_log(logger_instance, level, message, exc_info_val=False):
-     """Effectue un log de manière sécurisée, avec un fallback sur print."""
-     try:
-         if logger_instance.hasHandlers():
-             logger_instance.log(level, message, exc_info=exc_info_val)
-         else:
-             print(f"FALLBACK LOG ({logging.getLevelName(level)}): {message}")
-             if exc_info_val:
-                 import traceback
-                 traceback.print_exc()
-     except Exception:
-         print(f"FALLBACK LOG (Exception in logger) ({logging.getLevelName(level)}): {message}")
- 
- def set_session_fixture_owns_jvm(owns: bool = True):
-     """
-     Définit si la fixture de session contrôle la JVM.
-     
-     Args:
-         owns: True si la fixture de session contrôle la JVM, False sinon
-     """
-     global _SESSION_FIXTURE_OWNS_JVM
-     _SESSION_FIXTURE_OWNS_JVM = owns
-     logger.info(f"JVM_SETUP: _SESSION_FIXTURE_OWNS_JVM défini à {owns}")
- 
- def is_session_fixture_owns_jvm() -> bool:
-     """Retourne si la fixture de session contrôle la JVM."""
-     return _SESSION_FIXTURE_OWNS_JVM
- 
- def reset_session_flags():
-     """Remet à zéro les flags de session (utile pour les tests)."""
-     global _JVM_INITIALIZED_THIS_SESSION, _SESSION_FIXTURE_OWNS_JVM
-     _JVM_INITIALIZED_THIS_SESSION = False
-     _SESSION_FIXTURE_OWNS_JVM = False
-     logger.info("JVM_SETUP: Flags de session remis à zéro")
- 
- def shutdown_jvm_if_needed():
-     """
-     Arrête la JVM si elle est démarrée.
-     
-     ATTENTION: Une fois la JVM arrêtée avec jpype.shutdownJVM(),
-     elle ne peut plus être redémarrée dans le même processus Python.
-     """
-     global _JVM_WAS_SHUTDOWN
-     
-     _safe_log(logger, logging.INFO, "JVM_SETUP: Appel de shutdown_jvm_if_needed.")
-     _safe_log(logger, logging.INFO, f"JVM_SETUP: _SESSION_FIXTURE_OWNS_JVM: {_SESSION_FIXTURE_OWNS_JVM}")
-     
-     try:
-         if jpype.isJVMStarted():
-             _safe_log(logger, logging.INFO, f"JVM_SETUP: JVM est démarrée. Appel de jpype.shutdownJVM().")
-             jpype.shutdownJVM()
-             _JVM_WAS_SHUTDOWN = True
-             _safe_log(logger, logging.INFO, f"JVM_SETUP: jpype.shutdownJVM() exécuté. Flag _JVM_WAS_SHUTDOWN défini à True.")
-             _safe_log(logger, logging.INFO, f"JVM_SETUP: ATTENTION: La JVM ne peut plus être redémarrée dans ce processus.")
-         else:
-             _safe_log(logger, logging.INFO, "JVM_SETUP: JVM n'était pas démarrée.")
-     except Exception as e_shutdown:
-         _safe_log(logger, logging.ERROR, f"JVM_SETUP: Erreur lors de jpype.shutdownJVM(): {e_shutdown}", exc_info_val=True)
  
- # --- Exports pour l'importation par d'autres modules ---
- TWEETY_VERSION = "1.28" # Doit correspondre à la version dans libs
- LIBS_DIR = find_libs_dir()
+ def shutdown_jvm():
 -    """Arrête la JVM si elle est en cours d'exécution."""
 -    global _jvm_started
++    global _JVM_INITIALIZED_THIS_SESSION, _JVM_WAS_SHUTDOWN
+     if jpype.isJVMStarted():
 -        logging.info("Arrêt de la JVM...")
++        logger.info("Arrêt de la JVM...")
+         jpype.shutdownJVM()
 -        _jvm_started = False
 -        logging.info("JVM arrêtée.")
++        _JVM_INITIALIZED_THIS_SESSION = False
++        _JVM_WAS_SHUTDOWN = True
++        logger.info("JVM arrêtée.")
+     else:
 -        logging.debug("La JVM n'est pas en cours d'exécution.")
++        logger.debug("La JVM n'est pas en cours d'exécution.")
  
 -# --- Point d'entrée pour exemple ou test ---
  if __name__ == "__main__":
-     logging.basicConfig(level=logging.DEBUG)
-     
-     # Utiliser la variable exportée maintenant
-     if LIBS_DIR:
-         success = initialize_jvm()
-         if success:
-             logger.info("Test initialize_jvm: SUCCÈS")
 -    logging.info("--- Démonstration du module jvm_setup ---")
 -    try:
 -        logging.info("\n1. Première tentative de démarrage de la JVM...")
 -        start_jvm_if_needed()
++    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
++    main_logger = logging.getLogger(__name__)
+ 
 -        logging.info("\n2. Tentative de démarrage redondante (devrait être ignorée)...")
 -        start_jvm_if_needed()
++    main_logger.info("--- Démonstration du module jvm_setup ---")
++    try:
++        main_logger.info("\n1. Première tentative de démarrage de la JVM...")
++        if initialize_jvm():
++            main_logger.info("\n2. Tentative de démarrage redondante (devrait être ignorée)...")
++            initialize_jvm()
+ 
 -        # Test simple d'importation Java
 -        try:
 -            JString = jpype.JClass("java.lang.String")
 -            my_string = JString("Ceci est un test depuis Python!")
 -            logging.info(f"Test Java réussi: {my_string}")
 -        except Exception as e:
 -            logging.error(f"Le test d'importation Java a échoué: {e}")
 +            try:
-                 TestClass = jpype.JClass("org.tweetyproject.logics.pl.syntax.PropositionalSignature")
-                 logger.info(f"Classe de test chargée: {TestClass}")
-             except Exception as e:
-                 logger.error(f"Erreur lors du test: {e}", exc_info=True)
-             finally:
-                 shutdown_jvm_if_needed()
++                JString = jpype.JClass("java.lang.String")
++                my_string = JString("Ceci est un test depuis Python!")
++                main_logger.info(f"Test Java réussi: {my_string}")
++            except Exception as e_java_test:
++                main_logger.error(f"Le test d'importation Java a échoué: {e_java_test}")
 +        else:
-             logger.error("Test initialize_jvm: ÉCHEC")
-     else:
-         logger.error("Test initialize_jvm: ÉCHEC - LIBS_DIR non défini.")
++            main_logger.error("Échec du premier démarrage de la JVM. Démonstration interrompue.")
+ 
 -    except Exception as e:
 -        logging.error(f"Une erreur est survenue durant la démonstration : {e}")
++    except Exception as e_demo:
++        main_logger.error(f"Une erreur est survenue durant la démonstration : {e_demo}", exc_info=True)
+ 
+     finally:
 -        logging.info("\n3. Arrêt de la JVM...")
++        main_logger.info("\n3. Arrêt de la JVM...")
+         shutdown_jvm()
 -        logging.info("\n--- Fin de la démonstration ---")
++        main_logger.info("\n--- Fin de la démonstration ---")
diff --cc argumentation_analysis/core/strategies.py
index 1b8525d4,6fed32b9..8d1836f5
--- a/argumentation_analysis/core/strategies.py
+++ b/argumentation_analysis/core/strategies.py
@@@ -1,17 -1,16 +1,20 @@@
 -﻿# core/strategies.py
 +# core/strategies.py
- # CORRECTIF COMPATIBILITÉ: Import direct depuis semantic_kernel
- from semantic_kernel.contents import ChatMessageContent
- # from semantic_kernel.agents import Agent # AJOUTÉ POUR CORRIGER NameError - Commenté car non disponible dans SK 0.9.6b1
- # Note: Agent, TerminationStrategy, SelectionStrategy non disponibles dans SK 0.9.6b1
- from typing import List, Dict, TYPE_CHECKING
+ # CORRECTIF COMPATIBILITÉ: Utilisation du module de compatibilité
+ from argumentation_analysis.utils.semantic_kernel_compatibility import Agent, TerminationStrategy, SelectionStrategy
+ from semantic_kernel.contents import ChatMessageContent, ChatRole as AuthorRole
 -from typing import List, Dict, TYPE_CHECKING
++
++from typing import List, Dict, TYPE_CHECKING, Optional # Ajout de Optional
  import logging
  from pydantic import PrivateAttr
--from argumentation_analysis.orchestration.base import SelectionStrategy, TerminationStrategy
++# L'import de 'argumentation_analysis.orchestration.base.SelectionStrategy' et 
++# 'argumentation_analysis.orchestration.base.TerminationStrategy' est omis
++# car ces noms sont maintenant fournis par 'argumentation_analysis.utils.semantic_kernel_compatibility'.
++# Les classes de stratégies ci-dessous hériteront donc des versions du module de compatibilité.
  
  # Importer la classe d'état
  from .shared_state import RhetoricalAnalysisState
  
--# Type hinting (si nécessaire, mais RhetoricalAnalysisState est importé)
++# Type hinting
  if TYPE_CHECKING:
      pass
  
@@@ -44,7 -43,7 +47,7 @@@ class SimpleTerminationStrategy(Termina
          self._logger = termination_logger
          self._logger.info(f"SimpleTerminationStrategy instance {self._instance_id} créée (max_steps={self._max_steps}, state_id={id(self._state)}).")
  
--    async def should_terminate(self, agent, history: List[ChatMessageContent]) -> bool: # Agent type hint commenté
++    async def should_terminate(self, agent: Agent, history: List[ChatMessageContent]) -> bool:
          """Vérifie si la conversation doit se terminer."""
          self._step_count += 1
          step_info = f"Tour {self._step_count}/{self._max_steps}"
@@@ -80,35 -79,35 +83,26 @@@
  
  class DelegatingSelectionStrategy(SelectionStrategy):
      """Stratégie de sélection qui priorise la désignation explicite via l'état."""
--    # *** CORRECTION: Utiliser PrivateAttr pour les champs gérés par __init__ ***
--    _agents_map: Dict[str, any] = PrivateAttr() # Agent type hint remplacé par any
--    _default_agent_name: str = PrivateAttr(default="ProjectManagerAgent") # On peut mettre le défaut ici
++    _agents_map: Dict[str, Agent] = PrivateAttr()
++    _default_agent_name: str = PrivateAttr(default="ProjectManagerAgent")
      _analysis_state: 'RhetoricalAnalysisState' = PrivateAttr()
--    # Ces deux-là ne semblent pas faire partie du modèle Pydantic de base, on peut les laisser
--    # S'ils ne sont pas définis par Pydantic, ils doivent être explicitement typés
--    _instance_id: int
--    _logger: logging.Logger
++    _instance_id: int # Non géré par Pydantic, initialisé dans __init__
++    _logger: logging.Logger # Non géré par Pydantic, initialisé dans __init__
  
--    def __init__(self, agents: List, state: 'RhetoricalAnalysisState', default_agent_name: str = "ProjectManagerAgent"): # Agent type hint commenté dans List
--        """Initialise avec agents, état, et nom agent par défaut."""
--        # L'appel super() doit rester ici
++    def __init__(self, agents: List[Agent], state: 'RhetoricalAnalysisState', default_agent_name: str = "ProjectManagerAgent"):
          super().__init__()
          if not isinstance(agents, list):
              raise TypeError("'agents' doit être une liste d'agents.")
--        # Vérification assouplie pour les tests : on vérifie seulement que chaque agent a un attribut 'name'
          for a in agents:
              if not hasattr(a, 'name'):
--                raise TypeError("Chaque agent doit avoir un attribut 'name'.")
--        # S'assurer que la classe State est définie et que l'objet state a la bonne méthode
++                raise TypeError(f"Chaque agent doit avoir un attribut 'name'. Agent problématique: {a}")
          if not isinstance(state, RhetoricalAnalysisState) or not hasattr(state, 'consume_next_agent_designation'):
               raise TypeError("Objet 'state' invalide ou classe RhetoricalAnalysisState non définie pour DelegatingSelectionStrategy.")
  
--        # *** CORRECTION: Assigner aux attributs privés ***
          self._agents_map = {agent.name: agent for agent in agents}
          self._analysis_state = state
--        self._default_agent_name = default_agent_name # Le paramètre a priorité sur le défaut de PrivateAttr
++        self._default_agent_name = default_agent_name
  
--        # Le reste de l'initialisation utilise maintenant les attributs privés
          self._instance_id = id(self)
          self._logger = selection_logger
  
@@@ -116,15 -115,15 +110,14 @@@
              if not self._agents_map: raise ValueError("Liste d'agents vide.")
              first_agent_name = list(self._agents_map.keys())[0]
              self._logger.warning(f"[{self._instance_id}] Agent défaut '{self._default_agent_name}' non trouvé. Fallback -> '{first_agent_name}'.")
--            self._default_agent_name = first_agent_name # Mettre à jour l'attribut privé
++            self._default_agent_name = first_agent_name
  
          self._logger.info(f"DelegatingSelectionStrategy instance {self._instance_id} créée (agents: {list(self._agents_map.keys())}, default: '{self._default_agent_name}', state_id={id(self._analysis_state)}).")
  
--    async def next(self, agents: List, history: List[ChatMessageContent]): # Agent type hint commenté dans List et en retour
++    async def next(self, agents: List[Agent], history: List[ChatMessageContent]) -> Agent:
          """Sélectionne le prochain agent à parler."""
          self._logger.debug(f"[{self._instance_id}] Appel next()...")
  
--        # 1. Vérifier la désignation explicite en priorité absolue
          try:
              designated_agent_name = self._analysis_state.consume_next_agent_designation()
              if designated_agent_name:
@@@ -138,7 -137,7 +131,6 @@@
          except Exception as e_state_access:
              self._logger.error(f"[{self._instance_id}] Erreur accès état pour désignation: {e_state_access}. Poursuite avec fallback.")
  
--        # 2. Logique de fallback si aucune désignation valide n'a été trouvée
          default_agent_instance = self._agents_map.get(self._default_agent_name)
          if not default_agent_instance:
              self._logger.error(f"[{self._instance_id}] ERREUR: Agent défaut '{self._default_agent_name}' introuvable! Retourne premier agent.")
@@@ -151,9 -150,9 +143,6 @@@
              self._logger.info(f" -> Sélection (fallback): Premier tour -> Agent défaut ({self._default_agent_name}).")
              return default_agent_instance
  
--        # Le reste de la logique de fallback...
--        last_message = history[-1]
--        last_author_name = getattr(last_message, 'name', getattr(last_message, 'author_name', None))
          agent_to_select = default_agent_instance
          self._logger.info(f" -> Agent sélectionné (fallback): {agent_to_select.name}")
          return agent_to_select
@@@ -162,7 -161,7 +151,6 @@@
          """Réinitialise la stratégie."""
          self._logger.info(f"[{self._instance_id}] Reset DelegatingSelectionStrategy.")
          try:
--            # Utilise _analysis_state
              consumed = self._analysis_state.consume_next_agent_designation()
              if consumed: self._logger.debug(f"   Ancienne désignation '{consumed}' effacée.")
          except Exception as e:
@@@ -171,79 -170,79 +159,60 @@@
  
  class BalancedParticipationStrategy(SelectionStrategy):
      """Stratégie de sélection qui équilibre la participation des agents tout en respectant les désignations explicites."""
--    # Attributs privés
--    _agents_map: Dict[str, any] = PrivateAttr() # Agent type hint remplacé par any
++    _agents_map: Dict[str, Agent] = PrivateAttr(default_factory=dict)
      _default_agent_name: str = PrivateAttr(default="ProjectManagerAgent")
--    _analysis_state: 'RhetoricalAnalysisState' = PrivateAttr()
--    _participation_counts: Dict[str, int] = PrivateAttr()
--    _target_participation: Dict[str, float] = PrivateAttr()
--    _imbalance_budget: Dict[str, float] = PrivateAttr()
--    _total_turns: int = PrivateAttr()
--    _last_selected: Dict[str, int] = PrivateAttr()
--    _logger: logging.Logger = PrivateAttr()
--    
--    # Ces attributs ne sont pas gérés par Pydantic
--    _instance_id: int
++    _analysis_state: 'RhetoricalAnalysisState' = PrivateAttr() # Doit être passé à __init__
++    _participation_counts: Dict[str, int] = PrivateAttr(default_factory=dict)
++    _target_participation: Dict[str, float] = PrivateAttr(default_factory=dict)
++    _imbalance_budget: Dict[str, float] = PrivateAttr(default_factory=dict)
++    _total_turns: int = PrivateAttr(default=0)
++    _last_selected: Dict[str, int] = PrivateAttr(default_factory=dict)
++    _logger: logging.Logger = PrivateAttr() # Sera initialisé dans __init__
++    _instance_id: int # Sera initialisé dans __init__
  
--    def __init__(self, agents: List, state: 'RhetoricalAnalysisState', # Agent type hint commenté dans List
++    def __init__(self, agents: List[Agent], state: 'RhetoricalAnalysisState',
                   default_agent_name: str = "ProjectManagerAgent",
--                 target_participation: Dict[str, float] = None):
--        """
--        Initialise la stratégie avec les agents, l'état et les paramètres d'équilibrage.
--        
--        Args:
--            agents: Liste des agents disponibles
--            state: Instance de l'état partagé
--            default_agent_name: Nom de l'agent par défaut
--            target_participation: Dictionnaire des participations cibles par agent (%)
--        """
++                 target_participation: Optional[Dict[str, float]] = None):
          super().__init__()
          if not isinstance(agents, list):
              raise TypeError("'agents' doit être une liste d'agents.")
--        # Vérification assouplie pour les tests : on vérifie seulement que chaque agent a un attribut 'name'
          for a in agents:
              if not hasattr(a, 'name'):
--                raise TypeError("Chaque agent doit avoir un attribut 'name'.")
++                raise TypeError(f"Chaque agent doit avoir un attribut 'name'. Agent problématique: {a}")
          if not isinstance(state, RhetoricalAnalysisState) or not hasattr(state, 'consume_next_agent_designation'):
              raise TypeError("Objet 'state' invalide pour BalancedParticipationStrategy.")
  
--        # Initialisation des attributs privés
          self._agents_map = {agent.name: agent for agent in agents}
          self._analysis_state = state
          self._default_agent_name = default_agent_name
          self._total_turns = 0
          
--        # Initialisation des compteurs et budgets
          self._participation_counts = {agent.name: 0 for agent in agents}
          self._last_selected = {agent.name: 0 for agent in agents}
          
--        # Définition des participations cibles
          if target_participation and isinstance(target_participation, dict):
--            self._target_participation = target_participation
++            self._target_participation = target_participation.copy() # Copier pour éviter modif externe
          else:
--            # Distribution équitable par défaut
--            equal_share = 1.0 / len(agents) if agents else 0
++            num_agents = len(agents)
++            equal_share = 1.0 / num_agents if num_agents > 0 else 0
              self._target_participation = {agent.name: equal_share for agent in agents}
--            
--            # Donner une part plus importante au PM par défaut
--            if default_agent_name in self._target_participation:
--                pm_share = min(0.4, 1.0 / len(agents) * 2)  # Max 40% ou double de la part équitable
--                remaining_share = (1.0 - pm_share) / (len(agents) - 1) if len(agents) > 1 else 0
++            if default_agent_name in self._target_participation and num_agents > 0:
++                pm_share = min(0.4, (1.0 / num_agents) * 2.0) if num_agents > 0 else 0.0
++                num_other_agents = num_agents - 1
++                remaining_share_total = 1.0 - pm_share
++                individual_remaining_share = remaining_share_total / num_other_agents if num_other_agents > 0 else 0.0
                  
--                for name in self._target_participation:
--                    if name == default_agent_name:
--                        self._target_participation[name] = pm_share
++                for name_key in self._target_participation: # Utiliser name_key pour éviter conflit
++                    if name_key == default_agent_name:
++                        self._target_participation[name_key] = pm_share
                      else:
--                        self._target_participation[name] = remaining_share
++                        self._target_participation[name_key] = individual_remaining_share
          
--        # Initialisation des budgets de déséquilibre
          self._imbalance_budget = {agent.name: 0.0 for agent in agents}
          
--        # Configuration du logger
          self._instance_id = id(self)
--        self._logger = selection_logger
++        self._logger = selection_logger # Utiliser le logger défini globalement
          
--        # Vérification de l'agent par défaut
          if self._default_agent_name not in self._agents_map:
              if not self._agents_map:
                  raise ValueError("Liste d'agents vide.")
@@@ -254,21 -253,21 +223,10 @@@
          self._logger.info(f"BalancedParticipationStrategy instance {self._instance_id} créée (agents: {list(self._agents_map.keys())}, default: '{self._default_agent_name}', state_id={id(self._analysis_state)}).")
          self._logger.info(f"Participations cibles: {self._target_participation}")
  
--    async def next(self, agents: List, history: List[ChatMessageContent]): # Agent type hint commenté dans List et en retour
--        """
--        Sélectionne le prochain agent selon la stratégie d'équilibrage.
--        
--        Args:
--            agents: Liste des agents disponibles
--            history: Historique des messages
--            
--        Returns:
--            Agent: L'agent sélectionné pour le prochain tour
--        """
++    async def next(self, agents: List[Agent], history: List[ChatMessageContent]) -> Agent:
          self._logger.debug(f"[{self._instance_id}] Appel next()...")
          self._total_turns += 1
  
--        # 1. Vérifier la désignation explicite en priorité absolue
          try:
              designated_agent_name = self._analysis_state.consume_next_agent_designation()
              if designated_agent_name:
@@@ -284,119 -283,119 +242,90 @@@
          except Exception as e_state_access:
              self._logger.error(f"[{self._instance_id}] Erreur accès état pour désignation: {e_state_access}. Poursuite avec équilibrage.")
  
--        # 2. Sinon, calculer les scores de priorité pour chaque agent
          priority_scores = self._calculate_priority_scores()
          
--        # 3. Sélectionner l'agent avec le score le plus élevé
--        default_agent_instance = self._agents_map.get(self._default_agent_name) # Assurez-vous qu'il y a un fallback
++        default_agent_instance = self._agents_map.get(self._default_agent_name)
          selected_agent_name = max(priority_scores, key=priority_scores.get) if priority_scores else self._default_agent_name
          selected_agent = self._agents_map.get(selected_agent_name, default_agent_instance)
          
--        self._logger.info(f" -> Agent sélectionné (équilibrage): {selected_agent.name} (score: {priority_scores.get(selected_agent_name, 0):.2f})")
++        if not selected_agent:
++             if not self._agents_map: raise RuntimeError("Aucun agent disponible pour la sélection.")
++             selected_agent = list(self._agents_map.values())[0]
++             self._logger.error(f"[{self._instance_id}] ERREUR CRITIQUE: Agent sélectionné '{selected_agent_name}' ou défaut introuvable. Fallback au premier agent: {selected_agent.name}")
++
++        self._logger.info(f" -> Agent sélectionné (équilibrage): {selected_agent.name} (score: {priority_scores.get(selected_agent.name, 0.0):.2f})")
          
--        # 4. Mettre à jour les compteurs et budgets
          self._update_participation_counts(selected_agent.name)
          
          return selected_agent
      
      def _calculate_priority_scores(self) -> Dict[str, float]:
--        """
--        Calcule les scores de priorité pour chaque agent.
--        
--        Returns:
--            Dict[str, float]: Dictionnaire des scores de priorité par agent
--        """
          scores = {}
--        
--        for agent_name in self._agents_map:
--            # Calculer le taux de participation actuel
--            current_participation_rate = self._participation_counts.get(agent_name, 0) / max(1, self._total_turns)
--            
--            # Calculer l'écart par rapport à la cible
--            target_rate = self._target_participation.get(agent_name, 0)
++        if not self._agents_map: return scores
++
++        for agent_name_key in self._agents_map: # Utiliser agent_name_key
++            current_participation_rate = self._participation_counts.get(agent_name_key, 0) / max(1, self._total_turns)
++            target_rate = self._target_participation.get(agent_name_key, 0.0) # Fournir une valeur par défaut
              participation_gap = target_rate - current_participation_rate
              
--            # Facteur de temps depuis la dernière sélection
--            turns_since_last_selection = self._total_turns - self._last_selected.get(agent_name, 0)
--            recency_factor = min(1.0, turns_since_last_selection / 3.0)  # Plafonné à 1.0
++            turns_since_last_selection = self._total_turns - self._last_selected.get(agent_name_key, 0)
++            recency_factor = min(1.0, turns_since_last_selection / 3.0)
              
--            # Calculer le score final
--            # Plus l'écart est positif (sous-représenté), plus le score est élevé
--            # Plus le temps depuis la dernière sélection est grand, plus le score est élevé
--            base_score = participation_gap * 10.0  # Facteur d'échelle pour l'écart
--            recency_boost = recency_factor * 0.5   # Bonus pour le temps écoulé
--            budget_boost = self._imbalance_budget.get(agent_name, 0) * 0.2  # Bonus pour le budget accumulé
++            base_score = participation_gap * 10.0
++            recency_boost = recency_factor * 0.5
++            budget_boost = self._imbalance_budget.get(agent_name_key, 0.0) * 0.2 # Fournir une valeur par défaut
              
--            scores[agent_name] = base_score + recency_boost + budget_boost
++            scores[agent_name_key] = base_score + recency_boost + budget_boost
              
--            # Log détaillé pour le débogage
--            self._logger.debug(f"Score {agent_name}: {scores[agent_name]:.2f} (écart={participation_gap:.2f}, "
--                              f"récence={recency_factor:.2f}, budget={self._imbalance_budget.get(agent_name, 0):.2f})")
++            self._logger.debug(f"Score {agent_name_key}: {scores[agent_name_key]:.2f} (écart={participation_gap:.2f}, "
++                              f"récence={recency_factor:.2f}, budget={self._imbalance_budget.get(agent_name_key, 0.0):.2f})")
          
          return scores
      
      def _update_participation_counts(self, agent_name: str) -> None:
--        """
--        Met à jour les compteurs après sélection d'un agent.
--        
--        Args:
--            agent_name: Nom de l'agent sélectionné
--        """
          if agent_name in self._participation_counts:
              self._participation_counts[agent_name] += 1
              self._last_selected[agent_name] = self._total_turns
              
--            # Log des compteurs mis à jour
--            participation_rates = {name: count / max(1, self._total_turns)
--                                 for name, count in self._participation_counts.items()}
--            self._logger.debug(f"Compteurs mis à jour: {self._participation_counts}")
--            self._logger.debug(f"Taux participation: {participation_rates}")
++            if self._total_turns > 0:
++                participation_rates = {name_key: count / self._total_turns
++                                     for name_key, count in self._participation_counts.items()}
++                self._logger.debug(f"Compteurs mis à jour: {self._participation_counts}")
++                self._logger.debug(f"Taux participation: {participation_rates}")
      
      def _adjust_imbalance_budget(self, agent_name: str) -> None:
--        """
--        Ajuste le budget de déséquilibre après une désignation explicite.
--        
--        Args:
--            agent_name: Nom de l'agent désigné explicitement
--        """
--        # Calculer le taux de participation actuel pour tous les agents
--        current_rates = {name: count / max(1, self._total_turns)
--                        for name, count in self._participation_counts.items()}
++        if self._total_turns == 0: return
++
++        current_rates = {name_key: count / self._total_turns
++                        for name_key, count in self._participation_counts.items()}
          
--        # Pour l'agent sélectionné, calculer l'écart par rapport à sa cible
--        target_rate = self._target_participation.get(agent_name, 0)
--        current_rate = current_rates.get(agent_name, 0)
++        target_rate = self._target_participation.get(agent_name, 0.0)
++        current_rate = current_rates.get(agent_name, 0.0)
          
--        # Si l'agent est déjà surreprésenté, augmenter le budget des autres
          if current_rate > target_rate:
              excess = current_rate - target_rate
--            # Distribuer l'excès aux autres agents proportionnellement à leur cible
--            total_other_target = sum(self._target_participation.get(name, 0)
--                                   for name in self._agents_map if name != agent_name)
++            total_other_target = sum(self._target_participation.get(name_key, 0.0)
++                                   for name_key in self._agents_map if name_key != agent_name)
              
              if total_other_target > 0:
--                for other_name in self._agents_map:
--                    if other_name != agent_name:
--                        other_target = self._target_participation.get(other_name, 0)
--                        share = other_target / total_other_target if total_other_target > 0 else 0
--                        self._imbalance_budget[other_name] += excess * share
++                for other_name_key in self._agents_map: # Utiliser other_name_key
++                    if other_name_key != agent_name:
++                        other_target = self._target_participation.get(other_name_key, 0.0)
++                        share = other_target / total_other_target
++                        self._imbalance_budget[other_name_key] = self._imbalance_budget.get(other_name_key, 0.0) + excess * share
          
--        # Réduire le budget de l'agent sélectionné
--        self._imbalance_budget[agent_name] = max(0, self._imbalance_budget.get(agent_name, 0) - 0.1)
++        self._imbalance_budget[agent_name] = max(0.0, self._imbalance_budget.get(agent_name, 0.0) - 0.1) # Assurer float
          
          self._logger.debug(f"Budgets ajustés: {self._imbalance_budget}")
  
      async def reset(self) -> None:
--        """Réinitialise les compteurs et budgets."""
          self._logger.info(f"[{self._instance_id}] Reset BalancedParticipationStrategy.")
          
--        # Réinitialiser les compteurs
--        self._participation_counts = {name: 0 for name in self._agents_map}
++        self._participation_counts = {name_key: 0 for name_key in self._agents_map}
          self._total_turns = 0
--        self._last_selected = {name: 0 for name in self._agents_map}
--        self._imbalance_budget = {name: 0.0 for name in self._agents_map}
++        self._last_selected = {name_key: 0 for name_key in self._agents_map}
++        self._imbalance_budget = {name_key: 0.0 for name_key in self._agents_map}
          
--        # Effacer toute désignation en attente
          try:
              consumed = self._analysis_state.consume_next_agent_designation()
              if consumed:
@@@ -405,6 -404,6 +334,5 @@@
              self._logger.warning(f"   Erreur accès état pendant reset sélection: {e}")
  
  
--# Optionnel : Log de chargement
  module_logger = logging.getLogger(__name__)
  module_logger.debug("Module core.strategies chargé.")
diff --cc argumentation_analysis/orchestration/analysis_runner.py
index fc2d4cad,1468f20c..e8182926
--- a/argumentation_analysis/orchestration/analysis_runner.py
+++ b/argumentation_analysis/orchestration/analysis_runner.py
@@@ -26,13 -26,14 +26,14 @@@ from semantic_kernel.connectors.ai.open
  from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion
  from semantic_kernel.contents.chat_message_content import ChatMessageContent as SKChatMessageContent # Alias pour éviter conflit
  from semantic_kernel.kernel import Kernel as SKernel # Alias pour éviter conflit avec Kernel de SK
- 
- # Imports Semantic Kernel (partiellement du stash pour compatibilité)
+ # KernelArguments est déjà importé plus bas
+  # Imports Semantic Kernel
  import semantic_kernel as sk
- from semantic_kernel.contents import ChatMessageContent, AuthorRole
 -from semantic_kernel.contents import ChatMessageContent, ChatRole as AuthorRole
++from semantic_kernel.contents import ChatMessageContent, ChatRole as AuthorRole, ChatHistory, Role
  # CORRECTIF COMPATIBILITÉ: Utilisation du module de compatibilité
- from argumentation_analysis.utils.semantic_kernel_compatibility import AgentGroupChat, Agent
+ from argumentation_analysis.utils.semantic_kernel_compatibility import AgentGroupChat, ChatCompletionAgent, Agent
 -from semantic_kernel_compatibility import AgentChatException
 +from semantic_kernel.exceptions import AgentChatException
+ from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion, AzureChatCompletion # Pour type hint
  from semantic_kernel.connectors.ai.function_choice_behavior import FunctionChoiceBehavior
  from semantic_kernel.functions.kernel_arguments import KernelArguments
  
diff --cc argumentation_analysis/orchestration/cluedo_extended_orchestrator.py
index f8a13b45,22e5b9b0..857a138f
--- a/argumentation_analysis/orchestration/cluedo_extended_orchestrator.py
+++ b/argumentation_analysis/orchestration/cluedo_extended_orchestrator.py
@@@ -1,26 -1,64 +1,26 @@@
- # argumentation_analysis/orchestration/cluedo_extended_orchestrator.py
+ ﻿# argumentation_analysis/orchestration/cluedo_extended_orchestrator.py
  """
- Orchestrateur principal pour le workflow Cluedo étendu (Sherlock → Watson → Moriarty).
- Ce module gère l'orchestration du flux de conversation, en déléguant les
- logiques spécifiques (stratégies, métriques, analyse) à des composants dédiés.
+ Orchestrateur pour workflow Cluedo étendu avec 3 agents : Sherlock → Watson → Moriarty.
+ 
+ Ce module implémente l'orchestration avancée pour le workflow 3-agents avec agent Oracle,
+ incluant la sélection cyclique, la terminaison Oracle, et l'intégration avec CluedoOracleState.
  """
+ 
  import asyncio
  import logging
- import re
+ from typing import List, Dict, Any, Optional
  from datetime import datetime
- from typing import List, Dict, Any, Optional, Callable, Awaitable
  
  import semantic_kernel as sk
- from semantic_kernel.contents import ChatMessageContent
- from semantic_kernel.functions.kernel_arguments import KernelArguments
+ from semantic_kernel.functions import kernel_function
  from semantic_kernel.kernel import Kernel
- from semantic_kernel.functions.kernel_function_decorator import kernel_function
 -# Les modules agents ne sont pas disponibles dans cette version de semantic_kernel
 -# Utilisation de fallbacks pour compatibilité
 -try:
 -    from semantic_kernel_compatibility import Agent, AgentGroupChat
 -    from semantic_kernel.agents.strategies.selection.selection_strategy import SelectionStrategy
 -    from semantic_kernel.agents.strategies.termination.termination_strategy import TerminationStrategy
 -    AGENTS_AVAILABLE = True
 -except ImportError:
 -    # Fallbacks pour compatibilité
 -    class Agent:
 -        def __init__(self, name: str, kernel: Kernel = None, **kwargs):
 -            self.name = name
 -            self.kernel = kernel
 -            
 -    class AgentGroupChat:
 -        def __init__(self, agents: List[Agent] = None, **kwargs):
 -            self.agents = agents or []
 -            
 -    class SelectionStrategy:
 -        def select_next_agent(self, agents: List[Agent], last_agent: Agent = None) -> Agent:
 -            return agents[0] if agents else None
 -            
 -    class TerminationStrategy:
 -        def should_terminate(self, messages: List[Any]) -> bool:
 -            return False
 -            
 -    AGENTS_AVAILABLE = False
 +from semantic_kernel.contents.function_call_content import FunctionCallContent
+ from semantic_kernel.contents.chat_message_content import ChatMessageContent
 -from semantic_kernel.functions.kernel_arguments import KernelArguments
 -
 -# Import conditionnel pour les modules filters qui peuvent ne pas exister
 -try:
 -    from semantic_kernel.filters.functions.function_invocation_context import FunctionInvocationContext
 -    from semantic_kernel.filters.filter_types import FilterTypes
 -    FILTERS_AVAILABLE = True
 -except ImportError:
 -    # Fallbacks pour compatibilité
 -    class FunctionInvocationContext:
 -        def __init__(self, **kwargs):
 -            pass
 -            
 -    class FilterTypes:
 -        pass
 -        
 -    FILTERS_AVAILABLE = False
 +# CORRECTIF COMPATIBILITÉ: Utilisation du module de compatibilité pour agents et filters
 +from argumentation_analysis.utils.semantic_kernel_compatibility import (
 +    Agent, AgentGroupChat, SelectionStrategy, TerminationStrategy,
 +    FunctionInvocationContext, FilterTypes
 +)
  # from semantic_kernel.processes.runtime.in_process_runtime import InProcessRuntime  # Module non disponible
  from pydantic import Field
  
diff --cc argumentation_analysis/utils/extract_repair/verify_extracts_with_llm.py
index 0c521df6,3d697479..5b366e60
--- a/argumentation_analysis/utils/extract_repair/verify_extracts_with_llm.py
+++ b/argumentation_analysis/utils/extract_repair/verify_extracts_with_llm.py
@@@ -33,9 -33,9 +33,9 @@@ file_handler.setFormatter(logging.Forma
  logger.addHandler(file_handler)
  
  import semantic_kernel as sk
- from semantic_kernel.contents import ChatMessageContent, AuthorRole
+ from semantic_kernel.contents import ChatMessageContent, ChatRole as AuthorRole
  # CORRECTIF COMPATIBILITÉ: Utilisation du module de compatibilité
 -from semantic_kernel.agents import ChatCompletionAgent
 +from argumentation_analysis.utils.semantic_kernel_compatibility import ChatCompletionAgent
  try:
      # Import relatif depuis le package utils
      logger.info("Tentative d'import relatif...")

==================== COMMIT: ebaa56476c7113f9f4a1038d7c2658a00c7eb681 ====================
commit ebaa56476c7113f9f4a1038d7c2658a00c7eb681
Author: jsboigeEpita <jean-sylvain.boige@epita.fr>
Date:   Mon Jun 16 02:35:53 2025 +0200

    Fix: Revertir les dépendances Python à des versions compatibles avec Python 3.10
    
    Rétablit semantic-kernel>=1.33.0, scipy==1.15.3, networkx==3.4.2. Ajoute un avertissement critique pour semantic-kernel afin d'éviter une régression future.

diff --git a/requirements.txt b/requirements.txt
index 77223b13..076ab21c 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -44,6 +44,12 @@ torch>=1.12.0
 transformers>=4.20.0
 
 # Semantic Kernel - CRITICAL DEPENDENCY
+# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+# CRITICAL WARNING: DO NOT DOWNGRADE SEMANTIC-KERNEL BELOW 1.33.0.
+# This version IS REQUIRED and supports Python 3.10+.
+# Ensure the correct Python environment (3.10+) is active.
+# Downgrading will break the application.
+# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 semantic-kernel>=1.33.0
 # NOTE: Using latest version (>1.0.0) a modern API
 # CRITICAL UPDATE: Resolves Pydantic import errors and modernizes API

==================== COMMIT: d03f2d324e4abede3d8e04b4f499104fbc8f3110 ====================
commit d03f2d324e4abede3d8e04b4f499104fbc8f3110
Author: jsboigeEpita <jean-sylvain.boige@epita.fr>
Date:   Mon Jun 16 02:27:08 2025 +0200

    Fix: Stabilisation progressive des tests unitaires de la webapp

diff --git a/test_jpype_minimal.py b/test_jpype_minimal.py
new file mode 100644
index 00000000..1b039636
--- /dev/null
+++ b/test_jpype_minimal.py
@@ -0,0 +1,71 @@
+# test_jpype_minimal.py
+import jpype
+import jpype.imports # Nécessaire pour certaines versions/configurations de JPype
+import os
+import sys # Pour afficher la version de Python
+
+print(f"--- Début du script de test JPype minimal ---")
+print(f"Version de Python: {sys.version}")
+print(f"Version de JPype: {jpype.__version__}")
+
+# Chemin absolu vers le JDK portable
+jdk_base_path = r"D:\Dev\2025-Epita-Intelligence-Symbolique\libs\portable_jdk\jdk-17.0.11+9"
+# Construire le chemin vers jvm.dll (Windows)
+jvm_dll_path = os.path.join(jdk_base_path, "bin", "server", "jvm.dll")
+
+print(f"Chemin du JDK portable utilisé: {jdk_base_path}")
+print(f"Chemin attendu pour jvm.dll: {jvm_dll_path}")
+
+if not os.path.exists(jvm_dll_path):
+    print(f"ERREUR CRITIQUE: jvm.dll non trouvé à l'emplacement : {jvm_dll_path}")
+    print(f"Veuillez vérifier que le JDK portable est correctement installé à {jdk_base_path}")
+    exit(1)
+else:
+    print(f"Fichier jvm.dll trouvé à : {jvm_dll_path}")
+
+# Classpath vide
+classpath_jars = []
+print(f"Classpath utilisé (vide): {classpath_jars}")
+
+# Options JVM (minimales)
+jvm_options = ["-Xms64m", "-Xmx128m"] # Réduire l'usage mémoire pour le test
+print(f"Options JVM utilisées: {jvm_options}")
+
+try:
+    print(f"\nTentative de démarrage de la JVM avec jpype.startJVM()...")
+    print(f"  jvmpath='{jvm_dll_path}'")
+    print(f"  classpath={classpath_jars}")
+    print(f"  options={jvm_options}")
+
+    jpype.startJVM(
+        jvmpath=jvm_dll_path,
+        classpath=classpath_jars,
+        *jvm_options,
+        convertStrings=False # Option standard que nous utilisons
+    )
+    print("SUCCESS: JVM démarrée avec succès.")
+
+    # Test simple après démarrage
+    try:
+        print("Tentative d'accès à java.lang.System...")
+        System = jpype.JClass("java.lang.System")
+        java_version_from_jvm = System.getProperty("java.version")
+        print(f"SUCCESS: Version Java obtenue depuis la JVM: {java_version_from_jvm}")
+    except Exception as e_jclass:
+        print(f"ERREUR lors du test post-démarrage (JClass): {e_jclass}")
+        import traceback
+        traceback.print_exc()
+
+except Exception as e_start_jvm:
+    print(f"ERREUR CRITIQUE lors de jpype.startJVM(): {e_start_jvm}")
+    import traceback
+    traceback.print_exc()
+finally:
+    if jpype.isJVMStarted():
+        print("\nTentative d'arrêt de la JVM...")
+        jpype.shutdownJVM()
+        print("SUCCESS: JVM arrêtée.")
+    else:
+        print("\nINFO: La JVM n'a pas été démarrée ou a échoué avant l'arrêt.")
+
+print(f"--- Fin du script de test JPype minimal ---")
\ No newline at end of file
diff --git a/tests/_test_numpy_rec_mock.py b/tests/_test_numpy_rec_mock.py
new file mode 100644
index 00000000..3c80bc7f
--- /dev/null
+++ b/tests/_test_numpy_rec_mock.py
@@ -0,0 +1,92 @@
+#!/usr/bin/env python
+# -*- coding: utf-8 -*-
+
+"""
+Test spécifique pour vérifier que le mock numpy.rec fonctionne correctement.
+Ce test vérifie que le problème "ModuleNotFoundError: No module named 'numpy.rec'" est résolu.
+"""
+
+import unittest
+import sys
+import os
+
+# Ajouter la racine du projet à sys.path
+project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
+if project_root not in sys.path:
+    sys.path.insert(0, project_root)
+
+# Forcer l'utilisation du mock numpy
+sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'mocks')) # Doit être après l'ajout de project_root pour prioriser le mock local
+import legacy_numpy_array_mock
+sys.modules['numpy'] = legacy_numpy_array_mock
+
+class TestNumpyRecMock(unittest.TestCase):
+    """Tests pour vérifier que numpy.rec est correctement mocké."""
+    
+    def test_numpy_rec_import(self):
+        """Test que numpy.rec peut être importé sans erreur."""
+        try:
+            import numpy
+            self.assertTrue(hasattr(numpy, 'rec'), "numpy.rec doit exister")
+        except ImportError as e:
+            self.fail(f"Impossible d'importer numpy: {e}")
+    
+    def test_numpy_rec_recarray_exists(self):
+        """Test que numpy.rec.recarray existe."""
+        import numpy
+        self.assertTrue(hasattr(numpy.rec, 'recarray'), "numpy.rec.recarray doit exister")
+    
+    def test_numpy_rec_recarray_instantiation(self):
+        """Test que numpy.rec.recarray peut être instancié."""
+        import numpy
+        
+        # Test avec shape et formats requis
+        arr1 = numpy.rec.recarray((2, 2), formats=['i4', 'f8'], names=['id', 'value'])
+        self.assertIsNotNone(arr1)
+        self.assertEqual(arr1.shape, (2, 2))
+        
+        # Test avec formats et names
+        arr2 = numpy.rec.recarray(5, formats=['i4', 'f8'], names=['x', 'y'])
+        self.assertIsNotNone(arr2)
+        self.assertEqual(arr2.names, ['x', 'y'])
+        self.assertEqual(arr2.formats, ['i4', 'f8'])
+        
+        # Test avec shape et dtype
+        arr3 = numpy.rec.recarray(shape=(3, 3), dtype='f8')
+        self.assertIsNotNone(arr3)
+        self.assertEqual(arr3.shape, (3, 3))
+    
+    def test_numpy_rec_recarray_properties(self):
+        """Test que les propriétés de recarray fonctionnent."""
+        import numpy
+        
+        arr = numpy.rec.recarray(3, formats=['i4', 'f8'], names=['id', 'value'])
+        
+        # Test des propriétés
+        self.assertEqual(arr.names, ['id', 'value'])
+        self.assertEqual(arr.formats, ['i4', 'f8'])
+        
+        # Test d'accès aux champs (doit retourner un ndarray)
+        field_access = arr.id  # Ceci utilise __getattr__
+        self.assertIsNotNone(field_access)
+    
+    def test_pandas_compatibility(self):
+        """Test que le mock est compatible avec l'utilisation par pandas."""
+        import numpy
+        
+        # Simuler ce que pandas pourrait faire
+        try:
+            # pandas essaie souvent d'accéder à numpy.rec.recarray
+            recarray_class = numpy.rec.recarray
+            self.assertIsNotNone(recarray_class)
+            
+            # pandas pourrait instancier un recarray
+            arr = recarray_class((10,), formats=['i4'], names=['data'])
+            self.assertIsNotNone(arr)
+            
+        except Exception as e:
+            self.fail(f"Erreur de compatibilité pandas: {e}")
+
+
+if __name__ == "__main__":
+    unittest.main()
\ No newline at end of file
diff --git a/tests/conftest.py.disabled b/tests/conftest.py.disabled
new file mode 100644
index 00000000..be7f397f
--- /dev/null
+++ b/tests/conftest.py.disabled
@@ -0,0 +1,86 @@
+import pytest
+import jpype
+import os
+from pathlib import Path
+import logging
+
+# Importer les dépendances nécessaires depuis jvm_setup
+# Assurez-vous que PORTABLE_JDK_PATH et shutdown_jvm_if_needed sont accessibles
+# Si jvm_setup.py est dans un parent, ajustez l'import ou copiez les définitions nécessaires.
+# Pour cet exemple, nous supposons que argumentation_analysis est dans le PYTHONPATH
+from argumentation_analysis.core.jvm_setup import PORTABLE_JDK_PATH, shutdown_jvm_if_needed, LIBS_DIR
+
+logger = logging.getLogger(__name__)
+
+@pytest.fixture(scope="function") # Changé de "session" à "function"
+def real_jvm_minimal_function_scope(request):
+    """
+    Fixture de fonction pour démarrer et arrêter la JVM pour chaque test
+    qui en a besoin, en utilisant un classpath minimal (vide pour l'instant).
+    """
+    original_use_real_jpype = os.environ.get('USE_REAL_JPYPE')
+    os.environ['USE_REAL_JPYPE'] = 'true'
+    logger.info(f"FIXTURE real_jvm_minimal_function_scope: USE_REAL_JPYPE forcé à 'true'")
+
+    if jpype.isJVMStarted():
+        # Cela ne devrait pas arriver avec une scope="function" si les tests précédents nettoient bien,
+        # mais c'est une sécurité.
+        logger.warning("FIXTURE real_jvm_minimal_function_scope: JVM déjà démarrée au début d'une fixture de fonction. C'est inattendu.")
+        # Forcer l'arrêt pour essayer de repartir d'un état propre pour CE test.
+        shutdown_jvm_if_needed()
+        if jpype.isJVMStarted():
+            logger.error("FIXTURE real_jvm_minimal_function_scope: Tentative d'arrêt de la JVM préexistante a échoué.")
+            pytest.skip("Impossible de garantir un état JVM propre pour le test.")
+        logger.info("FIXTURE real_jvm_minimal_function_scope: JVM préexistante arrêtée.")
+
+
+    logger.info("FIXTURE real_jvm_minimal_function_scope: Tentative de démarrage de la JVM.")
+    jvm_started_by_fixture = False
+    try:
+        jvmpath = str(Path(PORTABLE_JDK_PATH) / "bin" / "server" / "jvm.dll")
+        classpath = []
+        jvm_options = ['-Xms128m', '-Xmx512m', '-Dfile.encoding=UTF-8', '-Djava.awt.headless=true']
+        
+        logger.info(f"  FIXTURE (function) jvmpath: {jvmpath}")
+        logger.info(f"  FIXTURE (function) classpath: {classpath}")
+        logger.info(f"  FIXTURE (function) jvm_options: {jvm_options}")
+        logger.info(f"  FIXTURE (function) convertStrings: False")
+
+        jpype.startJVM(
+            jvmpath=jvmpath,
+            classpath=classpath,
+            *jvm_options,
+            convertStrings=False
+        )
+        jvm_started_by_fixture = True
+        logger.info("FIXTURE real_jvm_minimal_function_scope: jpype.startJVM() exécuté avec succès.")
+        
+        yield # Les tests s'exécutent ici
+
+    except Exception as e:
+        logger.critical(f"FIXTURE real_jvm_minimal_function_scope: ERREUR CRITIQUE lors du démarrage de la JVM: {e}", exc_info=True)
+        raise
+    finally:
+        logger.info("FIXTURE real_jvm_minimal_function_scope: Bloc finally atteint.")
+        if jvm_started_by_fixture and jpype.isJVMStarted():
+            logger.info("FIXTURE real_jvm_minimal_function_scope: Arrêt de la JVM démarrée par cette fixture.")
+            shutdown_jvm_if_needed()
+            logger.info("FIXTURE real_jvm_minimal_function_scope: JVM arrêtée.")
+        elif jpype.isJVMStarted(): # Si elle est démarrée mais pas par cette fixture (ex: échec avant jvm_started_by_fixture=True)
+            logger.warning("FIXTURE real_jvm_minimal_function_scope: La JVM est démarrée mais n'a peut-être pas été initiée correctement par cette fixture. Tentative d'arrêt.")
+            shutdown_jvm_if_needed()
+        
+        if original_use_real_jpype is not None:
+            os.environ['USE_REAL_JPYPE'] = original_use_real_jpype
+            logger.info(f"FIXTURE real_jvm_minimal_function_scope: USE_REAL_JPYPE restauré à '{original_use_real_jpype}'")
+        elif 'USE_REAL_JPYPE' in os.environ:
+            del os.environ['USE_REAL_JPYPE']
+            logger.info(f"FIXTURE real_jvm_minimal_function_scope: USE_REAL_JPYPE supprimé.")
+
+# Vous pouvez ajouter d'autres fixtures ou configurations ici si nécessaire.
+# Par exemple, la logique de jpype_setup.py pourrait être intégrée ici
+# ou appelée depuis des fixtures si elle est toujours pertinente.
+# Pour l'instant, nous nous concentrons sur le démarrage minimal de la JVM.
+
+# Assurer que les logs de cette fixture sont visibles
+logging.getLogger("tests.conftest").setLevel(logging.INFO)
diff --git a/tests/e2e/python/test_framework_builder.py b/tests/e2e/python/test_framework_builder.py
index bf467cde..24620464 100644
--- a/tests/e2e/python/test_framework_builder.py
+++ b/tests/e2e/python/test_framework_builder.py
@@ -2,7 +2,7 @@
 from playwright.sync_api import Page, expect, TimeoutError
 
 # Import de la classe PlaywrightHelpers depuis le conftest unifié
-from .conftest import PlaywrightHelpers
+from ..conftest import PlaywrightHelpers
 
 
 @pytest.mark.skip(reason="Disabling all functional tests to isolate backend test failures.")
diff --git a/tests/minimal_jvm_pytest_test.py b/tests/minimal_jvm_pytest_test.py
new file mode 100644
index 00000000..ebf9fa1c
--- /dev/null
+++ b/tests/minimal_jvm_pytest_test.py
@@ -0,0 +1,103 @@
+import os
+import pytest # Importer pytest pour s'assurer qu'on est dans son contexte
+import jpype
+from pathlib import Path # Pour construire jvmpath dans la fonction locale
+from argumentation_analysis.core.jvm_setup import shutdown_jvm_if_needed, PORTABLE_JDK_PATH, LIBS_DIR # initialize_jvm n'est plus utilisé directement ici
+import logging
+
+# Configurer un logger pour voir les messages de jvm_setup et d'autres modules
+# Cela aidera à capturer plus d'informations si le test échoue.
+logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
+
+# Récupérer le logger spécifique utilisé dans jvm_setup.py pour s'assurer que ses messages sont visibles
+jpype_logger = logging.getLogger("Orchestration.JPype")
+jpype_logger.setLevel(logging.INFO) # Assurer que les messages INFO de jvm_setup sont capturés
+
+# Si d'autres loggers sont pertinents (par exemple, celui de jpype_setup.py), les configurer aussi.
+# jpype_setup_logger = logging.getLogger("JPypeSetup") # Ajuster le nom si nécessaire
+# jpype_setup_logger.setLevel(logging.INFO)
+
+
+def local_start_the_jvm_directly():
+    """Fonction locale pour encapsuler l'appel direct à jpype.startJVM qui fonctionnait."""
+    print("Appel direct de jpype.startJVM() depuis une fonction LOCALE au test...")
+    
+    jvmpath = str(Path(PORTABLE_JDK_PATH) / "bin" / "server" / "jvm.dll")
+    classpath = [] # Classpath vide pour le test
+    # Utiliser les options qui ont fonctionné lors de l'appel direct
+    jvm_options = ['-Xms128m', '-Xmx512m', '-Dfile.encoding=UTF-8', '-Djava.awt.headless=true']
+    
+    print(f"  LOCAL_CALL jvmpath: {jvmpath}")
+    print(f"  LOCAL_CALL classpath: {classpath}")
+    print(f"  LOCAL_CALL jvm_options: {jvm_options}")
+    print(f"  LOCAL_CALL convertStrings: False")
+
+    jpype.startJVM(
+        jvmpath=jvmpath,
+        classpath=classpath,
+        *jvm_options,
+        convertStrings=False
+    )
+    print(f"LOCAL_CALL jpype.startJVM() exécuté.")
+    return True # Simuler le retour de initialize_jvm
+
+@pytest.mark.real_jpype # Marqueur pour indiquer que ce test nécessite la vraie JVM
+def test_minimal_jvm_startup_in_pytest(caplog): # Nom de fixture retiré
+    """
+    Teste le démarrage minimal de la JVM via une fonction LOCALE dans un contexte pytest.
+    """
+    caplog.set_level(logging.INFO)
+
+    print(f"\n--- Début de test_minimal_jvm_startup_in_pytest (appel local) ---")
+    original_use_real_jpype = os.environ.get('USE_REAL_JPYPE')
+    os.environ['USE_REAL_JPYPE'] = 'true'
+    print(f"Variable d'environnement USE_REAL_JPYPE (forcée pour ce test): '{os.environ.get('USE_REAL_JPYPE')}'")
+    
+    print(f"Chemin JDK portable (variable globale importée): {PORTABLE_JDK_PATH}")
+    print(f"Chemin LIBS_DIR (variable globale importée): {LIBS_DIR}")
+
+    jvm_was_started_before_this_test = jpype.isJVMStarted()
+    print(f"JVM démarrée avant l'appel à la fonction locale (jpype.isJVMStarted()): {jvm_was_started_before_this_test}")
+    
+    call_succeeded = False
+    try:
+        print("Appel de local_start_the_jvm_directly()...")
+        call_succeeded = local_start_the_jvm_directly()
+        
+        print(f"local_start_the_jvm_directly() a retourné: {call_succeeded}")
+        assert call_succeeded, "local_start_the_jvm_directly devrait retourner True."
+        
+        current_jvm_status = jpype.isJVMStarted()
+        print(f"État de la JVM après local_start_the_jvm_directly (jpype.isJVMStarted()): {current_jvm_status}")
+        assert current_jvm_status, "La JVM devrait être marquée comme démarrée après un appel local réussi."
+        
+        print("SUCCESS: JVM démarrée via une fonction LOCALE dans le contexte pytest.")
+
+    except Exception as e:
+        print(f"ERREUR CRITIQUE lors de l'appel à local_start_the_jvm_directly ou des assertions: {e}")
+        # Afficher les logs capturés par caplog pour aider au diagnostic
+        log_messages = "\n".join([f"LOG CAPTURÉ ({record.levelname}): {record.message}" for record in caplog.records])
+        print(f"Logs capturés par caplog:\n{log_messages}")
+        raise # Relancer l'exception pour que pytest la marque comme un échec
+    finally:
+        print(f"--- Bloc finally de test_minimal_jvm_startup_in_pytest ---")
+        # N'arrêter la JVM que si ce test l'a potentiellement démarrée et qu'elle est toujours active.
+        # Si jvm_was_started_before_this_test est True, un autre test/fixture l'a démarrée.
+        if call_succeeded and jpype.isJVMStarted() and not jvm_was_started_before_this_test:
+             print("Tentative d'arrêt de la JVM (car démarrée par l'appel local)...")
+             shutdown_jvm_if_needed() # Utilise toujours la fonction de jvm_setup pour l'arrêt
+             print("Arrêt de la JVM tenté.")
+        elif not call_succeeded and jpype.isJVMStarted() and not jvm_was_started_before_this_test:
+             print("Appel local a échoué mais la JVM semble démarrée. Tentative d'arrêt...")
+             shutdown_jvm_if_needed()
+             print("Arrêt de la JVM tenté après échec de l'appel local.")
+        else:
+            print("La JVM n'a pas été (re)démarrée par ce test ou était déjà démarrée / est déjà arrêtée.")
+        
+        # Restaurer la variable d'environnement USE_REAL_JPYPE si elle existait
+        if original_use_real_jpype is not None:
+            os.environ['USE_REAL_JPYPE'] = original_use_real_jpype
+        elif 'USE_REAL_JPYPE' in os.environ: # Si elle n'existait pas, la supprimer
+            del os.environ['USE_REAL_JPYPE']
+        print(f"Variable d'environnement USE_REAL_JPYPE restaurée à: '{os.environ.get('USE_REAL_JPYPE')}'")
+        print(f"--- Fin de test_minimal_jvm_startup_in_pytest ---")
\ No newline at end of file
diff --git a/tests/mocks/jpype_setup.py.disabled b/tests/mocks/jpype_setup.py.disabled
new file mode 100644
index 00000000..a615299e
--- /dev/null
+++ b/tests/mocks/jpype_setup.py.disabled
@@ -0,0 +1,382 @@
+import sys
+import os
+import pytest
+from unittest.mock import MagicMock
+import importlib.util
+from argumentation_analysis.core.jvm_setup import shutdown_jvm_if_needed
+import logging
+
+# --- Configuration du Logger ---
+logger = logging.getLogger(__name__)
+# Configuration basique si le logger n'est pas déjà configuré par pytest ou autre
+if not logger.handlers:
+    handler = logging.StreamHandler(sys.stdout)
+    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
+    handler.setFormatter(formatter)
+    logger.addHandler(handler)
+    logger.setLevel(logging.INFO) # Ou logging.DEBUG pour plus de détails
+    logger.propagate = False
+
+# --- Détermination de la disponibilité du vrai JPype via variable d'environnement ---
+# Cette variable est utilisée par les décorateurs skipif dans les fichiers de test.
+logger.info(f"jpype_setup.py: Évaluation de _REAL_JPYPE_AVAILABLE...")
+logger.info(f"jpype_setup.py: Valeur brute de os.environ.get('USE_REAL_JPYPE', 'false'): '{os.environ.get('USE_REAL_JPYPE', 'false')}'")
+_REAL_JPYPE_AVAILABLE = os.environ.get('USE_REAL_JPYPE', 'false').lower() in ('true', '1')
+logger.info(f"jpype_setup.py: _REAL_JPYPE_AVAILABLE évalué à: {_REAL_JPYPE_AVAILABLE}")
+# Les prints de débogage précédents ont confirmé que _REAL_JPYPE_AVAILABLE est correctement évalué.
+# La cause du skip était une erreur dans la fixture integration_jvm (chemin des libs).
+
+
+# --- Sauvegarde du module JPype potentiellement pré-importé ou import frais ---
+_REAL_JPYPE_MODULE = None
+_PRE_EXISTING_JPYPE_IN_SYS_MODULES = sys.modules.get('jpype')
+
+if _PRE_EXISTING_JPYPE_IN_SYS_MODULES:
+    _REAL_JPYPE_MODULE = _PRE_EXISTING_JPYPE_IN_SYS_MODULES
+    logger.info(f"jpype_setup.py: _REAL_JPYPE_MODULE initialisé à partir de _PRE_EXISTING_JPYPE_IN_SYS_MODULES (ID: {id(_REAL_JPYPE_MODULE)}).")
+else:
+    logger.info("jpype_setup.py: JPype non préchargé, tentative d'import frais.")
+    try:
+        import jpype as r_jpype_fresh_import
+        _REAL_JPYPE_MODULE = r_jpype_fresh_import
+        logger.info(f"jpype_setup.py: Vrai module JPype importé fraîchement (ID: {id(_REAL_JPYPE_MODULE)}).")
+    except ImportError as e_fresh_import:
+        logger.warning(f"jpype_setup.py: Le vrai module JPype n'a pas pu être importé fraîchement: {e_fresh_import}")
+        _REAL_JPYPE_MODULE = None
+    except NameError as e_name_error_fresh_import:
+        logger.error(f"jpype_setup.py: NameError lors de l'import frais de JPype: {e_name_error_fresh_import}.")
+        _REAL_JPYPE_MODULE = None
+
+if _REAL_JPYPE_MODULE is None:
+    logger.error("jpype_setup.py: _REAL_JPYPE_MODULE EST NONE après la tentative d'initialisation.")
+else:
+    logger.info(f"jpype_setup.py: _REAL_JPYPE_MODULE est initialisé (ID: {id(_REAL_JPYPE_MODULE)}) avant la définition des fixtures.")
+# Initialisation des globales pour les objets mock, au cas où ils ne seraient pas créés
+_JPYPE_MODULE_MOCK_OBJ_GLOBAL = None
+_MOCK_DOT_JPYPE_MODULE_GLOBAL = None
+
+# --- Mock JPype ---
+if not _REAL_JPYPE_AVAILABLE:
+    try:
+        import jpype_mock # Importer le module directement
+        # Importer le vrai module mock d'imports depuis le sous-package jpype_components
+        from jpype_components.imports import imports as actual_mock_jpype_imports_module
+
+        jpype_module_mock_obj = MagicMock(name="jpype_module_mock")
+        jpype_module_mock_obj.__path__ = []
+        jpype_module_mock_obj.isJVMStarted = jpype_mock.isJVMStarted
+        jpype_module_mock_obj.startJVM = jpype_mock.startJVM
+        jpype_module_mock_obj.getJVMPath = jpype_mock.getJVMPath
+        jpype_module_mock_obj.getJVMVersion = jpype_mock.getJVMVersion
+        jpype_module_mock_obj.getDefaultJVMPath = jpype_mock.getDefaultJVMPath
+        jpype_module_mock_obj.JClass = jpype_mock.JClass
+        jpype_module_mock_obj.JException = jpype_mock.JException
+        jpype_module_mock_obj.JObject = jpype_mock.JObject
+        jpype_module_mock_obj.JVMNotFoundException = jpype_mock.JVMNotFoundException
+        jpype_module_mock_obj.__version__ = '1.4.1.mock' # ou jpype_mock.__version__ si défini
+        jpype_module_mock_obj.imports = actual_mock_jpype_imports_module
+        _JPYPE_MODULE_MOCK_OBJ_GLOBAL = jpype_module_mock_obj
+        _MOCK_DOT_JPYPE_MODULE_GLOBAL = jpype_mock._jpype # Accéder à _jpype depuis le module jpype_mock importé
+        logger.info("jpype_setup.py: Mock JPype préparé.")
+    except ImportError as e_jpype:
+        logger.error(f"jpype_setup.py: ERREUR CRITIQUE lors de l'import de jpype_mock ou ses composants: {e_jpype}. Utilisation de mocks de fallback pour JPype.")
+        _fb_jpype_mock = MagicMock(name="jpype_fallback_mock")
+        _fb_jpype_mock.imports = MagicMock(name="jpype.imports_fallback_mock")
+        _fb_dot_jpype_mock = MagicMock(name="_jpype_fallback_mock")
+
+        _JPYPE_MODULE_MOCK_OBJ_GLOBAL = _fb_jpype_mock
+        _MOCK_DOT_JPYPE_MODULE_GLOBAL = _fb_dot_jpype_mock
+        logger.info("jpype_setup.py: Mock JPype de FALLBACK préparé et assigné aux variables globales de mock.")
+
+
+@pytest.fixture(scope="function", autouse=True)
+def activate_jpype_mock_if_needed(request):
+    global _JPYPE_MODULE_MOCK_OBJ_GLOBAL, _MOCK_DOT_JPYPE_MODULE_GLOBAL, _REAL_JPYPE_MODULE
+
+    # Déterminer si le vrai JPype doit être utilisé
+    env_use_real_jpype = os.environ.get('USE_REAL_JPYPE', 'false').lower() in ('true', '1')
+    
+    use_real_jpype_marker = False
+    if request.node.get_closest_marker("real_jpype"):
+        use_real_jpype_marker = True
+        
+    use_real_jpype_path = False
+    path_str = str(request.node.fspath).replace(os.sep, '/')
+    if 'tests/integration/' in path_str or 'tests/minimal_jpype_tweety_tests/' in path_str:
+        use_real_jpype_path = True
+        
+    final_use_real_jpype = False
+    if env_use_real_jpype:
+        final_use_real_jpype = True
+        logger.info(f"Test {request.node.name}: REAL JPype forcé par la variable d'environnement USE_REAL_JPYPE.")
+    elif use_real_jpype_marker:
+        final_use_real_jpype = True
+        logger.info(f"Test {request.node.name}: REAL JPype demandé par le marqueur 'real_jpype'.")
+    elif use_real_jpype_path:
+        final_use_real_jpype = True
+        logger.info(f"Test {request.node.name}: REAL JPype activé par chemin ({path_str}).")
+    # else: final_use_real_jpype reste False
+
+    if final_use_real_jpype:
+        logger.info(f"Test {request.node.name} demande REAL JPype. Configuration de sys.modules pour utiliser le vrai JPype.")
+        if _REAL_JPYPE_MODULE:
+            sys.modules['jpype'] = _REAL_JPYPE_MODULE
+            if hasattr(_REAL_JPYPE_MODULE, '_jpype'):
+                sys.modules['_jpype'] = _REAL_JPYPE_MODULE._jpype
+            elif '_jpype' in sys.modules and sys.modules.get('_jpype') is not getattr(_REAL_JPYPE_MODULE, '_jpype', None) :
+                del sys.modules['_jpype']
+            if hasattr(_REAL_JPYPE_MODULE, 'imports'):
+                sys.modules['jpype.imports'] = _REAL_JPYPE_MODULE.imports
+            elif 'jpype.imports' in sys.modules and sys.modules.get('jpype.imports') is not getattr(_REAL_JPYPE_MODULE, 'imports', None):
+                del sys.modules['jpype.imports']
+            logger.debug(f"REAL JPype (ID: {id(_REAL_JPYPE_MODULE)}) est maintenant sys.modules['jpype'].")
+        else:
+            logger.error(f"Test {request.node.name} demande REAL JPype, mais _REAL_JPYPE_MODULE n'est pas disponible. Test échouera probablement.")
+        yield
+    else:
+        logger.info(f"Test {request.node.name} utilise MOCK JPype.")
+        try:
+            jpype_components_jvm_module = sys.modules.get('tests.mocks.jpype_components.jvm')
+            if jpype_components_jvm_module:
+                if hasattr(jpype_components_jvm_module, '_jvm_started'):
+                    jpype_components_jvm_module._jvm_started = False
+                if hasattr(jpype_components_jvm_module, '_jvm_path'):
+                    jpype_components_jvm_module._jvm_path = None
+                if hasattr(_JPYPE_MODULE_MOCK_OBJ_GLOBAL, 'config') and hasattr(_JPYPE_MODULE_MOCK_OBJ_GLOBAL.config, 'jvm_path'):
+                    _JPYPE_MODULE_MOCK_OBJ_GLOBAL.config.jvm_path = None
+                logger.info("État (_jvm_started, _jvm_path, config.jvm_path) du mock JPype réinitialisé pour le test.")
+            else:
+                logger.warning("Impossible de réinitialiser l'état du mock JPype: module 'tests.mocks.jpype_components.jvm' non trouvé.")
+        except Exception as e_reset_mock:
+            logger.error(f"Erreur lors de la réinitialisation de l'état du mock JPype: {e_reset_mock}")
+
+        original_modules = {}
+        modules_to_handle = ['jpype', '_jpype', 'jpype._core', 'jpype.imports', 'jpype.types', 'jpype.config', 'jpype.JProxy']
+
+        if 'jpype.imports' in sys.modules and \
+           hasattr(sys.modules['jpype.imports'], '_jpype') and \
+           _MOCK_DOT_JPYPE_MODULE_GLOBAL is not None and \
+           hasattr(_MOCK_DOT_JPYPE_MODULE_GLOBAL, 'isStarted'):
+            if sys.modules['jpype.imports']._jpype is not _MOCK_DOT_JPYPE_MODULE_GLOBAL:
+                if 'jpype.imports._jpype_original' not in original_modules:
+                     original_modules['jpype.imports._jpype_original'] = sys.modules['jpype.imports']._jpype
+                logger.debug(f"Patch direct de sys.modules['jpype.imports']._jpype avec notre mock _jpype.")
+                sys.modules['jpype.imports']._jpype = _MOCK_DOT_JPYPE_MODULE_GLOBAL
+            else:
+                logger.debug("sys.modules['jpype.imports']._jpype est déjà notre mock.")
+
+        for module_name in modules_to_handle:
+            if module_name in sys.modules:
+                is_current_module_our_mock = False
+                if module_name == 'jpype' and sys.modules[module_name] is _JPYPE_MODULE_MOCK_OBJ_GLOBAL: is_current_module_our_mock = True
+                elif module_name in ['_jpype', 'jpype._core'] and sys.modules[module_name] is _MOCK_DOT_JPYPE_MODULE_GLOBAL: is_current_module_our_mock = True
+                elif module_name == 'jpype.imports' and hasattr(_JPYPE_MODULE_MOCK_OBJ_GLOBAL, 'imports') and sys.modules[module_name] is _JPYPE_MODULE_MOCK_OBJ_GLOBAL.imports: is_current_module_our_mock = True
+                elif module_name == 'jpype.config' and hasattr(_JPYPE_MODULE_MOCK_OBJ_GLOBAL, 'config') and sys.modules[module_name] is _JPYPE_MODULE_MOCK_OBJ_GLOBAL.config: is_current_module_our_mock = True
+
+                if not is_current_module_our_mock and module_name not in original_modules:
+                    original_modules[module_name] = sys.modules.pop(module_name)
+                    logger.debug(f"Supprimé et sauvegardé sys.modules['{module_name}']")
+                elif module_name in sys.modules and is_current_module_our_mock:
+                    del sys.modules[module_name]
+                    logger.debug(f"Supprimé notre mock préexistant pour sys.modules['{module_name}'].")
+                elif module_name in sys.modules:
+                    del sys.modules[module_name]
+                    logger.debug(f"Supprimé sys.modules['{module_name}'] (sauvegarde prioritaire existante).")
+
+        sys.modules['jpype'] = _JPYPE_MODULE_MOCK_OBJ_GLOBAL
+        sys.modules['_jpype'] = _MOCK_DOT_JPYPE_MODULE_GLOBAL
+        sys.modules['jpype._core'] = _MOCK_DOT_JPYPE_MODULE_GLOBAL
+        if hasattr(_JPYPE_MODULE_MOCK_OBJ_GLOBAL, 'imports'):
+            sys.modules['jpype.imports'] = _JPYPE_MODULE_MOCK_OBJ_GLOBAL.imports
+        else:
+            sys.modules['jpype.imports'] = MagicMock(name="jpype.imports_fallback_in_fixture")
+
+        if hasattr(_JPYPE_MODULE_MOCK_OBJ_GLOBAL, 'config'):
+            sys.modules['jpype.config'] = _JPYPE_MODULE_MOCK_OBJ_GLOBAL.config
+        else:
+            sys.modules['jpype.config'] = MagicMock(name="jpype.config_fallback_in_fixture")
+
+        mock_types_module = MagicMock(name="jpype.types_mock_module_dynamic_in_fixture")
+        for type_name in ["JString", "JArray", "JObject", "JBoolean", "JInt", "JDouble", "JLong", "JFloat", "JShort", "JByte", "JChar"]:
+            if hasattr(_JPYPE_MODULE_MOCK_OBJ_GLOBAL, type_name):
+                setattr(mock_types_module, type_name, getattr(_JPYPE_MODULE_MOCK_OBJ_GLOBAL, type_name))
+            else:
+                setattr(mock_types_module, type_name, MagicMock(name=f"Mock{type_name}_in_fixture"))
+        sys.modules['jpype.types'] = mock_types_module
+
+        sys.modules['jpype.JProxy'] = MagicMock(name="jpype.JProxy_mock_module_dynamic_in_fixture")
+        logger.debug(f"Mocks JPype (principal, _jpype/_core, imports, config, types, JProxy) mis en place.")
+        yield
+        logger.debug(f"Nettoyage après test {request.node.name} (utilisation du mock).")
+
+        if 'jpype.imports._jpype_original' in original_modules:
+            if 'jpype.imports' in sys.modules and hasattr(sys.modules['jpype.imports'], '_jpype'):
+                sys.modules['jpype.imports']._jpype = original_modules['jpype.imports._jpype_original']
+                logger.debug("Restauré jpype.imports._jpype à sa valeur originale.")
+            del original_modules['jpype.imports._jpype_original']
+
+        modules_we_set_up_in_fixture = ['jpype', '_jpype', 'jpype._core', 'jpype.imports', 'jpype.config', 'jpype.types', 'jpype.JProxy']
+        for module_name in modules_we_set_up_in_fixture:
+            current_module_in_sys = sys.modules.get(module_name)
+            is_our_specific_mock_from_fixture = False
+            if module_name == 'jpype' and current_module_in_sys is _JPYPE_MODULE_MOCK_OBJ_GLOBAL: is_our_specific_mock_from_fixture = True
+            elif module_name in ['_jpype', 'jpype._core'] and current_module_in_sys is _MOCK_DOT_JPYPE_MODULE_GLOBAL: is_our_specific_mock_from_fixture = True
+            elif module_name == 'jpype.imports' and hasattr(_JPYPE_MODULE_MOCK_OBJ_GLOBAL, 'imports') and current_module_in_sys is _JPYPE_MODULE_MOCK_OBJ_GLOBAL.imports: is_our_specific_mock_from_fixture = True
+            elif module_name == 'jpype.config' and hasattr(_JPYPE_MODULE_MOCK_OBJ_GLOBAL, 'config') and current_module_in_sys is _JPYPE_MODULE_MOCK_OBJ_GLOBAL.config: is_our_specific_mock_from_fixture = True
+            elif module_name == 'jpype.types' and current_module_in_sys is mock_types_module: is_our_specific_mock_from_fixture = True
+            elif module_name == 'jpype.JProxy' and isinstance(current_module_in_sys, MagicMock) and hasattr(current_module_in_sys, 'name') and "jpype.JProxy_mock_module_dynamic_in_fixture" in current_module_in_sys.name : is_our_specific_mock_from_fixture = True
+
+            if is_our_specific_mock_from_fixture:
+                if module_name in sys.modules:
+                    del sys.modules[module_name]
+                    logger.debug(f"Supprimé notre mock pour sys.modules['{module_name}']")
+
+        for module_name, original_module in original_modules.items():
+            sys.modules[module_name] = original_module
+            logger.debug(f"Restauré sys.modules['{module_name}'] à {original_module}")
+
+        logger.info(f"État de JPype restauré après test {request.node.name} (utilisation du mock).")
+
+def pytest_sessionstart(session):
+    global _REAL_JPYPE_MODULE, _JPYPE_MODULE_MOCK_OBJ_GLOBAL, logger
+    logger.info("jpype_setup.py: pytest_sessionstart hook triggered.")
+    if not hasattr(logger, 'info'):
+        import logging
+        logger = logging.getLogger(__name__)
+
+    if _REAL_JPYPE_MODULE and _REAL_JPYPE_MODULE is not _JPYPE_MODULE_MOCK_OBJ_GLOBAL:
+        logger.info("   pytest_sessionstart: Real JPype module is available.")
+        # try:
+            # La logique de configuration de destroy_jvm et l'import de jpype.config
+            # sont maintenant gérés de manière centralisée par initialize_jvm lors du premier démarrage réel.
+            # Commenter cette section pour éviter les conflits ou les configurations prématurées.
+            # original_sys_jpype_module = sys.modules.get('jpype')
+            # if sys.modules.get('jpype') is not _REAL_JPYPE_MODULE:
+            #     sys.modules['jpype'] = _REAL_JPYPE_MODULE
+            #     logger.info("   pytest_sessionstart: Temporarily set sys.modules['jpype'] to _REAL_JPYPE_MODULE for config import.")
+
+            # if not hasattr(_REAL_JPYPE_MODULE, 'config') or _REAL_JPYPE_MODULE.config is None:
+            #     logger.info("   pytest_sessionstart: Attempting to import jpype.config explicitly.")
+            #     import jpype.config # This might be problematic if called before JVM start or with wrong classpath context
+            
+            # if original_sys_jpype_module is not None and sys.modules.get('jpype') is not original_sys_jpype_module:
+            #     sys.modules['jpype'] = original_sys_jpype_module
+            #     logger.info("   pytest_sessionstart: Restored original sys.modules['jpype'].")
+            # elif original_sys_jpype_module is None and 'jpype' in sys.modules and sys.modules['jpype'] is _REAL_JPYPE_MODULE:
+            #     pass # It was correctly set
+            
+            # Tentative d'assurer que jpype.config est le vrai config, si possible.
+            # initialize_jvm s'occupera de mettre destroy_jvm à False.
+            # Bloc try/except correctement indenté :
+        try:
+            if not hasattr(_REAL_JPYPE_MODULE, 'config') or _REAL_JPYPE_MODULE.config is None:
+                logger.info("   pytest_sessionstart: _REAL_JPYPE_MODULE.config non trouvé, tentative d'import de jpype.config.")
+                _current_sys_jpype = sys.modules.get('jpype')
+                sys.modules['jpype'] = _REAL_JPYPE_MODULE
+                import jpype.config
+                sys.modules['jpype'] = _current_sys_jpype
+                logger.info(f"   pytest_sessionstart: Import de jpype.config tenté. hasattr(config): {hasattr(_REAL_JPYPE_MODULE, 'config')}")
+
+            if hasattr(_REAL_JPYPE_MODULE, 'config') and _REAL_JPYPE_MODULE.config is not None:
+                if 'jpype.config' not in sys.modules or sys.modules.get('jpype.config') is not _REAL_JPYPE_MODULE.config:
+                    sys.modules['jpype.config'] = _REAL_JPYPE_MODULE.config
+                    logger.info("   pytest_sessionstart: Assuré que sys.modules['jpype.config'] est _REAL_JPYPE_MODULE.config.")
+            else:
+                logger.warning("   pytest_sessionstart: _REAL_JPYPE_MODULE.config toujours non disponible après tentative d'import.")
+
+        except ImportError as e_cfg_imp_sess_start:
+            logger.error(f"   pytest_sessionstart: ImportError lors de la tentative d'import de jpype.config: {e_cfg_imp_sess_start}")
+        except Exception as e_sess_start_cfg:
+            logger.error(f"   pytest_sessionstart: Erreur inattendue lors de la manipulation de jpype.config: {e_sess_start_cfg}", exc_info=True)
+
+        logger.info("   pytest_sessionstart: La configuration de jpype.config.destroy_jvm est gérée par initialize_jvm.")
+    elif _JPYPE_MODULE_MOCK_OBJ_GLOBAL and _REAL_JPYPE_MODULE is _JPYPE_MODULE_MOCK_OBJ_GLOBAL:
+        logger.info("   pytest_sessionstart: JPype module is the MOCK. No changes to destroy_jvm needed for the mock.")
+    else:
+        logger.info("   pytest_sessionstart: Real JPype module not definitively available or identified as mock. La configuration de jpype.config est gérée par initialize_jvm.")
+
+def pytest_sessionfinish(session, exitstatus):
+    global _REAL_JPYPE_MODULE, _JPYPE_MODULE_MOCK_OBJ_GLOBAL, logger
+    logger.info(f"jpype_setup.py: pytest_sessionfinish hook triggered. Exit status: {exitstatus}")
+
+    # Déterminer si le vrai JPype a été utilisé pour la session ou le dernier test
+    # Cela est une heuristique. Idéalement, on saurait si la JVM a été démarrée par notre code.
+    real_jpype_was_potentially_used = False
+    if _REAL_JPYPE_MODULE and sys.modules.get('jpype') is _REAL_JPYPE_MODULE:
+        logger.info("   pytest_sessionfinish: sys.modules['jpype'] IS _REAL_JPYPE_MODULE. Le vrai JPype a potentiellement été utilisé.")
+        real_jpype_was_potentially_used = True
+    elif _REAL_JPYPE_MODULE and _REAL_JPYPE_MODULE is not _JPYPE_MODULE_MOCK_OBJ_GLOBAL:
+        logger.info("   pytest_sessionfinish: _REAL_JPYPE_MODULE est disponible et n'est pas le mock global. Le vrai JPype a potentiellement été utilisé.")
+        real_jpype_was_potentially_used = True
+    else:
+        logger.info("   pytest_sessionfinish: sys.modules['jpype'] n'est pas _REAL_JPYPE_MODULE ou _REAL_JPYPE_MODULE est le mock. Le mock JPype a probablement été utilisé.")
+
+    if real_jpype_was_potentially_used:
+        logger.info("   pytest_sessionfinish: Tentative d'arrêt de la JVM via shutdown_jvm_if_needed() car le vrai JPype a potentiellement été utilisé.")
+        try:
+            # S'assurer que le vrai jpype est dans sys.modules pour que shutdown_jvm_if_needed fonctionne correctement
+            original_jpype_in_sys = sys.modules.get('jpype')
+            if original_jpype_in_sys is not _REAL_JPYPE_MODULE and _REAL_JPYPE_MODULE is not None:
+                logger.info(f"   pytest_sessionfinish: Temporairement, sys.modules['jpype'] = _REAL_JPYPE_MODULE (ID: {id(_REAL_JPYPE_MODULE)}) pour shutdown.")
+                sys.modules['jpype'] = _REAL_JPYPE_MODULE
+            
+            shutdown_jvm_if_needed() # Appel de notre fonction centralisée
+            
+            # Restaurer l'état précédent de sys.modules['jpype'] si modifié
+            if original_jpype_in_sys is not None and sys.modules.get('jpype') is not original_jpype_in_sys:
+                logger.info(f"   pytest_sessionfinish: Restauration de sys.modules['jpype'] à son état original (ID: {id(original_jpype_in_sys)}).")
+                sys.modules['jpype'] = original_jpype_in_sys
+            elif original_jpype_in_sys is None and 'jpype' in sys.modules: # Si on l'a ajouté et qu'il n'y était pas
+                del sys.modules['jpype']
+                logger.info("   pytest_sessionfinish: sys.modules['jpype'] supprimé car il n'était pas là initialement.")
+
+        except Exception as e_shutdown:
+            logger.error(f"   pytest_sessionfinish: Erreur lors de l'appel à shutdown_jvm_if_needed(): {e_shutdown}", exc_info=True)
+        
+        # La logique ci-dessous pour restaurer sys.modules['jpype'] et sys.modules['jpype.config']
+        # est importante si la JVM n'est PAS arrêtée par JPype via atexit (destroy_jvm=False).
+        # Si shutdown_jvm_if_needed() a bien arrêté la JVM, cette partie est moins critique mais ne fait pas de mal.
+        logger.info("   pytest_sessionfinish: Vérification de l'état de la JVM après tentative d'arrêt.")
+        try:
+            jvm_still_started_after_shutdown_attempt = False
+            if _REAL_JPYPE_MODULE and hasattr(_REAL_JPYPE_MODULE, 'isJVMStarted'):
+                 # Assurer que _REAL_JPYPE_MODULE est utilisé pour la vérification
+                _current_jpype_for_check = sys.modules.get('jpype')
+                if _current_jpype_for_check is not _REAL_JPYPE_MODULE and _REAL_JPYPE_MODULE is not None:
+                    sys.modules['jpype'] = _REAL_JPYPE_MODULE
+                jvm_still_started_after_shutdown_attempt = _REAL_JPYPE_MODULE.isJVMStarted()
+                if _current_jpype_for_check is not None and _current_jpype_for_check is not _REAL_JPYPE_MODULE: # restaurer
+                    sys.modules['jpype'] = _current_jpype_for_check
+                elif _current_jpype_for_check is None and 'jpype' in sys.modules and sys.modules['jpype'] is _REAL_JPYPE_MODULE:
+                    del sys.modules['jpype']
+
+
+            logger.info(f"   pytest_sessionfinish: JVM encore démarrée après tentative d'arrêt: {jvm_still_started_after_shutdown_attempt}")
+
+            destroy_jvm_is_false = False # Valeur par défaut si config non accessible
+            if _REAL_JPYPE_MODULE and hasattr(_REAL_JPYPE_MODULE, 'config') and _REAL_JPYPE_MODULE.config is not None and hasattr(_REAL_JPYPE_MODULE.config, 'destroy_jvm'):
+                destroy_jvm_is_false = not _REAL_JPYPE_MODULE.config.destroy_jvm
+            logger.info(f"   pytest_sessionfinish: destroy_jvm est False (selon config): {destroy_jvm_is_false}")
+
+            if jvm_still_started_after_shutdown_attempt and destroy_jvm_is_false:
+                logger.info("   pytest_sessionfinish: JVM est toujours active et destroy_jvm est False. Assurer la présence des modules jpype pour atexit.")
+                current_sys_jpype = sys.modules.get('jpype')
+                if current_sys_jpype is not _REAL_JPYPE_MODULE and _REAL_JPYPE_MODULE is not None:
+                    logger.warning(f"   pytest_sessionfinish: sys.modules['jpype'] (ID: {id(current_sys_jpype)}) n'est pas _REAL_JPYPE_MODULE (ID: {id(_REAL_JPYPE_MODULE)}). Restauration de _REAL_JPYPE_MODULE.")
+                    sys.modules['jpype'] = _REAL_JPYPE_MODULE
+                
+                if _REAL_JPYPE_MODULE and hasattr(_REAL_JPYPE_MODULE, 'config') and _REAL_JPYPE_MODULE.config is not None:
+                    current_sys_jpype_config = sys.modules.get('jpype.config')
+                    if current_sys_jpype_config is not _REAL_JPYPE_MODULE.config:
+                        logger.warning(f"   pytest_sessionfinish: sys.modules['jpype.config'] (ID: {id(current_sys_jpype_config)}) n'est pas _REAL_JPYPE_MODULE.config (ID: {id(_REAL_JPYPE_MODULE.config)}). Restauration.")
+                        sys.modules['jpype.config'] = _REAL_JPYPE_MODULE.config
+                else:
+                    logger.warning("   pytest_sessionfinish: _REAL_JPYPE_MODULE.config non disponible, ne peut pas assurer sys.modules['jpype.config'].")
+            else:
+                logger.info("   pytest_sessionfinish: JVM non démarrée ou destroy_jvm est True. Pas de gestion spéciale de sys.modules pour atexit depuis ici.")
+        except AttributeError as ae:
+             logger.error(f"   pytest_sessionfinish: AttributeError (vérification post-arrêt): {ae}.", exc_info=True)
+        except Exception as e:
+            logger.error(f"   pytest_sessionfinish: Erreur inattendue (vérification post-arrêt): {type(e).__name__}: {e}", exc_info=True)
+    else:
+        logger.info("   pytest_sessionfinish: Le mock JPype a probablement été utilisé. Aucun arrêt de JVM nécessaire depuis ici.")
\ No newline at end of file
diff --git a/tests/unit/webapp/conftest.py b/tests/unit/webapp/conftest.py
new file mode 100644
index 00000000..745791e8
--- /dev/null
+++ b/tests/unit/webapp/conftest.py
@@ -0,0 +1,41 @@
+import pytest
+from pathlib import Path
+
+@pytest.fixture
+def webapp_config():
+    """
+    Fournit une configuration de webapp mockée pour les tests unitaires.
+    Basé sur la structure de _get_default_config dans UnifiedWebOrchestrator.
+    """
+    return {
+        'webapp': {
+            'name': 'Test Web App',
+            'version': '0.1.0',
+            'environment': 'test'
+        },
+        'backend': {
+            'enabled': True,
+            'module': 'fake.backend.module:app',
+            'start_port': 8000,
+            'fallback_ports': [8001, 8002],
+            'timeout_seconds': 5,
+            'health_endpoint': '/api/health'
+        },
+        'frontend': {
+            'enabled': False,
+            'path': 'fake/frontend/path',
+            'port': 3000,
+            'start_command': 'npm start',
+            'timeout_seconds': 10
+        },
+        'playwright': {
+            'enabled': False
+        },
+        'logging': {
+            'level': 'DEBUG',
+            'file': 'logs/test_webapp.log'
+        },
+        'cleanup': {
+            'auto_cleanup': False
+        }
+    }
\ No newline at end of file
diff --git a/tests/unit/webapp/test_backend_manager.py b/tests/unit/webapp/test_backend_manager.py
index 2378f076..b36a6d5f 100644
--- a/tests/unit/webapp/test_backend_manager.py
+++ b/tests/unit/webapp/test_backend_manager.py
@@ -25,150 +25,109 @@ def manager(backend_config, logger_mock):
     return BackendManager(backend_config, logger_mock)
 
 def test_initialization(manager, backend_config):
-    """Tests that the manager initializes correctly."""
+    """Tests the constructor and basic attribute assignments."""
     assert manager.config == backend_config
-    assert manager.logger is not None
-    assert manager.start_port == backend_config['start_port']
-    assert manager.fallback_ports == backend_config['fallback_ports'] # Fix: Check 'fallback_ports'
-    assert manager.process is None
+    assert manager.module == backend_config['module']
 
 @pytest.mark.asyncio
+@patch('project_core.webapp_from_scripts.backend_manager.download_tweety_jars', new_callable=AsyncMock)
 @patch('subprocess.Popen')
-async def test_start_with_failover_success_first_port(mock_popen, manager):
-    """
-    Tests successful start on the primary port.
-    """
+async def test_start_success(mock_popen, mock_download_jars, manager):
+    """Tests a successful start call."""
+    manager._wait_for_backend = AsyncMock(return_value=True)
     manager._is_port_occupied = AsyncMock(return_value=False)
-    manager._start_on_port = AsyncMock(return_value={
-        'success': True, 'url': f'http://localhost:{manager.start_port}',
-        'port': manager.start_port, 'pid': 1234
-    })
     manager._save_backend_info = AsyncMock()
+    mock_download_jars.return_value = True
 
-    result = await manager.start_with_failover()
+    mock_popen.return_value.pid = 1234
+    
+    result = await manager.start()
 
     assert result['success'] is True
     assert result['port'] == manager.start_port
-    manager._is_port_occupied.assert_called_once_with(manager.start_port)
-    manager._start_on_port.assert_called_once_with(manager.start_port)
+    assert result['pid'] == 1234
+    mock_popen.assert_called_once()
+    manager._wait_for_backend.assert_called_once_with(manager.start_port)
     manager._save_backend_info.assert_called_once()
 
 @pytest.mark.asyncio
-@patch('subprocess.Popen')
-async def test_start_with_failover_uses_fallback_port(mock_popen, manager):
-    """
-    Tests that failover to a fallback port works correctly.
-    """
-    manager._is_port_occupied = AsyncMock(side_effect=[True, False]) # First busy, second free
-    manager._start_on_port = AsyncMock(return_value={
-        'success': True, 'url': f'http://localhost:{manager.fallback_ports[0]}',
-        'port': manager.fallback_ports[0], 'pid': 1234
-    })
-    manager._save_backend_info = AsyncMock()
-
-    result = await manager.start_with_failover()
-
-    assert result['success'] is True
-    assert result['port'] == manager.fallback_ports[0]
-    assert manager._is_port_occupied.call_count == 2
-    manager._start_on_port.assert_called_once_with(manager.fallback_ports[0])
-
-@pytest.mark.asyncio
-async def test_start_with_failover_all_ports_fail(manager):
-    """
-    Tests failure when all ports are unavailable.
-    """
-    manager._is_port_occupied = AsyncMock(return_value=True) # All ports are busy
-    manager._start_on_port = AsyncMock() # Should not be called
-
-    result = await manager.start_with_failover()
-
+async def test_start_port_occupied(manager):
+    """Tests that start fails if the port is already occupied."""
+    manager._is_port_occupied = AsyncMock(return_value=True)
+    
+    result = await manager.start()
+    
     assert result['success'] is False
-    assert "Impossible de démarrer" in result['error']
-    assert manager._is_port_occupied.call_count == len(manager.fallback_ports) + 1
-    manager._start_on_port.assert_not_called()
+    assert "est déjà occupé" in result['error']
 
 @pytest.mark.asyncio
 @patch('subprocess.Popen')
-async def test_start_on_port_success(mock_popen, manager):
-    """
-    Tests the "_start_on_port" method for a successful start.
-    """
-    manager._wait_for_backend = AsyncMock(return_value=True)
-    mock_popen.return_value.pid = 1234  # Set the expected PID on the mock
-    port = 8000
-
-    result = await manager._start_on_port(port)
-
-    assert result['success'] is True
-    assert result['port'] == port
-    assert result['pid'] == 1234
-    mock_popen.assert_called_once()
-    manager._wait_for_backend.assert_called_once_with(port)
-
-@pytest.mark.asyncio
-@patch('subprocess.Popen')
-async def test_start_on_port_backend_wait_fails(mock_popen, manager):
-    """
-    Tests "_start_on_port" when waiting for the backend fails.
-    """
+async def test_start_fails_if_wait_fails(mock_popen, manager):
+    """Tests that start fails if _wait_for_backend returns False."""
     manager._wait_for_backend = AsyncMock(return_value=False)
-    port = 8000
+    manager._is_port_occupied = AsyncMock(return_value=False)
+    manager._cleanup_failed_process = AsyncMock()
 
-    result = await manager._start_on_port(port)
+    result = await manager.start()
 
     assert result['success'] is False
-    assert "Backend failed on port 8000" in result['error']
-    mock_popen.return_value.terminate.assert_called_once()
-
+    assert f"a échoué à démarrer sur le port {manager.start_port}" in result['error']
+    manager._cleanup_failed_process.assert_called_once()
 
 @pytest.mark.asyncio
-@patch('subprocess.Popen')
-async def test_wait_for_backend_process_dies(mock_popen, manager):
-    """
-    Tests that _wait_for_backend returns False if the process dies.
-    """
-    mock_popen.return_value.poll.return_value = 1
-    mock_popen.return_value.returncode = 1 
+async def test_wait_for_backend_process_dies(manager):
+    """Tests _wait_for_backend when the process terminates prematurely."""
+    manager.process = MagicMock()
+    manager.process.poll.return_value = 1  # Simulate process ended with exit code 1
+    manager.process.returncode = 1
+    
+    # We need to mock the sleep to ensure the loop doesn't timeout before checking poll()
+    with patch('asyncio.sleep', new_callable=AsyncMock):
+        result = await manager._wait_for_backend(8000)
     
-    manager.process = mock_popen.return_value
-
-    result = await manager._wait_for_backend(8000)
-
     assert result is False
     manager.logger.error.assert_called_with("Processus backend terminé prématurément (code: 1)")
 
 @pytest.mark.asyncio
 @patch('aiohttp.ClientSession.get')
-@patch('subprocess.Popen')
-async def test_wait_for_backend_health_check_ok(mock_popen, mock_get, manager):
-    """
-    Tests _wait_for_backend with a successful health check.
-    """
-    # Simulate a running process
-    mock_popen.return_value.poll.return_value = None
-    manager.process = mock_popen.return_value
-    
+async def test_wait_for_backend_health_check_ok(mock_get, manager):
+    """Tests _wait_for_backend with a successful health check."""
+    manager.process = MagicMock()
+    manager.process.poll.return_value = None  # Simulate running process
+
     mock_response = AsyncMock()
     mock_response.status = 200
     mock_get.return_value.__aenter__.return_value = mock_response
 
+    # Mock asyncio.sleep to avoid actual waiting
     with patch('asyncio.sleep', new_callable=AsyncMock):
         result = await manager._wait_for_backend(8000)
 
     assert result is True
 
 @pytest.mark.asyncio
-@patch('subprocess.Popen')
-async def test_stop_process(mock_popen, manager):
-    """
-    Tests the stop method.
-    """
-    manager.process = mock_popen.return_value
-    manager.pid = 1234
+async def test_wait_for_backend_timeout(manager):
+    """Tests _wait_for_backend when it times out."""
+    manager.process = MagicMock()
+    manager.process.poll.return_value = None
+    
+    # Make health checks fail continuously
+    manager._is_port_occupied = AsyncMock(return_value=False) # Should not be called here but for safety
+    with patch('aiohttp.ClientSession.get', side_effect=asyncio.TimeoutError("Test Timeout")):
+        result = await manager._wait_for_backend(8000)
+    
+    assert result is False
+    manager.logger.error.assert_called_with("Timeout dépassé - Backend inaccessible sur http://127.0.0.1:8000/api/health")
 
+@pytest.mark.asyncio
+async def test_stop_process(manager):
+    """Tests the stop method."""
+    mock_process = MagicMock()
+    mock_process.pid = 1234
+    manager.process = mock_process
+    
     await manager.stop()
-
-    mock_popen.return_value.terminate.assert_called_once()
+    
+    mock_process.terminate.assert_called_once()
     assert manager.process is None
     assert manager.pid is None
\ No newline at end of file
diff --git a/tests/unit/webapp/test_frontend_manager.py b/tests/unit/webapp/test_frontend_manager.py
index 2f1ea701..d53357cb 100644
--- a/tests/unit/webapp/test_frontend_manager.py
+++ b/tests/unit/webapp/test_frontend_manager.py
@@ -78,14 +78,18 @@ async def test_ensure_dependencies_installs_if_needed(mock_popen, manager, tmp_p
     mock_env = {"PATH": os.environ.get("PATH", "")}
     await manager._ensure_dependencies(env=mock_env)
 
+    # On Windows, shell=True is used, so the command is a string.
+    expected_cmd = 'npm install'
     mock_popen.assert_called_with(
-        ['npm', 'install'],
+        expected_cmd,
         stdout=subprocess.PIPE,
         stderr=subprocess.PIPE,
-        cwd=tmp_path
+        cwd=tmp_path,
+        env=mock_env,
+        shell=True
     )
-    # Fix: Use assert_any_call because other logs might happen
-    manager.logger.info.assert_any_call("Installation dépendances npm...")
+    # Check for the more descriptive log message
+    manager.logger.info.assert_any_call("Le répertoire 'node_modules' est manquant. Lancement de 'npm install'...")
 
 
 @pytest.mark.asyncio
@@ -108,7 +112,7 @@ async def test_start_success(mock_popen, manager, tmp_path):
     assert result['pid'] == 5678
     assert result['port'] == manager.port
     mock_popen.assert_called_once()
-    assert manager._wait_for_frontend.call_count == 1
+    assert manager._wait_for_frontend_output.call_count == 1
 
 
 @pytest.mark.asyncio
diff --git a/tests/unit/webapp/test_playwright_runner.py b/tests/unit/webapp/test_playwright_runner.py
index 7c6b07b4..869415cf 100644
--- a/tests/unit/webapp/test_playwright_runner.py
+++ b/tests/unit/webapp/test_playwright_runner.py
@@ -1,20 +1,23 @@
 import pytest
 import logging
 import subprocess
-import json
-from unittest.mock import MagicMock, patch, AsyncMock
+from unittest.mock import MagicMock, patch, AsyncMock, ANY
 from pathlib import Path
 
-# On s'assure que le chemin est correct pour importer le runner
 import sys
 sys.path.insert(0, '.')
 
 from project_core.webapp_from_scripts.playwright_runner import PlaywrightRunner
 
 @pytest.fixture
-def playwright_config(webapp_config):
-    """Provides a base playwright config."""
-    return webapp_config['playwright']
+def playwright_config(webapp_config, tmp_path):
+    """Provides a base playwright config, ensuring temp dirs are used."""
+    config = webapp_config['playwright']
+    # Force 'enabled' for most tests and use temp paths
+    config['enabled'] = True
+    config['screenshots_dir'] = str(tmp_path / 'screenshots')
+    config['traces_dir'] = str(tmp_path / 'traces')
+    return config
 
 @pytest.fixture
 def logger_mock():
@@ -22,23 +25,20 @@ def logger_mock():
     return MagicMock(spec=logging.Logger)
 
 @pytest.fixture
-def runner(playwright_config, logger_mock, tmp_path):
-    """Initializes PlaywrightRunner with a default config."""
-    # Ensure artifact dirs are using a temporary path
-    playwright_config['screenshots_dir'] = str(tmp_path / 'screenshots')
-    playwright_config['traces_dir'] = str(tmp_path / 'traces')
+def runner(playwright_config, logger_mock):
+    """Initializes PlaywrightRunner with a test-safe config."""
     return PlaywrightRunner(playwright_config, logger_mock)
 
 def test_initialization(runner, playwright_config):
     """Tests that the runner initializes correctly."""
     assert runner.enabled is True
-    assert runner.browser == 'chromium'
+    assert runner.browser == 'chromium' # Default from the mock config
     assert Path(runner.screenshots_dir).exists()
     assert Path(runner.traces_dir).exists()
 
 @pytest.mark.asyncio
 async def test_run_tests_when_disabled(logger_mock):
-    """Tests that run_tests returns True if disabled."""
+    """Tests that run_tests returns True if the runner is disabled."""
     config = {'enabled': False}
     runner = PlaywrightRunner(config, logger_mock)
     result = await runner.run_tests()
@@ -63,59 +63,60 @@ async def test_prepare_test_environment(runner):
         assert mock_environ['HEADLESS'] == 'false'
         assert mock_environ['BROWSER'] == 'firefox'
 
-@patch('sys.platform', 'win32')
-@patch('os.getenv')
-def test_build_pytest_command_windows(mock_getenv, runner):
-    """Tests command building on Windows."""
-    mock_getenv.return_value = 'C:/fake/node/home'
-    with patch('pathlib.Path.is_file', return_value=True):
-        cmd = runner._build_playwright_command_string(['tests/'], {'headless': True, 'browser': 'chromium'})
-        assert 'npx.cmd' in cmd[0]
-        assert '--headed' not in cmd
-        assert '--project=chromium' in cmd
-        assert 'tests/' in cmd
-
-@patch('sys.platform', 'linux')
-@patch('os.getenv')
-def test_build_pytest_command_linux(mock_getenv, runner):
-    """Tests command building on Linux."""
-    mock_getenv.return_value = '/fake/node/home'
-    with patch('pathlib.Path.is_file', return_value=True):
-        cmd = runner._build_playwright_command_string(['tests/'], {'headless': False, 'browser': 'webkit'})
-        assert 'npx' in cmd[0]
-        assert '--headed' in cmd
-        assert '--project=webkit' in cmd
-        assert 'tests/' in cmd
-
-
-@patch('subprocess.run')
-async def test_run_tests_execution_flow(mock_subprocess_run, runner):
-    """Tests the main execution flow of run_tests."""
-    # Mock the subprocess call
-    mock_result = MagicMock(spec=subprocess.CompletedProcess)
-    mock_result.returncode = 0
-    mock_result.stdout = "================= 1 passed in 5.00s =================="
-    mock_result.stderr = ""
-    mock_subprocess_run.return_value = mock_result
-
-    # Mock internal methods to isolate run_tests
-    runner._build_playwright_command_string = MagicMock(return_value=['fake_command'])
+@patch('shutil.which')
+def test_build_python_command(mock_which, runner):
+    """Tests the Python command building logic."""
+    mock_which.return_value = '/fake/path/to/pytest'
+    
+    cmd = runner._build_python_command(['tests/my_test.py'], {'browser': 'chromium', 'headless': True}, [])
+    
+    # Check that the command starts with a pytest executable, path can vary
+    assert 'pytest' in cmd[0]
+    assert 'tests/my_test.py' in cmd
+    assert '--browser=chromium' in cmd
+    assert '--headed' not in cmd
+
+def test_build_js_command(runner):
+    """Tests the JS command building logic."""
+    with patch('os.getenv', return_value='C:/fake_node_home'), \
+         patch('pathlib.Path.is_file', return_value=True):
+        
+        cmd = runner._build_js_command(['tests/js/my_test.spec.js'], {'browser': 'firefox'}, None)
+        
+        assert str(cmd[0]).endswith('npx.cmd')
+        assert 'playwright' in cmd
+        assert 'test' in cmd
+        assert 'tests/js/my_test.spec.js' in cmd
+        assert '--project=firefox' in cmd
+
+@pytest.mark.asyncio
+async def test_run_tests_happy_path(runner):
+    """Tests the main execution flow of run_tests on a successful run."""
     runner._prepare_test_environment = AsyncMock()
+    runner._build_command = MagicMock(return_value=['fake_command'])
+    runner._execute_tests = AsyncMock(return_value=MagicMock(returncode=0))
+    runner._analyze_results = AsyncMock(return_value=True)
 
     success = await runner.run_tests()
 
     assert success is True
     runner._prepare_test_environment.assert_called_once()
-    runner._build_playwright_command_string.assert_called_once()
-    mock_subprocess_run.assert_called_once_with(
-        ['fake_command'], capture_output=True, text=True, encoding='utf-8',
-        errors='replace', timeout=300, cwd=Path.cwd()
-    )
-
-    # Check if report was created
-    report_file = Path(runner.traces_dir) / 'test_report.json'
-    assert report_file.exists()
-    with open(report_file, 'r') as f:
-        report_data = json.load(f)
-        assert report_data['success'] is True
-        assert report_data['stats']['passed'] == 1
\ No newline at end of file
+    runner._build_command.assert_called_once()
+    runner._execute_tests.assert_called_once_with(['fake_command'], ANY)
+    runner._analyze_results.assert_called_once()
+
+@pytest.mark.asyncio
+async def test_run_tests_execution_fails(runner):
+    """Tests the main execution flow when the test subprocess fails."""
+    runner._prepare_test_environment = AsyncMock()
+    runner._build_command = MagicMock(return_value=['fake_command'])
+    # Simulate a non-zero return code
+    runner._execute_tests = AsyncMock(return_value=MagicMock(returncode=1))
+    # _analyze_results should still be called to log the failure
+    runner._analyze_results = AsyncMock(return_value=False) 
+
+    success = await runner.run_tests()
+
+    assert success is False
+    runner._execute_tests.assert_called_once()
+    runner._analyze_results.assert_called_once()
\ No newline at end of file

==================== COMMIT: 966d11c176342ef5e533f7edac06d2a664817247 ====================
commit 966d11c176342ef5e533f7edac06d2a664817247
Author: jsboigeEpita <jean-sylvain.boige@epita.fr>
Date:   Mon Jun 16 02:19:20 2025 +0200

    Resolve conflicts from stash@{1} (enrichissement-readme changes)

diff --git a/argumentation_analysis/utils/extract_repair/README.md b/argumentation_analysis/utils/extract_repair/README.md
index 1570a2ba..092c6890 100644
--- a/argumentation_analysis/utils/extract_repair/README.md
+++ b/argumentation_analysis/utils/extract_repair/README.md
@@ -36,7 +36,6 @@ Cet outil est essentiel pour maintenir la qualité des extraits utilisés dans l
   * **[`repair_report.html`](./docs/repair_report.html)** : Rapport HTML généré par le script de réparation.
   * **[`verify_extracts_report.html`](./docs/verify_extracts_report.html)** : Rapport HTML généré par le script de vérification.
   * **[`extract_sources_updated.json`](./docs/extract_sources_updated.json)** : Version mise à jour des sources d'extraits après réparation.
-
 #### Vue d'ensemble du contenu de `docs/`
 
 Le module de réparation des extraits (`extract_repair`) est conçu pour résoudre les problèmes liés aux marqueurs de début et de fin dans les définitions d'extraits. Cette documentation fournit des informations détaillées sur :

==================== COMMIT: 8d416d32578e99bad1ce7c9ae34a96af60f5cd7a ====================
commit 8d416d32578e99bad1ce7c9ae34a96af60f5cd7a
Author: jsboigeEpita <jean-sylvain.boige@epita.fr>
Date:   Mon Jun 16 02:13:25 2025 +0200

    MERGE: Apply final stash, reverting crypto refactoring for consistency

diff --git a/argumentation_analysis/ui/file_operations.py b/argumentation_analysis/ui/file_operations.py
index 61ab6356..3cab4076 100644
--- a/argumentation_analysis/ui/file_operations.py
+++ b/argumentation_analysis/ui/file_operations.py
@@ -1,33 +1,26 @@
-from typing import Optional, Union, List, Dict, Any
 # argumentation_analysis/ui/file_operations.py
 import json
 import gzip
 import logging
-import base64 # NOUVEAU: Pour décoder la clé b64 en clé Fernet
 from pathlib import Path
 from typing import Optional, List, Dict, Any
-from cryptography.fernet import InvalidToken # NÉCESSAIRE pour lever l'exception
-# cryptography.fernet et exceptions sont maintenant gérés dans project_core.utils.crypto_utils
-# et les fonctions encrypt/decrypt sont importées depuis là.
-# from cryptography.fernet import Fernet, InvalidToken # SUPPRIMÉ
-# from cryptography.exceptions import InvalidSignature # SUPPRIMÉ
+from cryptography.fernet import Fernet, InvalidToken
+from cryptography.exceptions import InvalidSignature
 
 # Importer les éléments nécessaires depuis config et utils
 # Attention à ne pas recréer de cycle.
 # On importe 'config as ui_config_module' pour accéder aux constantes.
 from . import config as ui_config_module
 # On importe les fonctions de utils qui ne dépendent pas de config de manière cyclique.
-# encrypt_data et decrypt_data sont maintenant importées depuis project_core
-from .utils import get_full_text_for_source, utils_logger # utils_logger est déjà configuré dans utils.py
-from project_core.utils.crypto_utils import encrypt_data_with_fernet, decrypt_data_with_fernet # NOUVEAU
+from .utils import encrypt_data, decrypt_data, get_full_text_for_source, utils_logger # utils_logger est déjà configuré dans utils.py
 
 # Logger spécifique pour les opérations sur fichiers si besoin, ou utiliser utils_logger
 file_ops_logger = utils_logger # Ou logging.getLogger("App.UI.FileOps")
 
 
 def load_extract_definitions(
-    config_file: Path,
-    b64_derived_key: Optional[str], # MODIFIÉ: La clé reçue est la chaîne b64 dérivée, rendue optionnelle
+    config_file: Path, 
+    key: bytes,
     # app_config est utilisé par get_full_text_for_source, mais load_extract_definitions
     # lui-même ne l'utilise pas directement pour le chargement/déchiffrement.
     # Cependant, si on voulait que load_extract_definitions peuple les full_text au chargement,
@@ -35,74 +28,56 @@ def load_extract_definitions(
     app_config: Optional[Dict[str, Any]] = None 
 ) -> list:
     """Charge, déchiffre et décompresse les définitions depuis le fichier chiffré."""
-    # Utiliser uniquement DEFAULT_EXTRACT_SOURCES comme fallback pour éviter le cycle avec EXTRACT_SOURCES
-    # qui est en cours de définition par l'appelant (config.py)
-    fallback_definitions = ui_config_module.DEFAULT_EXTRACT_SOURCES
+    fallback_definitions = ui_config_module.EXTRACT_SOURCES if ui_config_module.EXTRACT_SOURCES else ui_config_module.DEFAULT_EXTRACT_SOURCES
 
     if not config_file.exists():
-        file_ops_logger.info(f"Fichier config '{config_file}' non trouvé. Utilisation définitions par défaut.")
+        file_ops_logger.info(f"Fichier config chiffré '{config_file}' non trouvé. Utilisation définitions par défaut.")
+        return [item.copy() for item in fallback_definitions]
+    if not key:
+        file_ops_logger.warning("Clé chiffrement absente. Chargement config impossible. Utilisation définitions par défaut.")
         return [item.copy() for item in fallback_definitions]
 
-    if b64_derived_key: # Clé fournie, tenter le déchiffrement
-        file_ops_logger.info(f"Chargement et déchiffrement de '{config_file}' avec clé...")
-        try:
-            with open(config_file, 'rb') as f: encrypted_data = f.read()
-            decrypted_compressed_data = decrypt_data_with_fernet(encrypted_data, b64_derived_key)
-            
-            if not decrypted_compressed_data: # decrypt_data_with_fernet retourne None en cas d'InvalidToken ou autre erreur de déchiffrement
-                file_ops_logger.warning(f"⚠️ Échec du déchiffrement pour '{config_file}' (decrypt_data_with_fernet a retourné None). Utilisation des définitions par défaut.")
-                return [item.copy() for item in fallback_definitions] # MODIFIÉ: Retourner fallback au lieu de lever
-            
-            decompressed_data = gzip.decompress(decrypted_compressed_data)
-            definitions = json.loads(decompressed_data.decode('utf-8'))
-            file_ops_logger.info("✅ Définitions chargées et déchiffrées.")
-
-        except InvalidToken: # Attrapé si decrypt_data_with_fernet lève InvalidToken (par ex. mock avec side_effect)
-            file_ops_logger.error(f"❌ InvalidToken explicitement levée lors du déchiffrement de '{config_file}'. Utilisation définitions par défaut.", exc_info=True)
-            return [item.copy() for item in fallback_definitions] # MODIFIÉ: Retourner fallback
-        except Exception as e:
-            file_ops_logger.error(f"❌ Erreur chargement/déchiffrement '{config_file}': {e}. Utilisation définitions par défaut.", exc_info=True)
+    file_ops_logger.info(f"Chargement et déchiffrement de '{config_file}'...")
+    try:
+        with open(config_file, 'rb') as f: encrypted_data = f.read()
+        decrypted_compressed_data = decrypt_data(encrypted_data, key) # Utilise decrypt_data de utils.py
+        if not decrypted_compressed_data:
+            file_ops_logger.warning("Échec déchiffrement. Utilisation définitions par défaut.")
             return [item.copy() for item in fallback_definitions]
-    
-    else: # Pas de clé, essayer de lire comme JSON simple
-        file_ops_logger.info(f"Aucune clé fournie. Tentative de chargement de '{config_file}' comme JSON simple...")
-        try:
-            with open(config_file, 'r', encoding='utf-8') as f:
-                definitions = json.load(f)
-            file_ops_logger.info(f"✅ Définitions chargées comme JSON simple depuis '{config_file}'.")
-        
-        except json.JSONDecodeError as e_json:
-            file_ops_logger.error(f"❌ Erreur décodage JSON pour '{config_file}': {e_json}. L'exception sera relancée.", exc_info=False)
-            raise
-        except Exception as e:
-            file_ops_logger.error(f"❌ Erreur chargement JSON simple '{config_file}': {e}. Utilisation définitions par défaut.", exc_info=True)
+        decompressed_data = gzip.decompress(decrypted_compressed_data)
+        definitions = json.loads(decompressed_data.decode('utf-8'))
+        file_ops_logger.info("✅ Définitions chargées et déchiffrées.")
+
+        if not isinstance(definitions, list) or not all(
+            isinstance(item, dict) and
+            "source_name" in item and "source_type" in item and "schema" in item and
+            "host_parts" in item and "path" in item and isinstance(item.get("extracts"), list)
+            for item in definitions
+        ):
+            file_ops_logger.warning("⚠️ Format définitions invalide après chargement. Utilisation définitions par défaut.")
             return [item.copy() for item in fallback_definitions]
 
-    # Validation du format (commun aux deux chemins)
-    if not isinstance(definitions, list) or not all(
-        isinstance(item, dict) and
-        "source_name" in item and "source_type" in item and "schema" in item and
-        "host_parts" in item and "path" in item and isinstance(item.get("extracts"), list)
-        for item in definitions
-    ):
-        file_ops_logger.warning(f"⚠️ Format définitions invalide après chargement de '{config_file}'. Utilisation définitions par défaut.")
+        file_ops_logger.info(f"-> {len(definitions)} définitions chargées depuis fichier.")
+        return definitions
+    except (InvalidToken, InvalidSignature) as e: # Intercepter spécifiquement et relancer
+        file_ops_logger.error(f"❌ Erreur déchiffrement/validation token pour '{config_file}': {e}. L'exception sera relancée.", exc_info=True)
+        raise # Relancer l'exception InvalidToken ou InvalidSignature
+    except Exception as e:
+        file_ops_logger.error(f"❌ Erreur chargement/traitement général '{config_file}': {e}. Utilisation définitions par défaut.", exc_info=True)
         return [item.copy() for item in fallback_definitions]
 
-    file_ops_logger.info(f"-> {len(definitions)} définitions chargées depuis '{config_file}'.")
-    return definitions
-
 def save_extract_definitions(
     extract_definitions: List[Dict[str, Any]],
-    config_file: Path,
-    b64_derived_key: Optional[Union[str, bytes]], # MODIFIÉ: Accepte str, bytes ou None
+    config_file: Path, # Renommé de config_path pour correspondre à l'usage dans embed_all_sources.py
+    encryption_key: bytes, # Renommé de passphrase pour refléter qu'une clé Fernet est attendue par encrypt_data
     embed_full_text: bool = False,
     config: Optional[Dict[str, Any]] = None # 'config' est le app_config passé à get_full_text_for_source
 ) -> bool:
     """Sauvegarde, compresse et chiffre les définitions dans le fichier.
     Peut optionnellement embarquer le texte complet des sources.
     """
-    if not b64_derived_key: # Vérifie si la clé est None ou une chaîne/bytes vide
-        file_ops_logger.error("Clé chiffrement (b64_derived_key) absente ou vide. Sauvegarde annulée.")
+    if not encryption_key:
+        file_ops_logger.error("Clé chiffrement absente. Sauvegarde annulée.")
         return False
     if not isinstance(extract_definitions, list):
         file_ops_logger.error("Erreur sauvegarde: définitions non valides (doit être une liste).")
@@ -151,14 +126,11 @@ def save_extract_definitions(
                 file_ops_logger.debug(f"Champ 'full_text' retiré pour '{source_info.get('source_name', 'Source inconnue')}'.")
 
     try:
-        # actual_fernet_key = base64.urlsafe_b64decode(b64_derived_key.encode('utf-8')) # SUPPRIMÉ: La fonction attend la str b64
         json_data = json.dumps(definitions_to_process, indent=2, ensure_ascii=False).encode('utf-8') # Utiliser la copie traitée
         compressed_data = gzip.compress(json_data)
-        # MODIFIÉ: Utilisation de encrypt_data_with_fernet avec la clé b64_derived_key (str)
-        encrypted_data_to_save = encrypt_data_with_fernet(compressed_data, b64_derived_key)
+        encrypted_data_to_save = encrypt_data(compressed_data, encryption_key) # Utilise encrypt_data de utils.py
         if not encrypted_data_to_save:
-            # encrypt_data_with_fernet logge déjà l'erreur
-            raise ValueError("Échec du chiffrement des données (encrypt_data_with_fernet a retourné None).")
+            raise ValueError("Échec du chiffrement des données.")
 
         config_file.parent.mkdir(parents=True, exist_ok=True)
         with open(config_file, 'wb') as f:
diff --git a/argumentation_analysis/ui/utils.py b/argumentation_analysis/ui/utils.py
index 3156753a..6d48aed4 100644
--- a/argumentation_analysis/ui/utils.py
+++ b/argumentation_analysis/ui/utils.py
@@ -1,63 +1,460 @@
-"""
-Utilitaires pour l'interface utilisateur (UI) de l'analyse d'argumentation.
-
-Ce module sert de point d'entrée principal pour les utilitaires UI,
-important des fonctionnalités spécifiques depuis des sous-modules dédiés.
-"""
-import logging
-from pathlib import Path
-from typing import Optional, List, Dict, Any, Union # Gardé pour reconstruct_url et exposition potentielle
-
-# Import config depuis le même package ui (peut être nécessaire pour des constantes globales)
-from . import config as ui_config
-
-# Logger principal pour les utilitaires UI
-# Les sous-modules peuvent définir leurs propres loggers ou utiliser celui-ci s'il est importé.
-utils_logger = logging.getLogger("App.UI.Utils")
-if not utils_logger.handlers and not utils_logger.propagate:
-     handler = logging.StreamHandler()
-     formatter = logging.Formatter('%(asctime)s [%(levelname)s] [%(name)s] %(message)s', datefmt='%H:%M:%S')
-     handler.setFormatter(formatter)
-     utils_logger.addHandler(handler)
-     utils_logger.setLevel(logging.INFO)
-
-# --- Fonctions Utilitaires de Base ---
-
-def reconstruct_url(schema: str, host_parts: List[str], path: Optional[str]) -> Optional[str]:
-    """Reconstruit une URL complète à partir de ses composants.
-
-    :param schema: Le schéma de l'URL (par exemple, "http", "https").
-    :type schema: str
-    :param host_parts: Une liste des parties composant le nom d'hôte.
-    :type host_parts: List[str]
-    :param path: Le chemin de la ressource sur le serveur. Peut être None ou vide.
-    :type path: Optional[str]
-    :return: L'URL reconstruite, ou None si `schema` ou `host_parts` sont invalides.
-    :rtype: Optional[str]
-    """
-    if not schema or not host_parts: return None
-    host = ".".join(part for part in host_parts if part)
-    current_path = path if path is not None else ""
-    current_path = current_path if current_path.startswith('/') or not current_path else '/' + current_path
-    if not current_path:
-        current_path = "/"
-    return f"{schema}://{host}{current_path}"
-
-# --- Importation des fonctionnalités depuis les sous-modules ---
-
-# Exposer toutes les fonctions des sous-modules pour la compatibilité ascendante
-# ou importer sélectivement ce qui doit être public via ce module.
-# Pour l'instant, j'importe tout avec '*' pour simplifier.
-
-from .cache_utils import *
-from .fetch_utils import *
-from .verification_utils import *
-
-# Les fonctions de chiffrement ont été déplacées vers project_core.utils.crypto_utils
-# et ne sont plus directement exposées ou utilisées par ce module ui.utils central.
-# Les appels se font via les modules qui en ont besoin (par ex. file_operations.py)
-
-# Les fonctions load_extract_definitions et save_extract_definitions ont été déplacées
-# vers argumentation_analysis/ui/file_operations.py.
-
-utils_logger.info("Module principal des utilitaires UI (utils.py) initialisé et sous-modules importés.")
+# ui/utils.py
+import requests
+import json
+import gzip
+import hashlib
+import logging
+from pathlib import Path
+from typing import Optional, List, Dict, Any, Union
+from cryptography.fernet import Fernet, InvalidToken
+from cryptography.exceptions import InvalidSignature
+
+# Le reste des imports et du code...
+
+# Import config depuis le même package ui
+from . import config as ui_config
+
+utils_logger = logging.getLogger("App.UI.Utils")
+if not utils_logger.handlers and not utils_logger.propagate:
+     handler = logging.StreamHandler(); formatter = logging.Formatter('%(asctime)s [%(levelname)s] [%(name)s] %(message)s', datefmt='%H:%M:%S'); handler.setFormatter(formatter); utils_logger.addHandler(handler); utils_logger.setLevel(logging.INFO)
+
+# --- Fonctions Utilitaires (Cache, Crypto, Fetch, Verify) ---
+
+def reconstruct_url(schema: str, host_parts: list, path: str) -> Optional[str]:
+    """Reconstruit une URL à partir de schema, host_parts, et path."""
+    if not schema or not host_parts: return None # Path peut être vide et géré ci-dessous
+    host = ".".join(part for part in host_parts if part)
+    # Si path est None, on le traite comme une chaîne vide pour la logique suivante
+    current_path = path if path is not None else ""
+    current_path = current_path if current_path.startswith('/') or not current_path else '/' + current_path
+    # S'assurer qu'un path vide après traitement devienne au moins "/"
+    if not current_path:
+        current_path = "/"
+    return f"{schema}://{host}{current_path}"
+
+def get_cache_filepath(url: str) -> Path:
+    """Génère le chemin du fichier cache pour une URL."""
+    url_hash = hashlib.sha256(url.encode()).hexdigest()
+    # Utilise CACHE_DIR importé depuis config
+    return ui_config.CACHE_DIR / f"{url_hash}.txt"
+
+def load_from_cache(url: str) -> Optional[str]:
+    """Charge le contenu textuel depuis le cache si disponible."""
+    filepath = get_cache_filepath(url)
+    if filepath.exists():
+        try:
+            utils_logger.info(f"   -> Lecture depuis cache : {filepath.name}")
+            return filepath.read_text(encoding='utf-8')
+        except Exception as e:
+            utils_logger.warning(f"   -> Erreur lecture cache {filepath.name}: {e}")
+            return None
+    utils_logger.debug(f"Cache miss pour URL: {url}")
+    return None
+
+def save_to_cache(url: str, text: str):
+    """Sauvegarde le contenu textuel dans le cache."""
+    if not text:
+        utils_logger.info("   -> Texte vide, non sauvegardé.")
+        return
+    filepath = get_cache_filepath(url)
+    try:
+        # S'assurer que le dossier cache existe
+        filepath.parent.mkdir(parents=True, exist_ok=True)
+        filepath.write_text(text, encoding='utf-8')
+        utils_logger.info(f"   -> Texte sauvegardé : {filepath.name}")
+    except Exception as e:
+        utils_logger.error(f"   -> Erreur sauvegarde cache {filepath.name}: {e}")
+
+def encrypt_data(data: bytes, key: bytes) -> Optional[bytes]:
+    """Chiffre des données binaires avec une clé Fernet."""
+    if not key:
+        utils_logger.error("Erreur chiffrement: Clé chiffrement manquante.")
+        return None
+    try:
+        f = Fernet(key)
+        return f.encrypt(data)
+    except Exception as e:
+        utils_logger.error(f"Erreur chiffrement: {e}")
+        return None
+
+def decrypt_data(encrypted_data: bytes, key: bytes) -> Optional[bytes]:
+    """Déchiffre des données binaires avec une clé Fernet."""
+    if not key:
+        utils_logger.error("Erreur déchiffrement: Clé chiffrement manquante.")
+        return None
+    try:
+        f = Fernet(key)
+        return f.decrypt(encrypted_data)
+    except (InvalidToken, InvalidSignature) as e: # Intercepter spécifiquement et relancer
+        utils_logger.error(f"Erreur déchiffrement (InvalidToken/Signature): {e}")
+        raise # Relancer l'exception capturée (InvalidToken ou InvalidSignature)
+    except Exception as e: # Intercepter les autres exceptions
+        utils_logger.error(f"Erreur déchiffrement (Autre): {e}")
+        return None
+
+# Les fonctions load_extract_definitions et save_extract_definitions ont été déplacées
+# vers argumentation_analysis/ui/file_operations.py pour éviter les imports circulaires.
+def fetch_direct_text(source_url: str, timeout: int = 60) -> str:
+    """Récupère contenu texte brut d'URL, utilise cache fichier."""
+    # Utilise les fonctions de cache de ce module
+    cached_text = load_from_cache(source_url)
+    if cached_text is not None: return cached_text
+    utils_logger.info(f"-> Téléchargement direct depuis : {source_url}...")
+    headers = {'User-Agent': 'ArgumentAnalysisApp/1.0'}
+    try:
+        response = requests.get(source_url, headers=headers, timeout=timeout)
+        response.raise_for_status()
+        texte_brut = response.content.decode('utf-8', errors='ignore')
+        utils_logger.info(f"   -> Contenu direct récupéré (longueur {len(texte_brut)}).")
+        save_to_cache(source_url, texte_brut)
+        return texte_brut
+    except requests.exceptions.RequestException as e:
+        utils_logger.error(f"Erreur téléchargement direct ({source_url}): {e}")
+        raise ConnectionError(f"Erreur téléchargement direct ({source_url}): {e}") from e
+
+def fetch_with_jina(
+    source_url: str,
+    timeout: int = 90,
+    jina_reader_prefix_override: Optional[str] = None
+) -> str:
+    """Récupère et extrait via Jina, utilise cache fichier."""
+    # Utilise les fonctions de cache de ce module
+    cached_text = load_from_cache(source_url)
+    if cached_text is not None: return cached_text
+
+    _jina_reader_prefix = jina_reader_prefix_override if jina_reader_prefix_override is not None else ui_config.JINA_READER_PREFIX
+    jina_url = f"{_jina_reader_prefix}{source_url}"
+
+    utils_logger.info(f"-> Récupération via Jina : {jina_url}...")
+    headers = {'Accept': 'text/markdown', 'User-Agent': 'ArgumentAnalysisApp/1.0'}
+    try:
+        response = requests.get(jina_url, headers=headers, timeout=timeout)
+        response.raise_for_status()
+    except requests.exceptions.RequestException as e:
+        utils_logger.error(f"Erreur Jina ({jina_url}): {e}")
+        raise ConnectionError(f"Erreur Jina ({jina_url}): {e}") from e
+    content = response.text
+    md_start_marker = "Markdown Content:"
+    md_start_index = content.find(md_start_marker)
+    texte_brut = content[md_start_index + len(md_start_marker):].strip() if md_start_index != -1 else content
+    utils_logger.info(f"   -> Contenu Jina récupéré (longueur {len(texte_brut)}).")
+    save_to_cache(source_url, texte_brut)
+    return texte_brut
+
+def fetch_with_tika(
+    source_url: Optional[str] = None,
+    file_content: Optional[bytes] = None,
+    file_name: str = "fichier",
+    raw_file_cache_path: Optional[Union[Path, str]] = None,
+    timeout_dl: int = 60,
+    timeout_tika: int = 600,
+    tika_server_url_override: Optional[str] = None,
+    plaintext_extensions_override: Optional[List[str]] = None,
+    temp_download_dir_override: Optional[Path] = None
+    ) -> str:
+    """Traite une source via Tika avec gestion cache brut et type texte."""
+    _tika_server_url = tika_server_url_override if tika_server_url_override is not None else ui_config.TIKA_SERVER_URL
+    _plaintext_extensions = plaintext_extensions_override if plaintext_extensions_override is not None else ui_config.PLAINTEXT_EXTENSIONS
+    _temp_download_dir = temp_download_dir_override if temp_download_dir_override is not None else ui_config.TEMP_DOWNLOAD_DIR
+
+    cache_key = source_url if source_url else f"file://{file_name}"
+    cached_text = load_from_cache(cache_key)
+    if cached_text is not None: return cached_text
+
+    content_to_send = None
+    # temp_download_dir = ui_config.TEMP_DOWNLOAD_DIR # Utiliser le chemin depuis config
+    # Remplacé par _temp_download_dir
+
+    if source_url:
+        original_filename = Path(source_url).name
+        if any(source_url.lower().endswith(ext) for ext in _plaintext_extensions):
+            utils_logger.info(f"   -> URL détectée comme texte simple ({source_url}). Fetch direct.")
+            return fetch_direct_text(source_url)
+
+        url_hash = hashlib.sha256(source_url.encode()).hexdigest()
+        file_extension = Path(original_filename).suffix if Path(original_filename).suffix else ".download"
+        effective_raw_cache_path = Path(raw_file_cache_path) if raw_file_cache_path else _temp_download_dir / f"{url_hash}{file_extension}"
+
+        if effective_raw_cache_path.exists() and effective_raw_cache_path.stat().st_size > 0:
+             try:
+                utils_logger.info(f"   -> Lecture fichier brut depuis cache local : {effective_raw_cache_path.name}")
+                content_to_send = effective_raw_cache_path.read_bytes()
+             except Exception as e_read_raw:
+                utils_logger.warning(f"   -> Erreur lecture cache brut {effective_raw_cache_path.name}: {e_read_raw}. Re-téléchargement...")
+                content_to_send = None
+
+        if content_to_send is None:
+             utils_logger.info(f"-> Téléchargement (pour Tika) depuis : {source_url}...")
+             try:
+                 response_dl = requests.get(source_url, stream=True, timeout=timeout_dl)
+                 response_dl.raise_for_status()
+                 content_to_send = response_dl.content
+                 utils_logger.info(f"   -> Doc téléchargé ({len(content_to_send)} bytes).")
+                 try:
+                     effective_raw_cache_path.parent.mkdir(parents=True, exist_ok=True)
+                     effective_raw_cache_path.write_bytes(content_to_send)
+                     utils_logger.info(f"   -> Doc brut sauvegardé: {effective_raw_cache_path.resolve()}")
+                 except Exception as e_save:
+                     utils_logger.error(f"   -> Erreur sauvegarde brut: {e_save}")
+             except requests.exceptions.RequestException as e:
+                 utils_logger.error(f"Erreur téléchargement {source_url}: {e}")
+                 raise ConnectionError(f"Erreur téléchargement {source_url}: {e}") from e
+
+    elif file_content:
+        utils_logger.info(f"-> Utilisation contenu fichier '{file_name}' ({len(file_content)} bytes)...")
+        content_to_send = file_content
+        if any(file_name.lower().endswith(ext) for ext in _plaintext_extensions):
+            utils_logger.info("   -> Fichier uploadé détecté comme texte simple. Lecture directe.")
+            try:
+                texte_brut = file_content.decode('utf-8', errors='ignore')
+                save_to_cache(cache_key, texte_brut)
+                return texte_brut
+            except Exception as e_decode:
+                utils_logger.warning(f"   -> Erreur décodage fichier texte '{file_name}': {e_decode}. Tentative avec Tika...")
+    else:
+        raise ValueError("fetch_with_tika: Il faut soit source_url soit file_content.")
+
+    if not content_to_send:
+        utils_logger.warning("   -> Contenu brut vide ou non récupéré. Impossible d'envoyer à Tika.")
+        save_to_cache(cache_key, "") # Sauvegarder une chaîne vide pour éviter re-fetch inutile
+        return ""
+
+    utils_logger.info(f"-> Envoi contenu à Tika ({_tika_server_url})... (Timeout={timeout_tika}s)")
+    headers = { 'Accept': 'text/plain', 'Content-Type': 'application/octet-stream', 'X-Tika-OCRLanguage': 'fra+eng' }
+    try:
+        response_tika = requests.put(_tika_server_url, data=content_to_send, headers=headers, timeout=timeout_tika)
+        response_tika.raise_for_status()
+        texte_brut = response_tika.text
+        if not texte_brut: utils_logger.warning(f"   -> Warning: Tika status {response_tika.status_code} sans texte.")
+        else: utils_logger.info(f"   -> Texte Tika extrait (longueur {len(texte_brut)}).")
+        save_to_cache(cache_key, texte_brut) # Appel fonction de ce module
+        return texte_brut
+    except requests.exceptions.Timeout:
+        utils_logger.error(f"   -> ❌ Timeout Tika ({timeout_tika}s).")
+        raise ConnectionError(f"Timeout Tika")
+    except requests.exceptions.RequestException as e:
+        utils_logger.error(f"Erreur Tika: {e}")
+        raise ConnectionError(f"Erreur Tika: {e}") from e
+
+
+        if not texte_brut: utils_logger.warning(f"   -> Warning: Tika status {response_tika.status_code} sans texte.")
+        else: utils_logger.info(f"   -> Texte Tika extrait (longueur {len(texte_brut)}).")
+        save_to_cache(cache_key, texte_brut)
+        return texte_brut
+    except requests.exceptions.Timeout:
+        utils_logger.error(f"   -> ❌ Timeout Tika ({timeout_tika}s).")
+        raise ConnectionError(f"Timeout Tika ({timeout_tika}s)") # Renvoyer une erreur plus spécifique
+    except requests.exceptions.RequestException as e:
+        utils_logger.error(f"Erreur Tika: {e}")
+        raise ConnectionError(f"Erreur Tika: {e}") from e
+
+
+def get_full_text_for_source(source_info: Dict[str, Any], app_config: Optional[Dict[str, Any]] = None) -> Optional[str]:
+    """
+    Récupère le texte complet pour une source donnée, en utilisant le cache et les configurations appropriées.
+    Centralise la logique de récupération de texte (Jina, Tika, téléchargement direct).
+
+    Args:
+        source_info: Dictionnaire contenant les informations de la source.
+                     Doit contenir "schema", "host_parts", "path", et "source_type".
+        app_config: Dictionnaire optionnel de configuration de l'application.
+                    Peut contenir des surcharges pour JINA_READER_PREFIX, TIKA_SERVER_URL,
+                    PLAINTEXT_EXTENSIONS, TEMP_DOWNLOAD_DIR.
+
+    Returns:
+        Le texte complet de la source, ou None en cas d'erreur.
+    """
+    source_name_for_log = source_info.get('source_name', 'Source inconnue')
+    utils_logger.debug(f"get_full_text_for_source appelée pour: {source_name_for_log}")
+
+    reconstructed_url = reconstruct_url(
+        source_info.get("schema"), source_info.get("host_parts", []), source_info.get("path")
+    )
+    if not reconstructed_url:
+        utils_logger.error(f"URL invalide pour source: {source_name_for_log}")
+        return None
+
+    # Essayer de charger depuis le cache fichier d'abord
+    cached_text = load_from_cache(reconstructed_url)
+    if cached_text is not None:
+        utils_logger.info(f"Texte chargé depuis cache fichier pour URL '{reconstructed_url}' ({source_name_for_log})")
+        return cached_text
+
+    source_type = source_info.get("source_type")
+    texte_brut_source: Optional[str] = None
+
+    # Récupérer les configurations, en privilégiant app_config si fourni
+    jina_prefix_val = ui_config.JINA_READER_PREFIX
+    tika_server_url_val = ui_config.TIKA_SERVER_URL
+    plaintext_extensions_val = ui_config.PLAINTEXT_EXTENSIONS
+    temp_download_dir_val = ui_config.TEMP_DOWNLOAD_DIR
+
+    if app_config:
+        jina_prefix_val = app_config.get('JINA_READER_PREFIX', jina_prefix_val)
+        tika_server_url_val = app_config.get('TIKA_SERVER_URL', tika_server_url_val)
+        plaintext_extensions_val = app_config.get('PLAINTEXT_EXTENSIONS', plaintext_extensions_val)
+        # Pour TEMP_DOWNLOAD_DIR, s'assurer que c'est un objet Path si surchargé
+        temp_download_dir_str_or_path = app_config.get('TEMP_DOWNLOAD_DIR')
+        if temp_download_dir_str_or_path is not None:
+            temp_download_dir_val = Path(temp_download_dir_str_or_path)
+
+
+    utils_logger.info(f"Cache texte absent pour '{reconstructed_url}' ({source_name_for_log}). Récupération (type: {source_type})...")
+    try:
+        if source_type == "jina":
+            texte_brut_source = fetch_with_jina(
+                reconstructed_url,
+                jina_reader_prefix_override=jina_prefix_val
+            )
+        elif source_type == "direct_download":
+            # fetch_direct_text n'a pas de config spécifique à surcharger via app_config pour l'instant
+            texte_brut_source = fetch_direct_text(reconstructed_url)
+        elif source_type == "tika":
+            # fetch_with_tika gère déjà la logique plaintext vs binaire en interne
+            # On passe les configs potentiellement surchargées
+            texte_brut_source = fetch_with_tika(
+                source_url=reconstructed_url,
+                tika_server_url_override=tika_server_url_val,
+                plaintext_extensions_override=plaintext_extensions_val,
+                temp_download_dir_override=temp_download_dir_val
+                # raw_file_cache_path n'est pas géré par app_config ici, fetch_with_tika le déduit si besoin
+            )
+        else:
+            utils_logger.warning(f"Type de source inconnu '{source_type}' pour '{reconstructed_url}' ({source_name_for_log}). Impossible de récupérer le texte.")
+            return None
+
+        if texte_brut_source is not None:
+            utils_logger.info(f"Texte récupéré pour '{reconstructed_url}' ({source_name_for_log}), sauvegarde dans le cache...")
+            save_to_cache(reconstructed_url, texte_brut_source)
+        else:
+            utils_logger.warning(f"Aucun texte brut retourné par la fonction fetch pour '{reconstructed_url}' ({source_name_for_log}).")
+
+
+        return texte_brut_source
+
+    except ConnectionError as e: # Erreurs spécifiques levées par les fetch_*
+        utils_logger.error(f"Erreur de connexion lors de la récupération de '{reconstructed_url}' ({source_name_for_log}, type: {source_type}): {e}")
+        return None
+    except Exception as e:
+        utils_logger.error(f"Erreur inattendue lors de la récupération de '{reconstructed_url}' ({source_name_for_log}, type: {source_type}): {e}", exc_info=True)
+        return None
+
+def verify_extract_definitions(definitions_list: list) -> str:
+    """Vérifie la présence des marqueurs start/end pour chaque extrait défini."""
+    # Utilise les fonctions reconstruct_url, load_from_cache, fetch_* de ce module
+    # Et les constantes de ui.config
+    utils_logger.info("\n🔬 Lancement de la vérification des marqueurs d'extraits...")
+    results = []
+    total_checks = 0
+    total_errors = 0
+
+    # Utilise la constante depuis config
+    if not definitions_list or definitions_list == ui_config.DEFAULT_EXTRACT_SOURCES:
+        return "Aucune définition valide à vérifier."
+
+    for source_idx, source_info in enumerate(definitions_list):
+        source_name = source_info.get("source_name", f"Source Inconnue #{source_idx+1}")
+        utils_logger.info(f"\n--- Vérification Source: '{source_name}' ---")
+        source_errors = 0
+        source_checks = 0
+        texte_brut_source = None
+        reconstructed_url = None
+
+        try:
+            reconstructed_url = reconstruct_url(
+                source_info.get("schema"), source_info.get("host_parts", []), source_info.get("path")
+            )
+            if not reconstructed_url:
+                utils_logger.error("   -> ❌ Erreur: URL Invalide.")
+                results.append(f"<li>{source_name}: URL invalide</li>")
+                num_extracts = len(source_info.get("extracts", []))
+                total_errors += num_extracts
+                total_checks += num_extracts
+                continue
+
+            source_type = source_info.get("source_type")
+            cache_key = reconstructed_url
+            texte_brut_source = load_from_cache(cache_key)
+
+            if texte_brut_source is None:
+                utils_logger.info(f"   -> Cache texte absent. Récupération (type: {source_type})...")
+                try:
+                    if source_type == "jina": texte_brut_source = fetch_with_jina(reconstructed_url)
+                    elif source_type == "direct_download": texte_brut_source = fetch_direct_text(reconstructed_url)
+                    elif source_type == "tika":
+                        is_plaintext = any(source_info.get("path", "").lower().endswith(ext) for ext in ui_config.PLAINTEXT_EXTENSIONS)
+                        if is_plaintext: texte_brut_source = fetch_direct_text(reconstructed_url)
+                        else:
+                            utils_logger.warning("   -> ⚠️ Vérification marqueurs sautée pour source Tika binaire.")
+                            texte_brut_source = None
+                    else:
+                        utils_logger.warning(f"   -> ⚠️ Type source inconnu '{source_type}'. Vérification impossible.")
+                        texte_brut_source = None
+                except Exception as e_fetch_verify:
+                     utils_logger.error(f"   -> ❌ Erreur fetch pendant vérification pour '{source_name}': {e_fetch_verify}")
+                     texte_brut_source = None
+
+            # ... [Reste de la logique de vérification des marqueurs - IDENTIQUE A AVANT] ...
+            if texte_brut_source is not None:
+                utils_logger.info(f"   -> Texte complet récupéré (longueur: {len(texte_brut_source)}). Vérification des extraits...")
+                extracts = source_info.get("extracts", [])
+                if not extracts: utils_logger.info("      -> Aucun extrait défini.")
+
+                for extract_idx, extract_info in enumerate(extracts):
+                    extract_name = extract_info.get("extract_name", f"Extrait #{extract_idx+1}")
+                    start_marker = extract_info.get("start_marker")
+                    end_marker = extract_info.get("end_marker")
+                    total_checks += 1
+                    source_checks += 1
+                    marker_errors = []
+
+                    if not start_marker or not end_marker:
+                        marker_errors.append("Marqueur(s) Manquant(s)")
+                    else:
+                        start_found = start_marker in texte_brut_source
+                        end_found_after_start = False
+                        if start_found:
+                            try:
+                                start_pos = texte_brut_source.index(start_marker)
+                                end_found_after_start = end_marker in texte_brut_source[start_pos + len(start_marker):]
+                            except ValueError: start_found = False
+                        if not start_found: marker_errors.append("Début NON TROUVÉ")
+                        if not end_found_after_start: marker_errors.append("Fin NON TROUVÉE (après début)")
+
+                    if marker_errors:
+                        utils_logger.warning(f"      -> ❌ Problème Extrait '{extract_name}': {', '.join(marker_errors)}")
+                        results.append(f"<li>{source_name} -> {extract_name}: <strong style='color:red;'>{', '.join(marker_errors)}</strong></li>")
+                        source_errors += 1
+                        total_errors += 1
+                    else:
+                        utils_logger.info(f"      -> ✅ OK: Extrait '{extract_name}'")
+
+            else:
+                num_extracts = len(source_info.get("extracts",[]))
+                if source_type != 'tika' or is_plaintext:
+                    results.append(f"<li>{source_name}: Vérification impossible (texte source non obtenu)</li>")
+                    total_errors += num_extracts
+                else:
+                     results.append(f"<li>{source_name}: Vérification marqueurs sautée (source Tika binaire)</li>")
+                total_checks += num_extracts
+
+        except Exception as e_verify_global:
+            utils_logger.error(f"   -> ❌ Erreur inattendue vérification source '{source_name}': {e_verify_global}", exc_info=True)
+            num_extracts = len(source_info.get("extracts",[]))
+            results.append(f"<li>{source_name}: Erreur Vérification Générale ({type(e_verify_global).__name__})</li>")
+            total_errors += num_extracts
+            total_checks += num_extracts
+
+    # ... [Reste de la logique de formatage du résumé - IDENTIQUE A AVANT] ...
+    summary = f"--- Résultat Vérification ---<br/>{total_checks} extraits vérifiés. <strong style='color: {'red' if total_errors > 0 else 'green'};'>{total_errors} erreur(s) trouvée(s).</strong>"
+    if results:
+        summary += "<br/>Détails :<ul>" + "".join(results) + "</ul>"
+    else:
+         if total_checks > 0: summary += "<br/>Tous les marqueurs semblent corrects."
+         else: summary += "<br/>Aucun extrait n'a pu être vérifié."
+
+    utils_logger.info("\n" + f"{summary.replace('<br/>', chr(10)).replace('<li>', '- ').replace('</li>', '').replace('<ul>', '').replace('</ul>', '').replace('<strong>', '').replace('</strong>', '')}")
+    return summary
+
+
+utils_logger.info("Fonctions utilitaires UI définies.")
\ No newline at end of file

==================== COMMIT: 713535e14c3a04c8405db46a544961f192b7727e ====================
commit 713535e14c3a04c8405db46a544961f192b7727e
Author: jsboigeEpita <jean-sylvain.boige@epita.fr>
Date:   Mon Jun 16 02:11:49 2025 +0200

    Resolution du stash 'WIP on doc/enrichissement-readme - chiffrement':

diff --git a/argumentation_analysis/agents/tools/encryption/cleanup_after_encryption.py b/argumentation_analysis/agents/tools/encryption/cleanup_after_encryption.py
deleted file mode 100644
index f72a0c63..00000000
--- a/argumentation_analysis/agents/tools/encryption/cleanup_after_encryption.py
+++ /dev/null
@@ -1,128 +0,0 @@
-#!/usr/bin/env python
-# -*- coding: utf-8 -*-
-
-"""
-Script pour nettoyer les fichiers non nécessaires après avoir créé le fichier encrypté complet.
-"""
-
-import os
-import sys
-import shutil
-from pathlib import Path
-
-# Ajouter le répertoire parent au chemin de recherche des modules
-current_dir = Path(__file__).parent
-parent_dir = current_dir.parent
-if str(parent_dir) not in sys.path:
-    sys.path.append(str(parent_dir))
-
-# Importer les modules nécessaires
-from argumentation_analysis.ui.config import CONFIG_FILE_ENC
-
-from argumentation_analysis.paths import DATA_DIR
-
-
-# Définir les constantes
-TEXT_CACHE_DIR = parent_dir / "text_cache"
-TEMP_DOWNLOADS_DIR = parent_dir / "temp_downloads"
-EXTRACT_SOURCES_JSON = parent_dir / DATA_DIR / "extract_sources.json"
-
-# Afficher les chemins pour le débogage
-print(f"Chemin du fichier encrypté: {CONFIG_FILE_ENC}")
-print(f"Chemin du répertoire de cache: {TEXT_CACHE_DIR}")
-print(f"Chemin du répertoire de téléchargements temporaires: {TEMP_DOWNLOADS_DIR}")
-print(f"Chemin du fichier extract_sources.json: {EXTRACT_SOURCES_JSON}")
-
-def cleanup_files():
-    """Nettoie les fichiers non nécessaires après avoir créé le fichier encrypté complet."""
-    # Vérifier si le fichier encrypté existe
-    if not CONFIG_FILE_ENC.exists():
-        print(f"❌ Erreur: Le fichier encrypté '{CONFIG_FILE_ENC}' n'existe pas.")
-        print(f"   Veuillez d'abord créer le fichier encrypté complet.")
-        return False
-    
-    files_removed = 0
-    dirs_removed = 0
-    
-    # Supprimer les fichiers de cache
-    if TEXT_CACHE_DIR.exists():
-        print(f"Suppression du répertoire de cache '{TEXT_CACHE_DIR}'...")
-        try:
-            # Compter les fichiers
-            cache_files = list(TEXT_CACHE_DIR.glob("*.txt"))
-            files_removed += len(cache_files)
-            
-            # Supprimer le répertoire
-            shutil.rmtree(TEXT_CACHE_DIR)
-            dirs_removed += 1
-            print(f"[OK] Répertoire de cache '{TEXT_CACHE_DIR}' supprimé.")
-            print(f"   - {len(cache_files)} fichiers de cache supprimés.")
-        except Exception as e:
-            print(f"⚠️ Avertissement: Erreur lors de la suppression du répertoire de cache '{TEXT_CACHE_DIR}': {e}")
-    else:
-        print(f"ℹ️ Le répertoire de cache '{TEXT_CACHE_DIR}' n'existe pas.")
-    
-    # Supprimer les téléchargements temporaires
-    if TEMP_DOWNLOADS_DIR.exists():
-        print(f"Suppression du répertoire de téléchargements temporaires '{TEMP_DOWNLOADS_DIR}'...")
-        try:
-            # Compter les fichiers
-            temp_files = list(TEMP_DOWNLOADS_DIR.glob("*"))
-            files_removed += len(temp_files)
-            
-            # Supprimer le répertoire
-            shutil.rmtree(TEMP_DOWNLOADS_DIR)
-            dirs_removed += 1
-            print(f"[OK] Répertoire de téléchargements temporaires '{TEMP_DOWNLOADS_DIR}' supprimé.")
-            print(f"   - {len(temp_files)} fichiers temporaires supprimés.")
-        except Exception as e:
-            print(f"⚠️ Avertissement: Erreur lors de la suppression du répertoire de téléchargements temporaires '{TEMP_DOWNLOADS_DIR}': {e}")
-    else:
-        print(f"ℹ️ Le répertoire de téléchargements temporaires '{TEMP_DOWNLOADS_DIR}' n'existe pas.")
-    
-    # Supprimer le fichier extract_sources.json s'il existe
-    if EXTRACT_SOURCES_JSON.exists():
-        print(f"Suppression du fichier '{EXTRACT_SOURCES_JSON}'...")
-        try:
-            EXTRACT_SOURCES_JSON.unlink()
-            files_removed += 1
-            print(f"[OK] Fichier '{EXTRACT_SOURCES_JSON}' supprimé.")
-        except Exception as e:
-            print(f"⚠️ Avertissement: Erreur lors de la suppression du fichier '{EXTRACT_SOURCES_JSON}': {e}")
-    else:
-        print(f"ℹ️ Le fichier '{EXTRACT_SOURCES_JSON}' n'existe pas.")
-    
-    print(f"\nNettoyage terminé:")
-    print(f"   - {files_removed} fichiers supprimés.")
-    print(f"   - {dirs_removed} répertoires supprimés.")
-    
-    return True
-
-def main():
-    """Fonction principale."""
-    print("\n=== Nettoyage des fichiers non nécessaires ===\n")
-    
-    # Demander confirmation à l'utilisateur
-    print("⚠️ ATTENTION: Cette opération va supprimer définitivement les fichiers suivants:")
-    print(f"   - Tous les fichiers de cache dans '{TEXT_CACHE_DIR}'")
-    print(f"   - Tous les fichiers temporaires dans '{TEMP_DOWNLOADS_DIR}'")
-    print(f"   - Le fichier '{EXTRACT_SOURCES_JSON}' s'il existe")
-    print("\nAssurez-vous d'avoir d'abord créé le fichier encrypté complet.")
-    
-    confirmation = input("\nÊtes-vous sûr de vouloir continuer? (o/n): ")
-    if confirmation.lower() not in ["o", "oui", "y", "yes"]:
-        print("\nOpération annulée.")
-        sys.exit(0)
-    
-    # Nettoyer les fichiers
-    success = cleanup_files()
-    
-    if success:
-        print("\n[OK] Nettoyage des fichiers non nécessaires réussi !")
-    else:
-        print("\n❌ Échec du nettoyage des fichiers non nécessaires.")
-    
-    sys.exit(0 if success else 1)
-
-if __name__ == "__main__":
-    main()
diff --git a/argumentation_analysis/agents/tools/encryption/create_and_archive_encrypted_config.py b/argumentation_analysis/agents/tools/encryption/create_and_archive_encrypted_config.py
index f9d45822..5f258b2d 100644
--- a/argumentation_analysis/agents/tools/encryption/create_and_archive_encrypted_config.py
+++ b/argumentation_analysis/agents/tools/encryption/create_and_archive_encrypted_config.py
@@ -8,6 +8,7 @@ Script principal pour créer, vérifier et archiver le fichier encrypté complet
 import os
 import sys
 import time
+import shutil
 from pathlib import Path
 from dotenv import load_dotenv
 
@@ -22,6 +23,10 @@ load_dotenv(override=True)
 
 # Importer les modules nécessaires
 from argumentation_analysis.ui.config import CONFIG_FILE_ENC
+from argumentation_analysis.paths import DATA_DIR
+
+# Afficher les chemins pour le débogage
+print(f"Chemin du fichier encrypté: {CONFIG_FILE_ENC}")
 
 # Importer les fonctions des autres scripts
 from create_complete_encrypted_config import create_complete_encrypted_config
@@ -58,7 +63,7 @@ def main():
     time.sleep(2)
     
     # Créer un répertoire temporaire pour la vérification
-    temp_cache_dir = TEXT_CACHE_DIR = parent_dir / "text_cache"
+    temp_cache_dir = parent_dir / "text_cache"
     
     # Sauvegarder le répertoire de cache original
     original_cache_dir = parent_dir / "text_cache_original"
@@ -82,7 +87,6 @@ def main():
         if original_cache_dir.exists():
             if temp_cache_dir.exists():
                 try:
-                    import shutil
                     shutil.rmtree(temp_cache_dir)
                 except Exception:
                     pass
@@ -113,7 +117,6 @@ def main():
         # Supprimer le répertoire de cache
         if temp_cache_dir.exists():
             try:
-                import shutil
                 shutil.rmtree(temp_cache_dir)
                 print(f"[OK] Répertoire de cache '{temp_cache_dir}' supprimé.")
             except Exception as e:
@@ -122,7 +125,6 @@ def main():
         # Supprimer le répertoire de cache original
         if original_cache_dir.exists():
             try:
-                import shutil
                 shutil.rmtree(original_cache_dir)
                 print(f"[OK] Répertoire de cache original '{original_cache_dir}' supprimé.")
             except Exception as e:
@@ -145,10 +147,6 @@ def main():
         if original_cache_dir.exists():
             if temp_cache_dir.exists():
                 try:
-                    import shutil
-
-from argumentation_analysis.paths import DATA_DIR
-
                     shutil.rmtree(temp_cache_dir)
                 except Exception:
                     pass
diff --git a/argumentation_analysis/agents/tools/encryption/create_complete_encrypted_config.py b/argumentation_analysis/agents/tools/encryption/create_complete_encrypted_config.py
index 68b4f819..3f61ca80 100644
--- a/argumentation_analysis/agents/tools/encryption/create_complete_encrypted_config.py
+++ b/argumentation_analysis/agents/tools/encryption/create_complete_encrypted_config.py
@@ -27,9 +27,9 @@ load_dotenv(override=True)
 from argumentation_analysis.ui.config import ENCRYPTION_KEY, CONFIG_FILE_ENC
 from argumentation_analysis.ui.utils import reconstruct_url, get_cache_filepath
 
-# Définir les constantes
-EXTRACT_SOURCES_UPDATED_PATH = parent_dir / "utils" / "extract_repair" / "docs" / "extract_sources_updated.json"
-TEXT_CACHE_DIR = parent_dir / "text_cache"
+# Définir les constantes avec le chemin absolu correct
+EXTRACT_SOURCES_UPDATED_PATH = current_dir.parent.parent.parent / "utils" / "extract_repair" / "docs" / "extract_sources_updated.json"
+TEXT_CACHE_DIR = current_dir.parent.parent.parent / "text_cache"
 
 # Afficher les chemins pour le débogage
 print(f"Chemin du fichier de configuration: {EXTRACT_SOURCES_UPDATED_PATH}")
diff --git a/argumentation_analysis/agents/tools/encryption/load_complete_encrypted_config.py b/argumentation_analysis/agents/tools/encryption/load_complete_encrypted_config.py
deleted file mode 100644
index 6a6734b0..00000000
--- a/argumentation_analysis/agents/tools/encryption/load_complete_encrypted_config.py
+++ /dev/null
@@ -1,131 +0,0 @@
-#!/usr/bin/env python
-# -*- coding: utf-8 -*-
-
-"""
-Script pour charger le fichier encrypté complet et restaurer les fichiers de cache.
-"""
-
-import os
-import sys
-import json
-import gzip
-import hashlib
-from pathlib import Path
-from dotenv import load_dotenv
-from cryptography.fernet import Fernet
-
-# Ajouter le répertoire parent au chemin de recherche des modules
-current_dir = Path(__file__).parent
-parent_dir = current_dir.parent
-if str(parent_dir) not in sys.path:
-    sys.path.append(str(parent_dir))
-
-# Charger les variables d'environnement
-load_dotenv(override=True)
-
-# Importer les modules nécessaires
-from argumentation_analysis.ui.config import ENCRYPTION_KEY, CONFIG_FILE_ENC
-
-# Définir les constantes
-TEXT_CACHE_DIR = parent_dir / "text_cache"
-
-# Afficher les chemins pour le débogage
-print(f"Chemin du fichier encrypté: {CONFIG_FILE_ENC}")
-print(f"Chemin du répertoire de cache: {TEXT_CACHE_DIR}")
-
-def decrypt_data(encrypted_data, key):
-    """Déchiffre des données binaires avec une clé Fernet."""
-    if not key:
-        print("Erreur déchiffrement: Clé chiffrement manquante.")
-        return None
-    try:
-        f = Fernet(key)
-        return f.decrypt(encrypted_data)
-    except Exception as e:
-        print(f"Erreur déchiffrement: {e}")
-        return None
-
-def load_complete_encrypted_config():
-    """Charge le fichier encrypté complet et restaure les fichiers de cache."""
-    # Vérifier si la clé de chiffrement est disponible
-    if not ENCRYPTION_KEY:
-        print(f"❌ Erreur: La clé de chiffrement n'est pas disponible. Vérifiez la variable d'environnement 'TEXT_CONFIG_PASSPHRASE'.")
-        return False
-    
-    # Vérifier si le fichier encrypté existe
-    if not CONFIG_FILE_ENC.exists():
-        print(f"❌ Erreur: Le fichier encrypté '{CONFIG_FILE_ENC}' n'existe pas.")
-        return False
-    
-    try:
-        # Lire le contenu du fichier encrypté
-        with open(CONFIG_FILE_ENC, 'rb') as f:
-            encrypted_data = f.read()
-        
-        print(f"[OK] Fichier encrypté '{CONFIG_FILE_ENC}' chargé avec succès.")
-        print(f"   - Taille: {len(encrypted_data)} octets.")
-        
-        # Déchiffrer les données
-        decrypted_compressed_data = decrypt_data(encrypted_data, ENCRYPTION_KEY)
-        if not decrypted_compressed_data:
-            print(f"❌ Erreur: Échec du déchiffrement des données.")
-            return False
-        
-        # Décompresser les données
-        decompressed_data = gzip.decompress(decrypted_compressed_data)
-        print(f"[OK] Données décompressées: {len(decrypted_compressed_data)} -> {len(decompressed_data)} octets.")
-        
-        # Charger le JSON
-        complete_config = json.loads(decompressed_data.decode('utf-8'))
-        
-        # Extraire les sources et le cache
-        sources = complete_config.get("sources", [])
-        cache = complete_config.get("cache", {})
-        
-        print(f"[OK] Configuration chargée avec succès.")
-        print(f"   - {len(sources)} sources trouvées.")
-        print(f"   - {len(cache)} fichiers de cache trouvés.")
-        
-        # Créer le répertoire de cache s'il n'existe pas
-        TEXT_CACHE_DIR.mkdir(parents=True, exist_ok=True)
-        
-        # Restaurer les fichiers de cache
-        for url_hash, content in cache.items():
-            cache_filepath = TEXT_CACHE_DIR / f"{url_hash}.txt"
-            
-            # Écrire le contenu dans le fichier de cache
-            try:
-                cache_filepath.write_text(content, encoding='utf-8')
-                print(f"[OK] Fichier de cache '{cache_filepath.name}' restauré.")
-                print(f"   - Longueur: {len(content)} caractères.")
-            except Exception as e:
-                print(f"⚠️ Avertissement: Erreur lors de l'écriture du fichier de cache '{cache_filepath.name}': {e}")
-        
-        return True
-    
-    except Exception as e:
-        print(f"❌ Erreur inattendue: {e}")
-        return False
-
-def main():
-    """Fonction principale."""
-    print("\n=== Chargement du fichier encrypté complet ===\n")
-    
-    # Vérifier si la variable d'environnement TEXT_CONFIG_PASSPHRASE est définie
-    if not os.getenv("TEXT_CONFIG_PASSPHRASE"):
-        print(f"⚠️ La variable d'environnement 'TEXT_CONFIG_PASSPHRASE' n'est pas définie.")
-        print(f"   Veuillez la définir avant d'exécuter ce script.")
-        sys.exit(1)
-    
-    # Charger le fichier encrypté complet
-    success = load_complete_encrypted_config()
-    
-    if success:
-        print("\n[OK] Chargement du fichier encrypté complet réussi !")
-    else:
-        print("\n❌ Échec du chargement du fichier encrypté complet.")
-    
-    sys.exit(0 if success else 1)
-
-if __name__ == "__main__":
-    main()

==================== COMMIT: fbc8a904cf635aa27c3872ae18bbc25a4ff0df57 ====================
commit fbc8a904cf635aa27c3872ae18bbc25a4ff0df57
Author: jsboigeEpita <jean-sylvain.boige@epita.fr>
Date:   Mon Jun 16 02:11:00 2025 +0200

    MERGE: Apply stash with Tika/Conda fixes and file reorg

diff --git a/argumentation_analysis/agents/core/extract/extract_definitions.py b/argumentation_analysis/agents/core/extract/extract_definitions.py
index 66c02aa6..47520166 100644
--- a/argumentation_analysis/agents/core/extract/extract_definitions.py
+++ b/argumentation_analysis/agents/core/extract/extract_definitions.py
@@ -1,435 +1,321 @@
-# argumentation_analysis/agents/core/extract/extract_definitions.py
-"""
-Définitions et structures de données pour l'agent d'extraction.
-
-Ce module contient les classes Pydantic (ou similaires) et les structures de données
-utilisées par `ExtractAgent` et ses composants. Il définit :
-    - `ExtractResult`: Pour encapsuler le résultat d'une opération d'extraction.
-    - `ExtractAgentPlugin`: Un plugin contenant des fonctions natives utiles
-      pour le traitement de texte dans le contexte de l'extraction.
-    - `ExtractDefinition`: Pour représenter la définition d'un extrait spécifique
-      à rechercher dans un texte source.
-"""
-
-import re
-import logging
-from pathlib import Path # De la version stashed
-from typing import List, Dict, Any, Tuple, Optional, Union
-
-# Importer PROJECT_ROOT depuis la configuration centrale (de la version stashed)
-try:
-    from argumentation_analysis.ui.config import PROJECT_ROOT
-except ImportError:
-    # Fallback si le script est exécuté dans un contexte où l'import direct n'est pas possible
-    PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent.parent.parent
-
-# Configuration du logging (de la version stashed)
-logging.basicConfig(
-    level=logging.INFO,
-    format='%(asctime)s [%(levelname)s] [%(name)s] %(message)s',
-    datefmt='%H:%M:%S'
-)
-logger = logging.getLogger("ExtractAgent.Definitions")
-
-# Création d'un handler pour écrire les logs dans un fichier (de la version stashed)
-log_dir = PROJECT_ROOT / "_temp" / "logs"
-log_dir.mkdir(parents=True, exist_ok=True)
-log_file_path = log_dir / "extract_agent.log"
-
-file_handler = logging.FileHandler(log_file_path, mode='a', encoding='utf-8')
-file_handler.setFormatter(logging.Formatter('%(asctime)s [%(levelname)s] [%(name)s] %(message)s', datefmt='%H:%M:%S'))
-logger.addHandler(file_handler)
-
-
-class ExtractResult: # De la version HEAD (Updated upstream)
-    """
-    Classe représentant le résultat d'une opération d'extraction.
-
-    Cette classe encapsule toutes les informations pertinentes suite à une tentative
-    d'extraction, y compris le statut, les marqueurs, le texte extrait et
-    toute explication ou message d'erreur.
-
-    Attributes:
-        source_name (str): Nom de la source du texte.
-        extract_name (str): Nom de l'extrait.
-        status (str): Statut de l'extraction (ex: "valid", "rejected", "error").
-        message (str): Message descriptif concernant le résultat.
-        start_marker (str): Marqueur de début utilisé ou proposé.
-        end_marker (str): Marqueur de fin utilisé ou proposé.
-        template_start (str): Template de début utilisé ou proposé.
-        explanation (str): Explication fournie par l'agent pour l'extraction.
-        extracted_text (str): Le texte effectivement extrait.
-    """
-    
-    def __init__(
-        self,
-        source_name: str,
-        extract_name: str,
-        status: str,
-        message: str,
-        start_marker: str = "",
-        end_marker: str = "",
-        template_start: str = "",
-        explanation: str = "",
-        extracted_text: str = ""
-    ):
-        """
-        Initialise un objet `ExtractResult`.
-
-        :param source_name: Nom de la source du texte.
-        :type source_name: str
-        :param extract_name: Nom de l'extrait.
-        :type extract_name: str
-        :param status: Statut de l'extraction (par exemple, "valid", "rejected", "error").
-        :type status: str
-        :param message: Message descriptif concernant le résultat de l'extraction.
-        :type message: str
-        :param start_marker: Marqueur de début utilisé ou proposé. Par défaut "".
-        :type start_marker: str
-        :param end_marker: Marqueur de fin utilisé ou proposé. Par défaut "".
-        :type end_marker: str
-        :param template_start: Template de début utilisé ou proposé. Par défaut "".
-        :type template_start: str
-        :param explanation: Explication fournie par l'agent pour l'extraction. Par défaut "".
-        :type explanation: str
-        :param extracted_text: Le texte effectivement extrait. Par défaut "".
-        :type extracted_text: str
-        """
-        self.source_name = source_name
-        self.extract_name = extract_name
-        self.status = status
-        self.message = message
-        self.start_marker = start_marker
-        self.end_marker = end_marker
-        self.template_start = template_start
-        self.explanation = explanation
-        self.extracted_text = extracted_text
-    
-    def to_dict(self) -> Dict[str, Any]:
-        """Convertit l'instance `ExtractResult` en un dictionnaire.
-
-        :return: Un dictionnaire représentant l'objet.
-        :rtype: Dict[str, Any]
-        """
-        return {
-            "source_name": self.source_name,
-            "extract_name": self.extract_name,
-            "status": self.status,
-            "message": self.message,
-            "start_marker": self.start_marker,
-            "end_marker": self.end_marker,
-            "template_start": self.template_start,
-            "explanation": self.explanation,
-            "extracted_text": self.extracted_text
-        }
-    
-    @classmethod
-    def from_dict(cls, data: Dict[str, Any]) -> 'ExtractResult':
-        """Crée une instance de `ExtractResult` à partir d'un dictionnaire.
-
-        :param data: Dictionnaire contenant les données pour initialiser l'objet.
-        :type data: Dict[str, Any]
-        :return: Une nouvelle instance de `ExtractResult`.
-        :rtype: ExtractResult
-        """
-        return cls(
-            source_name=data.get("source_name", ""),
-            extract_name=data.get("extract_name", ""),
-            status=data.get("status", ""),
-            message=data.get("message", ""),
-            start_marker=data.get("start_marker", ""),
-            end_marker=data.get("end_marker", ""),
-            template_start=data.get("template_start", ""),
-            explanation=data.get("explanation", ""),
-            extracted_text=data.get("extracted_text", "")
-        )
-
-
-class ExtractAgentPlugin: # De la version HEAD (Updated upstream)
-    """
-    Plugin contenant des fonctions natives utiles pour l'agent d'extraction.
-
-    Ce plugin regroupe des méthodes de traitement de texte qui ne nécessitent pas
-    d'appel à un LLM mais sont utiles pour préparer les données ou analyser
-    les textes sources dans le cadre du processus d'extraction.
-
-    Attributes:
-        extract_results (List[Dict[str, Any]]): Une liste pour stocker les résultats
-            des opérations d'extraction effectuées, à des fins de journalisation ou de suivi.
-            (Note: L'utilisation de cette liste pourrait être revue pour une meilleure gestion d'état).
-    """
-    
-    def __init__(self):
-        """Initialise le plugin `ExtractAgentPlugin`.
-
-        Initialise une liste vide `extract_results` pour stocker les résultats
-        des opérations d'extraction effectuées par ce plugin.
-        """
-        self.extract_results: List[Dict[str, Any]] = []
-    
-    def find_similar_markers(
-        self, 
-        text: str, 
-        marker: str, 
-        max_results: int = 5,
-        find_similar_text_func=None
-    ) -> List[Dict[str, Any]]:
-        """
-        Trouve des marqueurs textuels similaires à un marqueur donné dans un texte source.
-
-        Utilise soit une fonction `find_similar_text_func` fournie, soit une
-        implémentation basique par défaut basée sur des regex simples.
-
-        :param text: Le texte source complet dans lequel rechercher.
-        :type text: str
-        :param marker: Le marqueur (chaîne de caractères) à rechercher.
-        :type marker: str
-        :param max_results: Le nombre maximum de résultats similaires à retourner.
-        :type max_results: int
-        :param find_similar_text_func: Fonction optionnelle à utiliser pour trouver
-                                       du texte similaire. Si None, une recherche
-                                       basique est effectuée.
-        :type find_similar_text_func: Optional[Callable]
-        :return: Une liste de dictionnaires, chaque dictionnaire représentant un marqueur
-                 similaire trouvé et contenant "marker", "position", et "context".
-                 Retourne une liste vide si aucun marqueur similaire n'est trouvé ou
-                 si `text` ou `marker` sont vides.
-        :rtype: List[Dict[str, Any]]
-        """
-        if not text or not marker:
-            return []
-        
-        if find_similar_text_func is None:
-            # Implémentation par défaut si la fonction n'est pas fournie
-            logger.warning("Fonction find_similar_text non fournie, utilisation d'une implémentation basique")
-            
-            similar_markers = []
-            try:
-                # Recherche simple avec regex
-                pattern = re.escape(marker[:min(10, len(marker))])
-                matches = list(re.finditer(pattern, text, re.IGNORECASE))
-                
-                for match in matches[:max_results]:
-                    start_pos = max(0, match.start() - 50)
-                    end_pos = min(len(text), match.end() + 50)
-                    context = text[start_pos:end_pos]
-                    
-                    similar_markers.append({
-                        "marker": match.group(),
-                        "position": match.start(),
-                        "context": context
-                    })
-                
-                return similar_markers
-            except Exception as e:
-                logger.error(f"Erreur lors de la recherche de marqueurs similaires: {e}")
-                return []
-        else:
-            # Utiliser la fonction fournie
-            similar_markers = []
-            results = find_similar_text_func(text, marker, context_size=50, max_results=max_results)
-            
-            for context, position, found_text in results:
-                similar_markers.append({
-                    "marker": found_text,
-                    "position": position,
-                    "context": context
-                })
-            
-            return similar_markers
-    
-    def search_text_dichotomically(
-        self, 
-        text: str, 
-        search_term: str, 
-        block_size: int = 500, 
-        overlap: int = 50
-    ) -> List[Dict[str, Any]]:
-        """
-        Recherche un terme dans un texte en le divisant d'abord en blocs.
-
-        Cette méthode est une simplification et ne réalise pas une recherche
-        dichotomique au sens strict algorithmique, mais plutôt une recherche
-        par blocs. Elle divise le texte en blocs avec chevauchement et recherche
-        le terme (insensible à la casse) dans chaque bloc.
-
-        :param text: Le texte source complet dans lequel rechercher.
-        :type text: str
-        :param search_term: Le terme à rechercher.
-        :type search_term: str
-        :param block_size: La taille des blocs dans lesquels diviser le texte.
-        :type block_size: int
-        :param overlap: Le chevauchement entre les blocs consécutifs.
-        :type overlap: int
-        :return: Une liste de dictionnaires. Chaque dictionnaire représente une
-                 correspondance trouvée et contient "match", "position", "context",
-                 "block_start", et "block_end".
-                 Retourne une liste vide si `text` ou `search_term` sont vides.
-        :rtype: List[Dict[str, Any]]
-        """
-        if not text or not search_term:
-            return []
-        
-        results = []
-        text_length = len(text)
-        
-        # Diviser le texte en blocs avec chevauchement
-        for i in range(0, text_length, block_size - overlap):
-            start_pos = i
-            end_pos = min(i + block_size, text_length)
-            block = text[start_pos:end_pos]
-            
-            # Rechercher le terme dans le bloc
-            if search_term.lower() in block.lower():
-                # Trouver toutes les occurrences
-                for match in re.finditer(re.escape(search_term), block, re.IGNORECASE):
-                    match_start = start_pos + match.start()
-                    match_end = start_pos + match.end()
-                    
-                    # Extraire le contexte
-                    context_start = max(0, match_start - 50)
-                    context_end = min(text_length, match_end + 50)
-                    context = text[context_start:context_end]
-                    
-                    results.append({
-                        "match": match.group(),
-                        "position": match_start,
-                        "context": context,
-                        "block_start": start_pos,
-                        "block_end": end_pos
-                    })
-        
-        return results
-    
-    def extract_blocks(
-        self, 
-        text: str, 
-        block_size: int = 500, 
-        overlap: int = 50
-    ) -> List[Dict[str, Any]]:
-        """
-        Divise un texte en blocs de taille spécifiée avec un chevauchement défini.
-
-        Utile pour traiter de grands textes par morceaux.
-
-        :param text: Le texte source complet à diviser en blocs.
-        :type text: str
-        :param block_size: La taille souhaitée pour chaque bloc de texte.
-        :type block_size: int
-        :param overlap: Le nombre de caractères de chevauchement entre les blocs consécutifs.
-        :type overlap: int
-        :return: Une liste de dictionnaires. Chaque dictionnaire représente un bloc et
-                 contient "block", "start_pos", et "end_pos".
-                 Retourne une liste vide si le texte d'entrée est vide.
-        :rtype: List[Dict[str, Any]]
-        """
-        if not text:
-            return []
-        
-        blocks = []
-        text_length = len(text)
-        
-        for i in range(0, text_length, block_size - overlap):
-            start_pos = i
-            end_pos = min(i + block_size, text_length)
-            block = text[start_pos:end_pos]
-            
-            blocks.append({
-                "block": block,
-                "start_pos": start_pos,
-                "end_pos": end_pos
-            })
-        
-        return blocks
-    
-    def get_extract_results(self) -> List[Dict[str, Any]]:
-        """Récupère la liste des résultats des opérations d'extraction stockées.
-
-        :return: Une liste de dictionnaires, chaque dictionnaire représentant
-                 le résultat d'une opération d'extraction.
-        :rtype: List[Dict[str, Any]]
-        """
-        return self.extract_results
-
-
-class ExtractDefinition: # De la version HEAD (Updated upstream)
-    """
-    Classe représentant la définition d'un extrait à rechercher ou à gérer.
-
-    Cette structure de données contient les informations nécessaires pour identifier
-    et localiser un segment de texte spécifique (un "extrait") au sein d'un
-    document source plus large.
-
-    Attributes:
-        source_name (str): Nom de la source du texte.
-        extract_name (str): Nom ou description de l'extrait.
-        start_marker (str): Le marqueur textuel indiquant le début de l'extrait.
-        end_marker (str): Le marqueur textuel indiquant la fin de l'extrait.
-        template_start (str): Un template optionnel qui peut précéder le `start_marker`.
-        description (str): Une description optionnelle de ce que représente l'extrait.
-    """
-    
-    def __init__(
-        self,
-        source_name: str,
-        extract_name: str,
-        start_marker: str,
-        end_marker: str,
-        template_start: str = "",
-        description: str = ""
-    ):
-        """
-        Initialise un objet `ExtractDefinition`.
-
-        :param source_name: Nom de la source du texte.
-        :type source_name: str
-        :param extract_name: Nom de l'extrait.
-        :type extract_name: str
-        :param start_marker: Marqueur de début pour l'extrait.
-        :type start_marker: str
-        :param end_marker: Marqueur de fin pour l'extrait.
-        :type end_marker: str
-        :param template_start: Template optionnel pour le marqueur de début. Par défaut "".
-        :type template_start: str
-        :param description: Description optionnelle de l'extraction. Par défaut "".
-        :type description: str
-        """
-        self.source_name = source_name
-        self.extract_name = extract_name
-        self.start_marker = start_marker
-        self.end_marker = end_marker
-        self.template_start = template_start
-        self.description = description
-    
-    def to_dict(self) -> Dict[str, Any]:
-        """Convertit l'instance `ExtractDefinition` en un dictionnaire.
-
-        :return: Un dictionnaire représentant l'objet.
-        :rtype: Dict[str, Any]
-        """
-        return {
-            "source_name": self.source_name,
-            "extract_name": self.extract_name,
-            "start_marker": self.start_marker,
-            "end_marker": self.end_marker,
-            "template_start": self.template_start,
-            "description": self.description
-        }
-    
-    @classmethod
-    def from_dict(cls, data: Dict[str, Any]) -> 'ExtractDefinition':
-        """Crée une instance de `ExtractDefinition` à partir d'un dictionnaire.
-
-        :param data: Dictionnaire contenant les données pour initialiser l'objet.
-        :type data: Dict[str, Any]
-        :return: Une nouvelle instance de `ExtractDefinition`.
-        :rtype: ExtractDefinition
-        """
-        return cls(
-            source_name=data.get("source_name", ""),
-            extract_name=data.get("extract_name", ""),
-            start_marker=data.get("start_marker", ""),
-            end_marker=data.get("end_marker", ""),
-            template_start=data.get("template_start", ""),
-            description=data.get("description", "")
+"""
+Définitions et structures de données pour l'agent d'extraction.
+
+Ce module contient les classes et structures de données utilisées par l'agent d'extraction
+pour gérer les extraits et leurs métadonnées.
+"""
+
+import re
+import logging
+from pathlib import Path # Ajout de l'import pour Path
+from typing import List, Dict, Any, Tuple, Optional, Union
+
+# Importer PROJECT_ROOT depuis la configuration centrale
+try:
+    from argumentation_analysis.ui.config import PROJECT_ROOT
+except ImportError:
+    # Fallback si le script est exécuté dans un contexte où l'import direct n'est pas possible
+    PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent.parent.parent
+
+# Configuration du logging
+logging.basicConfig(
+    level=logging.INFO,
+    format='%(asctime)s [%(levelname)s] [%(name)s] %(message)s',
+    datefmt='%H:%M:%S'
+)
+logger = logging.getLogger("ExtractAgent.Definitions")
+
+# Création d'un handler pour écrire les logs dans un fichier
+# Définition du chemin du fichier de log
+log_dir = PROJECT_ROOT / "_temp" / "logs"
+log_dir.mkdir(parents=True, exist_ok=True)
+log_file_path = log_dir / "extract_agent.log"
+
+file_handler = logging.FileHandler(log_file_path, mode='a', encoding='utf-8')
+file_handler.setFormatter(logging.Formatter('%(asctime)s [%(levelname)s] [%(name)s] %(message)s', datefmt='%H:%M:%S'))
+logger.addHandler(file_handler)
+
+
+class ExtractResult:
+    """Classe représentant le résultat d'une extraction."""
+    
+    def __init__(
+        self,
+        source_name: str,
+        extract_name: str,
+        status: str,
+        message: str,
+        start_marker: str = "",
+        end_marker: str = "",
+        template_start: str = "",
+        explanation: str = "",
+        extracted_text: str = ""
+    ):
+        """
+        Initialise un résultat d'extraction.
+        
+        Args:
+            source_name: Nom de la source
+            extract_name: Nom de l'extrait
+            status: Statut de l'extraction (valid, rejected, error)
+            message: Message explicatif
+            start_marker: Marqueur de début
+            end_marker: Marqueur de fin
+            template_start: Template pour le marqueur de début
+            explanation: Explication de l'extraction
+            extracted_text: Texte extrait
+        """
+        self.source_name = source_name
+        self.extract_name = extract_name
+        self.status = status
+        self.message = message
+        self.start_marker = start_marker
+        self.end_marker = end_marker
+        self.template_start = template_start
+        self.explanation = explanation
+        self.extracted_text = extracted_text
+    
+    def to_dict(self) -> Dict[str, Any]:
+        """Convertit le résultat en dictionnaire."""
+        return {
+            "source_name": self.source_name,
+            "extract_name": self.extract_name,
+            "status": self.status,
+            "message": self.message,
+            "start_marker": self.start_marker,
+            "end_marker": self.end_marker,
+            "template_start": self.template_start,
+            "explanation": self.explanation,
+            "extracted_text": self.extracted_text
+        }
+    
+    @classmethod
+    def from_dict(cls, data: Dict[str, Any]) -> 'ExtractResult':
+        """Crée un résultat à partir d'un dictionnaire."""
+        return cls(
+            source_name=data.get("source_name", ""),
+            extract_name=data.get("extract_name", ""),
+            status=data.get("status", ""),
+            message=data.get("message", ""),
+            start_marker=data.get("start_marker", ""),
+            end_marker=data.get("end_marker", ""),
+            template_start=data.get("template_start", ""),
+            explanation=data.get("explanation", ""),
+            extracted_text=data.get("extracted_text", "")
+        )
+
+
+class ExtractAgentPlugin:
+    """Plugin pour les fonctions natives de l'extracteur agentique."""
+    
+    def __init__(self):
+        """Initialise le plugin d'extraction."""
+        self.extract_results = []
+    
+    def find_similar_markers(
+        self, 
+        text: str, 
+        marker: str, 
+        max_results: int = 5,
+        find_similar_text_func=None
+    ) -> List[Dict[str, Any]]:
+        """
+        Trouve des marqueurs similaires dans le texte source.
+        
+        Args:
+            text: Texte source complet
+            marker: Marqueur à rechercher
+            max_results: Nombre maximum de résultats à retourner
+            find_similar_text_func: Fonction pour trouver du texte similaire
+            
+        Returns:
+            Liste de dictionnaires contenant les marqueurs similaires
+        """
+        if not text or not marker:
+            return []
+        
+        if find_similar_text_func is None:
+            # Implémentation par défaut si la fonction n'est pas fournie
+            logger.warning("Fonction find_similar_text non fournie, utilisation d'une implémentation basique")
+            
+            similar_markers = []
+            try:
+                # Recherche simple avec regex
+                pattern = re.escape(marker[:min(10, len(marker))])
+                matches = list(re.finditer(pattern, text, re.IGNORECASE))
+                
+                for match in matches[:max_results]:
+                    start_pos = max(0, match.start() - 50)
+                    end_pos = min(len(text), match.end() + 50)
+                    context = text[start_pos:end_pos]
+                    
+                    similar_markers.append({
+                        "marker": match.group(),
+                        "position": match.start(),
+                        "context": context
+                    })
+                
+                return similar_markers
+            except Exception as e:
+                logger.error(f"Erreur lors de la recherche de marqueurs similaires: {e}")
+                return []
+        else:
+            # Utiliser la fonction fournie
+            similar_markers = []
+            results = find_similar_text_func(text, marker, context_size=50, max_results=max_results)
+            
+            for context, position, found_text in results:
+                similar_markers.append({
+                    "marker": found_text,
+                    "position": position,
+                    "context": context
+                })
+            
+            return similar_markers
+    
+    def search_text_dichotomically(
+        self, 
+        text: str, 
+        search_term: str, 
+        block_size: int = 500, 
+        overlap: int = 50
+    ) -> List[Dict[str, Any]]:
+        """
+        Recherche un terme dans le texte en utilisant une approche dichotomique.
+        
+        Args:
+            text: Texte source complet
+            search_term: Terme à rechercher
+            block_size: Taille des blocs de texte à analyser
+            overlap: Chevauchement entre les blocs
+            
+        Returns:
+            Liste de dictionnaires contenant les résultats de recherche
+        """
+        if not text or not search_term:
+            return []
+        
+        results = []
+        text_length = len(text)
+        
+        # Diviser le texte en blocs avec chevauchement
+        for i in range(0, text_length, block_size - overlap):
+            start_pos = i
+            end_pos = min(i + block_size, text_length)
+            block = text[start_pos:end_pos]
+            
+            # Rechercher le terme dans le bloc
+            if search_term.lower() in block.lower():
+                # Trouver toutes les occurrences
+                for match in re.finditer(re.escape(search_term), block, re.IGNORECASE):
+                    match_start = start_pos + match.start()
+                    match_end = start_pos + match.end()
+                    
+                    # Extraire le contexte
+                    context_start = max(0, match_start - 50)
+                    context_end = min(text_length, match_end + 50)
+                    context = text[context_start:context_end]
+                    
+                    results.append({
+                        "match": match.group(),
+                        "position": match_start,
+                        "context": context,
+                        "block_start": start_pos,
+                        "block_end": end_pos
+                    })
+        
+        return results
+    
+    def extract_blocks(
+        self, 
+        text: str, 
+        block_size: int = 500, 
+        overlap: int = 50
+    ) -> List[Dict[str, Any]]:
+        """
+        Extrait des blocs de texte avec chevauchement pour l'analyse.
+        
+        Args:
+            text: Texte source complet
+            block_size: Taille des blocs de texte à extraire
+            overlap: Chevauchement entre les blocs
+            
+        Returns:
+            Liste de dictionnaires contenant les blocs de texte
+        """
+        if not text:
+            return []
+        
+        blocks = []
+        text_length = len(text)
+        
+        for i in range(0, text_length, block_size - overlap):
+            start_pos = i
+            end_pos = min(i + block_size, text_length)
+            block = text[start_pos:end_pos]
+            
+            blocks.append({
+                "block": block,
+                "start_pos": start_pos,
+                "end_pos": end_pos
+            })
+        
+        return blocks
+    
+    def get_extract_results(self) -> List[Dict[str, Any]]:
+        """Récupère les résultats des extractions effectuées."""
+        return self.extract_results
+
+
+class ExtractDefinition:
+    """Classe représentant la définition d'une extraction."""
+    
+    def __init__(
+        self,
+        source_name: str,
+        extract_name: str,
+        start_marker: str,
+        end_marker: str,
+        template_start: str = "",
+        description: str = ""
+    ):
+        """
+        Initialise une définition d'extraction.
+        
+        Args:
+            source_name: Nom de la source
+            extract_name: Nom de l'extrait
+            start_marker: Marqueur de début
+            end_marker: Marqueur de fin
+            template_start: Template pour le marqueur de début
+            description: Description de l'extraction
+        """
+        self.source_name = source_name
+        self.extract_name = extract_name
+        self.start_marker = start_marker
+        self.end_marker = end_marker
+        self.template_start = template_start
+        self.description = description
+    
+    def to_dict(self) -> Dict[str, Any]:
+        """Convertit la définition en dictionnaire."""
+        return {
+            "source_name": self.source_name,
+            "extract_name": self.extract_name,
+            "start_marker": self.start_marker,
+            "end_marker": self.end_marker,
+            "template_start": self.template_start,
+            "description": self.description
+        }
+    
+    @classmethod
+    def from_dict(cls, data: Dict[str, Any]) -> 'ExtractDefinition':
+        """Crée une définition à partir d'un dictionnaire."""
+        return cls(
+            source_name=data.get("source_name", ""),
+            extract_name=data.get("extract_name", ""),
+            start_marker=data.get("start_marker", ""),
+            end_marker=data.get("end_marker", ""),
+            template_start=data.get("template_start", ""),
+            description=data.get("description", "")
         )
\ No newline at end of file
diff --git a/argumentation_analysis/ui/config.py b/argumentation_analysis/ui/config.py
index 35421d34..bf0b2e30 100644
--- a/argumentation_analysis/ui/config.py
+++ b/argumentation_analysis/ui/config.py
@@ -1,150 +1,120 @@
-# ui/config.py
-import os
-import logging
-from pathlib import Path
-from dotenv import load_dotenv, find_dotenv # Gardé au cas où d'autres variables .env sont utilisées
-from cryptography.hazmat.primitives import hashes
-from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC
-from cryptography.hazmat.backends import default_backend
-import base64
-import json
-from argumentation_analysis.paths import DATA_DIR
-# Import pour la fonction de chargement JSON mutualisée (chemin corrigé)
-from argumentation_analysis.utils.core_utils.file_utils import load_json_file
-from pydantic import BaseModel, Field
-from typing import List, Dict, Any, Optional
-
-config_logger = logging.getLogger("App.UI.Config")
-if not config_logger.handlers and not config_logger.propagate:
-     handler = logging.StreamHandler(); formatter = logging.Formatter('%(asctime)s [%(levelname)s] [%(name)s] %(message)s', datefmt='%H:%M:%S'); handler.setFormatter(formatter); config_logger.addHandler(handler); config_logger.setLevel(logging.INFO)
-
-# --- Chargement .env et Dérivation Clé ---
-load_dotenv(find_dotenv()) # Gardé au cas où d'autres variables .env sont utilisées
-
-# MODIFICATION: Utiliser directement "Propaganda" comme passphrase
-TEXT_CONFIG_PASSPHRASE = "Propaganda"
-passphrase = TEXT_CONFIG_PASSPHRASE # Assignation directe
-ENCRYPTION_KEY = None
-FIXED_SALT = b'q\x8b\t\x97\x8b\xe9\xa3\xf2\xe4\x8e\xea\xf5\xe8\xb7\xd6\x8c' # Sel fixe
-
-config_logger.info(f"Utilisation de la phrase secrète fixe pour la dérivation de la clé.")
-if passphrase: # Cette condition sera toujours vraie maintenant
-    config_logger.info(f"[OK] Phrase secrète définie sur \"{passphrase}\". Dérivation de la clé...")
-    try:
-        kdf = PBKDF2HMAC(
-            algorithm=hashes.SHA256(), length=32, salt=FIXED_SALT,
-            iterations=480000, backend=default_backend()
-        )
-        derived_key_raw = kdf.derive(passphrase.encode('utf-8'))
-        ENCRYPTION_KEY = base64.urlsafe_b64encode(derived_key_raw)
-        if ENCRYPTION_KEY: config_logger.info("[OK] Clé de chiffrement dérivée et encodée.")
-    except Exception as e:
-        config_logger.error(f"⚠️ Erreur dérivation clé : {e}. Chiffrement désactivé.", exc_info=True)
-        ENCRYPTION_KEY = None
-else:
-    # Ce bloc ne devrait plus être atteint car passphrase est maintenant fixée.
-    config_logger.critical(f"⚠️ La phrase secrète n'est pas définie malgré la modification. Problème inattendu.")
-    ENCRYPTION_KEY = None
-
-# --- URLs et Chemins ---
-# Utiliser l'URL du serveur Tika depuis le fichier .env ou utiliser l'URL par défaut
-# Assurez-vous que l'URL du serveur Tika se termine par '/tika'
-tika_url_from_env = os.getenv("TIKA_SERVER_ENDPOINT")
-if tika_url_from_env:
-    # Nettoyer les guillemets potentiels au début/fin et s'assurer que c'est une chaîne
-    tika_url = str(tika_url_from_env).strip('"')
-    config_logger.info(f"TIKA_SERVER_ENDPOINT depuis .env (nettoyé): '{tika_url}'")
-else:
-    tika_url = "https://tika.open-webui.myia.io/tika" # Valeur par défaut si non définie
-    config_logger.info(f"TIKA_SERVER_ENDPOINT non trouvé dans .env, utilisation de la valeur par défaut: '{tika_url}'")
-
-# S'assurer que l'URL se termine correctement par /tika
-if tika_url.endswith('/tika'):
-    TIKA_SERVER_URL = tika_url
-else:
-    TIKA_SERVER_URL = f"{tika_url.rstrip('/')}/tika"
-TIKA_SERVER_TIMEOUT = int(os.getenv("TIKA_SERVER_TIMEOUT", "30"))
-config_logger.info(f"URL du serveur Tika: {TIKA_SERVER_URL}")
-JINA_READER_PREFIX = "https://r.jina.ai/"
-
-# Chemins relatifs au projet
-_project_root = Path(__file__).parent.parent.parent # Remonte de ui/ -> argumentation_analysis/ -> racine du projet
-CACHE_DIR = _project_root / "_temp" / "text_cache" # Modifié
-CONFIG_DIR = _project_root / "argumentation_analysis" / "data" # Maintenu ici (contient extract_sources.json.gz.enc)
-CONFIG_FILE_JSON = CONFIG_DIR / "extract_sources.json"
-CONFIG_FILE_ENC = CONFIG_DIR / "extract_sources.json.gz.enc"
-CONFIG_FILE = CONFIG_FILE_ENC
-TEMP_DOWNLOAD_DIR = _project_root / "_temp" / "temp_downloads" # Modifié
-
-# Extensions texte simple
-PLAINTEXT_EXTENSIONS = ['.txt', '.md', '.json', '.csv', '.xml', '.py', '.js', '.html', '.htm']
-
-# Création des répertoires nécessaires
-try:
-    CACHE_DIR.mkdir(parents=True, exist_ok=True)
-    config_logger.info(f"Cache répertoire assuré : {CACHE_DIR.resolve()}")
-    CONFIG_DIR.mkdir(parents=True, exist_ok=True) # S'assurer que data/ existe
-    config_logger.info(f"Répertoire config UI assuré : {CONFIG_DIR.resolve()}")
-    TEMP_DOWNLOAD_DIR.mkdir(parents=True, exist_ok=True)
-    config_logger.info(f"Répertoire temporaire assuré : {TEMP_DOWNLOAD_DIR.resolve()}")
-except Exception as e:
-    config_logger.error(f"Erreur création répertoires (cache/config/temp): {e}")
-
-
-# --- Définitions Sources par Défaut ---
-
-class ExtractItem(BaseModel):
-    extract_name: str
-    start_marker: str
-    end_marker: str
-    template_start: Optional[str] = None
-    description: Optional[str] = None
-    extracted_text: Optional[str] = None # Champ pour stocker le texte extrait
-
-class SourceDefinition(BaseModel):
-    source_name: str
-    source_type: str # 'url', 'file', 'jina', 'text'
-    schema_val: Optional[str] = Field(None, alias="schema") # 'http', 'https', 'file'
-    host_parts: Optional[List[str]] = None # Pour URL/Jina
-    path: Optional[str] = None # Pour URL/Jina/File
-    full_text: Optional[str] = None # Texte brut si type 'text' ou cache
-    extracts: List[ExtractItem] = Field(default_factory=list)
-    # Champs optionnels pour la gestion UI
-    is_expanded: Optional[bool] = True
-    is_editing: Optional[bool] = False
-    # Champs pour le cache et la récupération
-    last_fetched: Optional[str] = None # Timestamp ISO
-    fetch_error: Optional[str] = None
-    # Champs pour Jina
-    jina_url: Optional[str] = None # URL complète pour Jina (r.jina.ai/...)
-
-    class Config:
-        populate_by_name = True
-DEFAULT_EXTRACT_SOURCES = [
-    {"source_name": "Exemple Vide (Config manquante)", "source_type": "jina",
-     "schema": "https:", "host_parts": ["example", "com"], "path": "/",
-     "extracts": []}
-]
-
-# --- Chargement des Sources d'Extraction ---
-
-# La fonction load_extract_sources est maintenant remplacée par l'utilisation directe de
-# project_core.utils.file_utils.load_json_file où c'est nécessaire.
-# La logique de chargement initiale des EXTRACT_SOURCES est adaptée ci-dessous.
-
-EXTRACT_SOURCES = DEFAULT_EXTRACT_SOURCES # Initialisation par défaut
-
-# La logique de chargement dynamique de EXTRACT_SOURCES sera gérée par les scripts appelants
-# pour éviter les importations circulaires lors de l'initialisation du module.
-# EXTRACT_SOURCES reste initialisé avec DEFAULT_EXTRACT_SOURCES.
-config_logger.info(f"Config UI initialisée. EXTRACT_SOURCES est sur DEFAULT_EXTRACT_SOURCES. Le chargement dynamique est délégué.")
-
-# --- État Global (pour ce module UI) ---
-# current_extract_definitions n'est plus pertinent ici si le chargement est externe.
-# Si d'autres parties de ui/ l'utilisaient, il faudrait revoir. Pour l'instant, on le commente/supprime.
-# current_extract_definitions = []
-
-config_logger.info(f"Module config.py initialisé. {len(EXTRACT_SOURCES)} sources par défaut disponibles dans EXTRACT_SOURCES.")
-
-PROJECT_ROOT = _project_root
-config_logger.info(f"PROJECT_ROOT exporté: {PROJECT_ROOT}")
+# ui/config.py
+import os
+import logging
+from pathlib import Path
+from dotenv import load_dotenv, find_dotenv # Gardé au cas où d'autres variables .env sont utilisées
+from cryptography.hazmat.primitives import hashes
+from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC
+from cryptography.hazmat.backends import default_backend
+import base64
+import json
+from argumentation_analysis.paths import DATA_DIR
+# Import pour la fonction de chargement JSON mutualisée
+from project_core.utils.file_utils import load_json_file
+
+config_logger = logging.getLogger("App.UI.Config")
+if not config_logger.handlers and not config_logger.propagate:
+     handler = logging.StreamHandler(); formatter = logging.Formatter('%(asctime)s [%(levelname)s] [%(name)s] %(message)s', datefmt='%H:%M:%S'); handler.setFormatter(formatter); config_logger.addHandler(handler); config_logger.setLevel(logging.INFO)
+
+# --- Chargement .env et Dérivation Clé ---
+load_dotenv(find_dotenv()) # Gardé au cas où d'autres variables .env sont utilisées
+
+# MODIFICATION: Utiliser directement "Propaganda" comme passphrase
+TEXT_CONFIG_PASSPHRASE = "Propaganda"
+passphrase = TEXT_CONFIG_PASSPHRASE # Assignation directe
+ENCRYPTION_KEY = None
+FIXED_SALT = b'q\x8b\t\x97\x8b\xe9\xa3\xf2\xe4\x8e\xea\xf5\xe8\xb7\xd6\x8c' # Sel fixe
+
+config_logger.info(f"Utilisation de la phrase secrète fixe pour la dérivation de la clé.")
+if passphrase: # Cette condition sera toujours vraie maintenant
+    config_logger.info(f"✅ Phrase secrète définie sur \"{passphrase}\". Dérivation de la clé...")
+    try:
+        kdf = PBKDF2HMAC(
+            algorithm=hashes.SHA256(), length=32, salt=FIXED_SALT,
+            iterations=480000, backend=default_backend()
+        )
+        derived_key_raw = kdf.derive(passphrase.encode('utf-8'))
+        ENCRYPTION_KEY = base64.urlsafe_b64encode(derived_key_raw)
+        if ENCRYPTION_KEY: config_logger.info("✅ Clé de chiffrement dérivée et encodée.")
+    except Exception as e:
+        config_logger.error(f"⚠️ Erreur dérivation clé : {e}. Chiffrement désactivé.", exc_info=True)
+        ENCRYPTION_KEY = None
+else:
+    # Ce bloc ne devrait plus être atteint car passphrase est maintenant fixée.
+    config_logger.critical(f"⚠️ La phrase secrète n'est pas définie malgré la modification. Problème inattendu.")
+    ENCRYPTION_KEY = None
+
+# --- URLs et Chemins ---
+# Utiliser l'URL du serveur Tika depuis le fichier .env ou utiliser l'URL par défaut
+# Assurez-vous que l'URL du serveur Tika se termine par '/tika'
+tika_url_from_env = os.getenv("TIKA_SERVER_ENDPOINT")
+if tika_url_from_env:
+    # Nettoyer les guillemets potentiels au début/fin et s'assurer que c'est une chaîne
+    tika_url = str(tika_url_from_env).strip('"')
+    config_logger.info(f"TIKA_SERVER_ENDPOINT depuis .env (nettoyé): '{tika_url}'")
+else:
+    tika_url = "https://tika.open-webui.myia.io/tika" # Valeur par défaut si non définie
+    config_logger.info(f"TIKA_SERVER_ENDPOINT non trouvé dans .env, utilisation de la valeur par défaut: '{tika_url}'")
+
+# S'assurer que l'URL se termine correctement par /tika
+if tika_url.endswith('/tika'):
+    TIKA_SERVER_URL = tika_url
+else:
+    TIKA_SERVER_URL = f"{tika_url.rstrip('/')}/tika"
+TIKA_SERVER_TIMEOUT = int(os.getenv("TIKA_SERVER_TIMEOUT", "30"))
+config_logger.info(f"URL du serveur Tika: {TIKA_SERVER_URL}")
+JINA_READER_PREFIX = "https://r.jina.ai/"
+
+# Chemins relatifs au projet
+_project_root = Path(__file__).parent.parent.parent # Remonte de ui/ -> argumentation_analysis/ -> racine du projet
+CACHE_DIR = _project_root / "_temp" / "text_cache" # Modifié
+CONFIG_DIR = _project_root / "argumentation_analysis" / "data" # Maintenu ici (contient extract_sources.json.gz.enc)
+CONFIG_FILE_JSON = CONFIG_DIR / "extract_sources.json"
+CONFIG_FILE_ENC = CONFIG_DIR / "extract_sources.json.gz.enc"
+CONFIG_FILE = CONFIG_FILE_ENC
+TEMP_DOWNLOAD_DIR = _project_root / "_temp" / "temp_downloads" # Modifié
+
+# Extensions texte simple
+PLAINTEXT_EXTENSIONS = ['.txt', '.md', '.json', '.csv', '.xml', '.py', '.js', '.html', '.htm']
+
+# Création des répertoires nécessaires
+try:
+    CACHE_DIR.mkdir(parents=True, exist_ok=True)
+    config_logger.info(f"Cache répertoire assuré : {CACHE_DIR.resolve()}")
+    CONFIG_DIR.mkdir(parents=True, exist_ok=True) # S'assurer que data/ existe
+    config_logger.info(f"Répertoire config UI assuré : {CONFIG_DIR.resolve()}")
+    TEMP_DOWNLOAD_DIR.mkdir(parents=True, exist_ok=True)
+    config_logger.info(f"Répertoire temporaire assuré : {TEMP_DOWNLOAD_DIR.resolve()}")
+except Exception as e:
+    config_logger.error(f"Erreur création répertoires (cache/config/temp): {e}")
+
+
+# --- Définitions Sources par Défaut ---
+DEFAULT_EXTRACT_SOURCES = [
+    {"source_name": "Exemple Vide (Config manquante)", "source_type": "jina",
+     "schema": "https:", "host_parts": ["example", "com"], "path": "/",
+     "extracts": []}
+]
+
+# --- Chargement des Sources d'Extraction ---
+
+# La fonction load_extract_sources est maintenant remplacée par l'utilisation directe de
+# project_core.utils.file_utils.load_json_file où c'est nécessaire.
+# La logique de chargement initiale des EXTRACT_SOURCES est adaptée ci-dessous.
+
+EXTRACT_SOURCES = DEFAULT_EXTRACT_SOURCES # Initialisation par défaut
+
+# La logique de chargement dynamique de EXTRACT_SOURCES sera gérée par les scripts appelants
+# pour éviter les importations circulaires lors de l'initialisation du module.
+# EXTRACT_SOURCES reste initialisé avec DEFAULT_EXTRACT_SOURCES.
+config_logger.info(f"Config UI initialisée. EXTRACT_SOURCES est sur DEFAULT_EXTRACT_SOURCES. Le chargement dynamique est délégué.")
+
+# --- État Global (pour ce module UI) ---
+# current_extract_definitions n'est plus pertinent ici si le chargement est externe.
+# Si d'autres parties de ui/ l'utilisaient, il faudrait revoir. Pour l'instant, on le commente/supprime.
+# current_extract_definitions = []
+
+config_logger.info(f"Module config.py initialisé. {len(EXTRACT_SOURCES)} sources par défaut disponibles dans EXTRACT_SOURCES.")
+
+PROJECT_ROOT = _project_root
+config_logger.info(f"PROJECT_ROOT exporté: {PROJECT_ROOT}")
\ No newline at end of file
diff --git a/argumentation_analysis/ui/file_operations.py b/argumentation_analysis/ui/file_operations.py
index 8ae710f9..61ab6356 100644
--- a/argumentation_analysis/ui/file_operations.py
+++ b/argumentation_analysis/ui/file_operations.py
@@ -1,152 +1,172 @@
-# argumentation_analysis/ui/file_operations.py
-from typing import Optional, Union, List, Dict, Any
-import json
-import gzip
-import logging
-import base64 
-from pathlib import Path
-# from typing import Optional, List, Dict, Any # Redondant avec la première ligne
-from cryptography.fernet import InvalidToken 
-
-from . import config as ui_config_module
-from .utils import get_full_text_for_source, utils_logger 
-from argumentation_analysis.utils.core_utils.crypto_utils import encrypt_data_with_fernet, decrypt_data_with_fernet
-
-file_ops_logger = utils_logger
-
-
-def load_extract_definitions(
-    config_file: Path,
-    b64_derived_key: Optional[str], 
-    app_config: Optional[Dict[str, Any]] = None 
-) -> list:
-    """Charge, déchiffre et décompresse les définitions depuis le fichier chiffré."""
-    # Utiliser uniquement DEFAULT_EXTRACT_SOURCES comme fallback pour éviter le cycle avec EXTRACT_SOURCES
-    # qui est en cours de définition par l'appelant (config.py)
-    fallback_definitions = ui_config_module.DEFAULT_EXTRACT_SOURCES
-
-    if not config_file.exists():
-        file_ops_logger.info(f"Fichier config '{config_file}' non trouvé. Utilisation définitions par défaut.")
-        return [item.copy() for item in fallback_definitions]
-
-    if b64_derived_key: # Clé fournie, tenter le déchiffrement
-        file_ops_logger.info(f"Chargement et déchiffrement de '{config_file}' avec clé...")
-        try:
-            with open(config_file, 'rb') as f: encrypted_data = f.read()
-            decrypted_compressed_data = decrypt_data_with_fernet(encrypted_data, b64_derived_key)
-            
-            if not decrypted_compressed_data: # decrypt_data_with_fernet retourne None en cas d'InvalidToken ou autre erreur de déchiffrement
-                file_ops_logger.warning(f"[WARN] Echec du dechiffrement pour '{config_file}' (decrypt_data_with_fernet a retourne None). Utilisation des definitions par defaut.")
-                return [item.copy() for item in fallback_definitions]
-            
-            decompressed_data = gzip.decompress(decrypted_compressed_data)
-            definitions = json.loads(decompressed_data.decode('utf-8'))
-            file_ops_logger.info("[OK] Definitions chargees et dechiffrees.")
-
-        except InvalidToken:
-            file_ops_logger.error(f"[FAIL] InvalidToken explicitement levee lors du dechiffrement de '{config_file}'. Utilisation definitions par defaut.", exc_info=True)
-            return [item.copy() for item in fallback_definitions]
-        except Exception as e:
-            file_ops_logger.error(f"[FAIL] Erreur chargement/dechiffrement '{config_file}': {e}. Utilisation definitions par defaut.", exc_info=True)
-            return [item.copy() for item in fallback_definitions]
-    
-    else: # Pas de clé, essayer de lire comme JSON simple
-        file_ops_logger.info(f"Aucune clé fournie. Tentative de chargement de '{config_file}' comme JSON simple...")
-        try:
-            with open(config_file, 'r', encoding='utf-8') as f:
-                definitions = json.load(f)
-            file_ops_logger.info(f"[OK] Définitions chargées comme JSON simple depuis '{config_file}'.")
-        
-        except json.JSONDecodeError as e_json:
-            file_ops_logger.error(f"[FAIL] Erreur decodage JSON pour '{config_file}': {e_json}. L'exception sera relancee.", exc_info=False)
-            raise
-        except Exception as e:
-            file_ops_logger.error(f"[FAIL] Erreur chargement JSON simple '{config_file}': {e}. Utilisation definitions par defaut.", exc_info=True)
-            return [item.copy() for item in fallback_definitions]
-
-    # Validation du format (commun aux deux chemins)
-    if not isinstance(definitions, list) or not all(
-        isinstance(item, dict) and
-        "source_name" in item and "source_type" in item and "schema" in item and
-        "host_parts" in item and "path" in item and isinstance(item.get("extracts"), list)
-        for item in definitions
-    ):
-        file_ops_logger.warning(f"[WARN] Format definitions invalide apres chargement de '{config_file}'. Utilisation definitions par defaut.")
-        return [item.copy() for item in fallback_definitions]
-
-    file_ops_logger.info(f"-> {len(definitions)} définitions chargées depuis '{config_file}'.")
-    return definitions
-
-def save_extract_definitions(
-    extract_definitions: List[Dict[str, Any]],
-    config_file: Path,
-    b64_derived_key: Optional[Union[str, bytes]], 
-    embed_full_text: bool = False,
-    config: Optional[Dict[str, Any]] = None 
-) -> bool:
-    """Sauvegarde, compresse et chiffre les définitions dans le fichier.
-    Peut optionnellement embarquer le texte complet des sources.
-    """
-    if not b64_derived_key: 
-        file_ops_logger.error("Clé chiffrement (b64_derived_key) absente ou vide. Sauvegarde annulée.")
-        return False
-    if not isinstance(extract_definitions, list):
-        file_ops_logger.error("Erreur sauvegarde: définitions non valides (doit être une liste).")
-        return False
-
-    file_ops_logger.info(f"Préparation sauvegarde vers '{config_file}'...")
-
-    definitions_to_process = [dict(d) for d in extract_definitions]
-
-
-    if embed_full_text:
-        file_ops_logger.info("Option embed_full_text activée. Tentative de récupération des textes complets manquants...")
-        for source_info in definitions_to_process: 
-            if not isinstance(source_info, dict):
-                file_ops_logger.warning(f"Élément non-dictionnaire ignoré dans extract_definitions: {type(source_info)}")
-                continue
-
-            current_full_text = source_info.get("full_text")
-            if not current_full_text:
-                source_name = source_info.get('source_name', 'Source inconnue')
-                file_ops_logger.info(f"Texte complet manquant pour '{source_name}'. Récupération...")
-                try:
-                    retrieved_text = get_full_text_for_source(source_info, app_config=config)
-                    if retrieved_text is not None:
-                        source_info["full_text"] = retrieved_text
-                        file_ops_logger.info(f"Texte complet récupéré et ajouté pour '{source_name}'.")
-                    else:
-                        file_ops_logger.warning(f"Échec de la récupération du texte complet (texte vide retourné) pour '{source_name}'. Champ 'full_text' non peuplé.")
-                        source_info["full_text"] = None
-                except ConnectionError as e_conn:
-                    file_ops_logger.warning(f"Erreur de connexion lors de la récupération du texte pour '{source_name}': {e_conn}. Champ 'full_text' non peuplé.")
-                    source_info["full_text"] = None
-                except Exception as e_get_text:
-                    file_ops_logger.error(f"Erreur inattendue lors de la récupération du texte pour '{source_name}': {e_get_text}. Champ 'full_text' non peuplé.", exc_info=True)
-                    source_info["full_text"] = None
-    else:
-        file_ops_logger.info("Option embed_full_text désactivée. Suppression des textes complets des définitions...")
-        for source_info in definitions_to_process: 
-            if not isinstance(source_info, dict):
-                continue
-            if "full_text" in source_info:
-                source_info.pop("full_text", None)
-                file_ops_logger.debug(f"Champ 'full_text' retiré pour '{source_info.get('source_name', 'Source inconnue')}'.")
-
-    try:
-        json_data = json.dumps(definitions_to_process, indent=2, ensure_ascii=False).encode('utf-8')
-        compressed_data = gzip.compress(json_data)
-        encrypted_data_to_save = encrypt_data_with_fernet(compressed_data, b64_derived_key)
-        if not encrypted_data_to_save:
-            raise ValueError("Échec du chiffrement des données (encrypt_data_with_fernet a retourné None).")
-
-        config_file.parent.mkdir(parents=True, exist_ok=True)
-        with open(config_file, 'wb') as f:
-            f.write(encrypted_data_to_save)
-        file_ops_logger.info(f"[OK] Définitions sauvegardées dans '{config_file}'.")
-        return True
-    except Exception as e:
-        file_ops_logger.error(f"[FAIL] Erreur lors de la sauvegarde chiffree vers '{config_file}': {e}", exc_info=True)
-        return False
-
+from typing import Optional, Union, List, Dict, Any
+# argumentation_analysis/ui/file_operations.py
+import json
+import gzip
+import logging
+import base64 # NOUVEAU: Pour décoder la clé b64 en clé Fernet
+from pathlib import Path
+from typing import Optional, List, Dict, Any
+from cryptography.fernet import InvalidToken # NÉCESSAIRE pour lever l'exception
+# cryptography.fernet et exceptions sont maintenant gérés dans project_core.utils.crypto_utils
+# et les fonctions encrypt/decrypt sont importées depuis là.
+# from cryptography.fernet import Fernet, InvalidToken # SUPPRIMÉ
+# from cryptography.exceptions import InvalidSignature # SUPPRIMÉ
+
+# Importer les éléments nécessaires depuis config et utils
+# Attention à ne pas recréer de cycle.
+# On importe 'config as ui_config_module' pour accéder aux constantes.
+from . import config as ui_config_module
+# On importe les fonctions de utils qui ne dépendent pas de config de manière cyclique.
+# encrypt_data et decrypt_data sont maintenant importées depuis project_core
+from .utils import get_full_text_for_source, utils_logger # utils_logger est déjà configuré dans utils.py
+from project_core.utils.crypto_utils import encrypt_data_with_fernet, decrypt_data_with_fernet # NOUVEAU
+
+# Logger spécifique pour les opérations sur fichiers si besoin, ou utiliser utils_logger
+file_ops_logger = utils_logger # Ou logging.getLogger("App.UI.FileOps")
+
+
+def load_extract_definitions(
+    config_file: Path,
+    b64_derived_key: Optional[str], # MODIFIÉ: La clé reçue est la chaîne b64 dérivée, rendue optionnelle
+    # app_config est utilisé par get_full_text_for_source, mais load_extract_definitions
+    # lui-même ne l'utilise pas directement pour le chargement/déchiffrement.
+    # Cependant, si on voulait que load_extract_definitions peuple les full_text au chargement,
+    # il faudrait le passer. Pour l'instant, on le garde optionnel et non utilisé ici.
+    app_config: Optional[Dict[str, Any]] = None 
+) -> list:
+    """Charge, déchiffre et décompresse les définitions depuis le fichier chiffré."""
+    # Utiliser uniquement DEFAULT_EXTRACT_SOURCES comme fallback pour éviter le cycle avec EXTRACT_SOURCES
+    # qui est en cours de définition par l'appelant (config.py)
+    fallback_definitions = ui_config_module.DEFAULT_EXTRACT_SOURCES
+
+    if not config_file.exists():
+        file_ops_logger.info(f"Fichier config '{config_file}' non trouvé. Utilisation définitions par défaut.")
+        return [item.copy() for item in fallback_definitions]
+
+    if b64_derived_key: # Clé fournie, tenter le déchiffrement
+        file_ops_logger.info(f"Chargement et déchiffrement de '{config_file}' avec clé...")
+        try:
+            with open(config_file, 'rb') as f: encrypted_data = f.read()
+            decrypted_compressed_data = decrypt_data_with_fernet(encrypted_data, b64_derived_key)
+            
+            if not decrypted_compressed_data: # decrypt_data_with_fernet retourne None en cas d'InvalidToken ou autre erreur de déchiffrement
+                file_ops_logger.warning(f"⚠️ Échec du déchiffrement pour '{config_file}' (decrypt_data_with_fernet a retourné None). Utilisation des définitions par défaut.")
+                return [item.copy() for item in fallback_definitions] # MODIFIÉ: Retourner fallback au lieu de lever
+            
+            decompressed_data = gzip.decompress(decrypted_compressed_data)
+            definitions = json.loads(decompressed_data.decode('utf-8'))
+            file_ops_logger.info("✅ Définitions chargées et déchiffrées.")
+
+        except InvalidToken: # Attrapé si decrypt_data_with_fernet lève InvalidToken (par ex. mock avec side_effect)
+            file_ops_logger.error(f"❌ InvalidToken explicitement levée lors du déchiffrement de '{config_file}'. Utilisation définitions par défaut.", exc_info=True)
+            return [item.copy() for item in fallback_definitions] # MODIFIÉ: Retourner fallback
+        except Exception as e:
+            file_ops_logger.error(f"❌ Erreur chargement/déchiffrement '{config_file}': {e}. Utilisation définitions par défaut.", exc_info=True)
+            return [item.copy() for item in fallback_definitions]
+    
+    else: # Pas de clé, essayer de lire comme JSON simple
+        file_ops_logger.info(f"Aucune clé fournie. Tentative de chargement de '{config_file}' comme JSON simple...")
+        try:
+            with open(config_file, 'r', encoding='utf-8') as f:
+                definitions = json.load(f)
+            file_ops_logger.info(f"✅ Définitions chargées comme JSON simple depuis '{config_file}'.")
+        
+        except json.JSONDecodeError as e_json:
+            file_ops_logger.error(f"❌ Erreur décodage JSON pour '{config_file}': {e_json}. L'exception sera relancée.", exc_info=False)
+            raise
+        except Exception as e:
+            file_ops_logger.error(f"❌ Erreur chargement JSON simple '{config_file}': {e}. Utilisation définitions par défaut.", exc_info=True)
+            return [item.copy() for item in fallback_definitions]
+
+    # Validation du format (commun aux deux chemins)
+    if not isinstance(definitions, list) or not all(
+        isinstance(item, dict) and
+        "source_name" in item and "source_type" in item and "schema" in item and
+        "host_parts" in item and "path" in item and isinstance(item.get("extracts"), list)
+        for item in definitions
+    ):
+        file_ops_logger.warning(f"⚠️ Format définitions invalide après chargement de '{config_file}'. Utilisation définitions par défaut.")
+        return [item.copy() for item in fallback_definitions]
+
+    file_ops_logger.info(f"-> {len(definitions)} définitions chargées depuis '{config_file}'.")
+    return definitions
+
+def save_extract_definitions(
+    extract_definitions: List[Dict[str, Any]],
+    config_file: Path,
+    b64_derived_key: Optional[Union[str, bytes]], # MODIFIÉ: Accepte str, bytes ou None
+    embed_full_text: bool = False,
+    config: Optional[Dict[str, Any]] = None # 'config' est le app_config passé à get_full_text_for_source
+) -> bool:
+    """Sauvegarde, compresse et chiffre les définitions dans le fichier.
+    Peut optionnellement embarquer le texte complet des sources.
+    """
+    if not b64_derived_key: # Vérifie si la clé est None ou une chaîne/bytes vide
+        file_ops_logger.error("Clé chiffrement (b64_derived_key) absente ou vide. Sauvegarde annulée.")
+        return False
+    if not isinstance(extract_definitions, list):
+        file_ops_logger.error("Erreur sauvegarde: définitions non valides (doit être une liste).")
+        return False
+
+    file_ops_logger.info(f"Préparation sauvegarde vers '{config_file}'...")
+
+    # Copie profonde pour éviter de modifier la liste originale en dehors de cette fonction
+    # lors du traitement de embed_full_text
+    definitions_to_process = [dict(d) for d in extract_definitions]
+
+
+    if embed_full_text:
+        file_ops_logger.info("Option embed_full_text activée. Tentative de récupération des textes complets manquants...")
+        for source_info in definitions_to_process: # Utiliser la copie
+            if not isinstance(source_info, dict):
+                file_ops_logger.warning(f"Élément non-dictionnaire ignoré dans extract_definitions: {type(source_info)}")
+                continue
+
+            current_full_text = source_info.get("full_text")
+            if not current_full_text:
+                source_name = source_info.get('source_name', 'Source inconnue')
+                file_ops_logger.info(f"Texte complet manquant pour '{source_name}'. Récupération...")
+                try:
+                    # Utilise get_full_text_for_source de utils.py
+                    retrieved_text = get_full_text_for_source(source_info, app_config=config)
+                    if retrieved_text is not None:
+                        source_info["full_text"] = retrieved_text
+                        file_ops_logger.info(f"Texte complet récupéré et ajouté pour '{source_name}'.")
+                    else:
+                        file_ops_logger.warning(f"Échec de la récupération du texte complet (texte vide retourné) pour '{source_name}'. Champ 'full_text' non peuplé.")
+                        source_info["full_text"] = None
+                except ConnectionError as e_conn:
+                    file_ops_logger.warning(f"Erreur de connexion lors de la récupération du texte pour '{source_name}': {e_conn}. Champ 'full_text' non peuplé.")
+                    source_info["full_text"] = None
+                except Exception as e_get_text:
+                    file_ops_logger.error(f"Erreur inattendue lors de la récupération du texte pour '{source_name}': {e_get_text}. Champ 'full_text' non peuplé.", exc_info=True)
+                    source_info["full_text"] = None
+    else:
+        file_ops_logger.info("Option embed_full_text désactivée. Suppression des textes complets des définitions...")
+        for source_info in definitions_to_process: # Utiliser la copie
+            if not isinstance(source_info, dict):
+                continue
+            if "full_text" in source_info:
+                source_info.pop("full_text", None)
+                file_ops_logger.debug(f"Champ 'full_text' retiré pour '{source_info.get('source_name', 'Source inconnue')}'.")
+
+    try:
+        # actual_fernet_key = base64.urlsafe_b64decode(b64_derived_key.encode('utf-8')) # SUPPRIMÉ: La fonction attend la str b64
+        json_data = json.dumps(definitions_to_process, indent=2, ensure_ascii=False).encode('utf-8') # Utiliser la copie traitée
+        compressed_data = gzip.compress(json_data)
+        # MODIFIÉ: Utilisation de encrypt_data_with_fernet avec la clé b64_derived_key (str)
+        encrypted_data_to_save = encrypt_data_with_fernet(compressed_data, b64_derived_key)
+        if not encrypted_data_to_save:
+            # encrypt_data_with_fernet logge déjà l'erreur
+            raise ValueError("Échec du chiffrement des données (encrypt_data_with_fernet a retourné None).")
+
+        config_file.parent.mkdir(parents=True, exist_ok=True)
+        with open(config_file, 'wb') as f:
+            f.write(encrypted_data_to_save)
+        file_ops_logger.info(f"✅ Définitions sauvegardées dans '{config_file}'.")
+        return True
+    except Exception as e:
+        file_ops_logger.error(f"❌ Erreur lors de la sauvegarde chiffrée vers '{config_file}': {e}", exc_info=True)
+        return False
+
 file_ops_logger.info("Fonctions d'opérations sur fichiers UI définies.")
\ No newline at end of file
diff --git a/scripts/data_processing/embed_all_sources.py b/scripts/data_processing/embed_all_sources.py
index d18074da..658533a6 100644
--- a/scripts/data_processing/embed_all_sources.py
+++ b/scripts/data_processing/embed_all_sources.py
@@ -1,60 +1,90 @@
-import project_core.core_from_scripts.auto_env
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
 Script pour s'assurer que toutes les sources dans une configuration d'extraits
-ont leur texte source complet (`full_text`) embarqué, et optionnellement
-générer des embeddings pour ces textes.
+ont leur texte source complet (`full_text`) embarqué.
 """
 
 import argparse
 import logging
+import os
 import sys
 from pathlib import Path
+import json
+# import base64 # Supprimé car la dérivation de clé est retirée de ce script
+# from cryptography.hazmat.primitives import hashes # Supprimé
+# from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC # Supprimé
+# from cryptography.hazmat.backends import default_backend # Supprimé
 
 # Assurer que le répertoire racine du projet est dans sys.path
+# pour permettre les imports relatifs (ex: from argumentation_analysis.ui import utils)
 SCRIPT_DIR = Path(__file__).resolve().parent
-PROJECT_ROOT = SCRIPT_DIR.parent.parent 
+PROJECT_ROOT = SCRIPT_DIR.parent.parent
 sys.path.insert(0, str(PROJECT_ROOT))
 
 try:
-    # Import de la nouvelle fonction de pipeline
-    from argumentation_analysis.pipelines.embedding_pipeline import run_embedding_generation_pipeline
+    # Importer les fonctions load/save depuis file_operations
+    from argumentation_analysis.ui.file_operations import load_extract_definitions, save_extract_definitions
+    # Importer get_full_text_for_source depuis utils
+    from argumentation_analysis.ui.utils import get_full_text_for_source
+    # Importer les configurations UI si nécessaire (par exemple, pour TIKA_SERVER_URL)
+    from argumentation_analysis.ui import config as ui_config
+    # Importer ENCRYPTION_KEY directement depuis la configuration UI
+    from argumentation_analysis.ui.config import ENCRYPTION_KEY as CONFIG_UI_ENCRYPTION_KEY
+    # Importer la fonction sanitize_filename depuis project_core.utils
+    from project_core.utils.file_utils import sanitize_filename, load_document_content
 except ImportError as e:
     print(f"Erreur d'importation: {e}. Assurez-vous que le script est exécuté depuis la racine du projet "
           "et que l'environnement est correctement configuré.")
     sys.exit(1)
 
-# Configuration du logging (prise de la version Stash)
+# Configuration du logging
 log_dir = PROJECT_ROOT / "_temp" / "logs"
 log_dir.mkdir(parents=True, exist_ok=True)
 log_file_path = log_dir / "embed_all_sources.log"
 
-# Le logger est configuré plus bas dans main() en utilisant args.log_level
-# Cette configuration initiale est un fallback ou sera écrasée.
 logging.basicConfig(level=logging.INFO,
-                    format='%(asctime)s - %(levelname)s - [%(name)s] %(message)s',
-                    datefmt='%H:%M:%S',
+                    format='%(asctime)s - %(levelname)s - %(message)s',
                     handlers=[
-                        logging.FileHandler(log_file_path, mode='a', encoding='utf-8'),
-                        logging.StreamHandler(sys.stdout)
+                        logging.FileHandler(log_file_path, mode='a', encoding='utf-8'), # 'a' pour append
+                        logging.StreamHandler(sys.stdout) # Garder les logs sur la console aussi
                     ])
 logger = logging.getLogger(__name__)
 
+# La fonction derive_key_from_passphrase est supprimée car ENCRYPTION_KEY de ui.config sera utilisée.
+# FIXED_SALT n'est plus directement utilisé ici non plus.
+
+# def derive_key_from_passphrase(passphrase: str) -> bytes:
+#     """
+#     Dérive une clé Fernet à partir d'une passphrase.
+#     Utilise la même logique que le vrai code.
+#     """
+#     if not passphrase:
+#         raise ValueError("Passphrase vide")
+    
+#     kdf = PBKDF2HMAC(
+#         algorithm=hashes.SHA256(),
+#         length=32,
+#         salt=CONFIG_FIXED_SALT,  # Utilisation du sel importé
+#         iterations=480000,
+#         backend=default_backend()
+#     )
+#     derived_key_raw = kdf.derive(passphrase.encode('utf-8'))
+#     return base64.urlsafe_b64encode(derived_key_raw)
+
 
 def main():
     """
     Fonction principale du script.
-    Parse les arguments et appelle le pipeline de génération d'embeddings.
     """
     parser = argparse.ArgumentParser(
-        description="Lanceur pour le pipeline d'embarquement de texte source complet et de génération d'embeddings."
+        description="Embarque le texte source complet dans un fichier de configuration d'extraits."
     )
     parser.add_argument(
         "--input-config",
         type=Path,
-        required=False,
+        required=False, # Modifié pour ne plus être requis si --json-string ou --input-json-file est utilisé
         help="Chemin vers le fichier de configuration chiffré d'entrée (.json.gz.enc). Optionnel si --json-string ou --input-json-file est fourni."
     )
     parser.add_argument(
@@ -79,65 +109,222 @@ def main():
         "--passphrase",
         type=str,
         default=None,
-        help="Passphrase (OBSOLÈTE pour la dérivation de clé dans ce script, ENCRYPTION_KEY de ui.config est utilisée par le pipeline). "
-             "Passé au pipeline pour information si nécessaire par d'autres composants."
+        help="Passphrase (OBSOLÈTE pour la dérivation de clé dans ce script, ENCRYPTION_KEY de ui.config est utilisée). "
+             "Peut être gardé pour une vérification future ou si une interaction avec la passphrase est nécessaire ailleurs."
     )
     parser.add_argument(
         "--force",
         action="store_true",
         help="Écrase le fichier de sortie s'il existe déjà."
     )
-    parser.add_argument(
-       "--generate-embeddings",
-       type=str,
-       metavar="MODEL_NAME",
-       default=None,
-       help="Nom du modèle d'embedding à utiliser pour générer les embeddings des textes complets. "
-            "Si fourni, active la génération d'embeddings."
-    )
-    parser.add_argument(
-        "--log-level",
-        type=str,
-        default="INFO",
-        choices=["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"],
-        help="Niveau de verbosité du logging pour le pipeline et ce script."
-    )
 
     args = parser.parse_args()
 
-    # Reconfigurer le logging de base avec le niveau choisi pour ce script
-    # et pour le logger de ce module spécifique.
-    logging.basicConfig(level=args.log_level.upper(), 
-                        format='%(asctime)s - %(levelname)s - [%(name)s] %(message)s',
-                        datefmt='%H:%M:%S',
-                        handlers=[
-                            logging.FileHandler(log_file_path, mode='a', encoding='utf-8'),
-                            logging.StreamHandler(sys.stdout)
-                        ],
-                        force=True) # force=True pour réécrire la config si basicConfig a déjà été appelé
-    logger.setLevel(args.log_level.upper()) # Assurer que le logger de ce module respecte aussi le niveau
-
-    logger.info(f"Lancement du script '{Path(__file__).name}'. Délégation au pipeline d'embedding...")
-    logger.debug(f"Arguments reçus: {args}")
+    logger.info(f"Démarrage du script d'embarquement des sources.")
+    if args.input_json_file:
+        logger.info(f"Utilisation des définitions JSON fournies via --input-json-file: {args.input_json_file}")
+    elif args.json_string:
+        logger.info(f"Utilisation des définitions JSON fournies via --json-string.")
+    elif args.input_config:
+        logger.info(f"Fichier d'entrée (chiffré): {args.input_config}")
+    else:
+        logger.error("Aucune source de configuration d'entrée spécifiée (--input-json-file, --json-string, ou --input-config). Arrêt.")
+        sys.exit(1)
+    logger.info(f"Fichier de sortie: {args.output_config}")
+
+    # 1. La configuration de l'application est gérée par ui_config
+    # Plus besoin de charger explicitement app_config ici
+
+
+    # 2. Obtenir la passphrase - N'est plus utilisé pour dériver la clé ici.
+    # La clé ENCRYPTION_KEY de ui.config est directement utilisée.
+    # passphrase = args.passphrase or os.getenv("TEXT_CONFIG_PASSPHRASE")
+    # if not passphrase:
+    #     logger.error("Passphrase non fournie (ni via --passphrase, ni via TEXT_CONFIG_PASSPHRASE). Arrêt.")
+    #     sys.exit(1)
+    # logger.info("Passphrase obtenue (pour information seulement).")
+
+    # 3. Vérifier les fichiers d'entrée/sortie
+    input_source_specified = args.input_json_file or args.json_string or args.input_config
+
+    if not input_source_specified:
+        logger.error("Aucune source de configuration d'entrée (--input-json-file, --json-string, ou --input-config) n'a été fournie. Arrêt.")
+        sys.exit(1)
+
+    if args.input_json_file and not args.input_json_file.exists():
+        logger.error(f"Le fichier d'entrée JSON {args.input_json_file} n'existe pas. Arrêt.")
+        sys.exit(1)
+    elif args.input_config and not args.input_config.exists() and not args.json_string and not args.input_json_file:
+         logger.error(f"Le fichier d'entrée chiffré {args.input_config} n'existe pas et aucune autre source n'est fournie. Arrêt.")
+         sys.exit(1)
+
+    if args.output_config.exists() and not args.force:
+        logger.error(
+            f"Le fichier de sortie {args.output_config} existe déjà. Utilisez --force pour l'écraser. Arrêt."
+        )
+        sys.exit(1)
+    elif args.output_config.exists() and args.force:
+        logger.warning(f"Le fichier de sortie {args.output_config} existe et sera écrasé (--force activé).")
+
+    # Créer le répertoire parent pour le fichier de sortie s'il n'existe pas
+    args.output_config.parent.mkdir(parents=True, exist_ok=True)
+
+    # 4. Charger les définitions d'extraits
+    extract_definitions = []
+    # Utiliser directement la clé de chiffrement de ui.config
+    encryption_key_to_use = CONFIG_UI_ENCRYPTION_KEY
+    if not encryption_key_to_use:
+        logger.error("ENCRYPTION_KEY n'est pas disponible depuis argumentation_analysis.ui.config. Impossible de continuer.")
+        sys.exit(1)
+    logger.info(f"Utilisation de ENCRYPTION_KEY directement depuis ui.config ('{encryption_key_to_use[:10].decode('utf-8', 'ignore')}...') pour toutes les opérations de chiffrement/déchiffrement.")
+
+    if args.input_json_file:
+        try:
+            logger.info(f"Chargement des définitions d'extraits depuis le fichier JSON: {args.input_json_file}...")
+            with open(args.input_json_file, 'r', encoding='utf-8') as f:
+                extract_definitions = json.load(f)
+            if not isinstance(extract_definitions, list):
+                logger.error(f"Le fichier JSON {args.input_json_file} ne contient pas une liste de définitions. Arrêt.")
+                sys.exit(1)
+            logger.info(f"{len(extract_definitions)} définitions d'extraits chargées depuis {args.input_json_file}.")
+        except json.JSONDecodeError as e:
+            logger.error(f"Erreur lors du décodage du fichier JSON {args.input_json_file}: {e}. Arrêt.")
+            sys.exit(1)
+        except Exception as e:
+            logger.error(f"Erreur lors de la lecture du fichier JSON {args.input_json_file}: {e}. Arrêt.")
+            sys.exit(1)
+    elif args.json_string:
+        try:
+            logger.info("Chargement des définitions d'extraits depuis la chaîne JSON fournie...")
+            extract_definitions = json.loads(args.json_string)
+            if not isinstance(extract_definitions, list):
+                logger.error("La chaîne JSON fournie ne contient pas une liste de définitions. Arrêt.")
+                sys.exit(1)
+            logger.info(f"{len(extract_definitions)} définitions d'extraits chargées depuis la chaîne JSON.")
+        except json.JSONDecodeError as e:
+            logger.error(f"Erreur lors du décodage de la chaîne JSON: {e}. Arrêt.")
+            sys.exit(1)
+    elif args.input_config: # Doit être un fichier chiffré
+        try:
+            logger.info(f"Chargement et déchiffrement des définitions d'extraits depuis: {args.input_config}...")
+            loaded_defs = load_extract_definitions(
+                config_file=args.input_config,
+                key=encryption_key_to_use # Utilisation de la clé de ui.config
+            )
+            if not loaded_defs:
+                 logger.warning(f"Aucune définition d'extrait trouvée ou erreur de chargement depuis {args.input_config}.")
+                 extract_definitions = []
+            else:
+                extract_definitions = loaded_defs
+            logger.info(f"{len(extract_definitions)} définitions d'extraits chargées et déchiffrées depuis le fichier.")
+        except Exception as e:
+            logger.error(f"Erreur lors du chargement ou du déchiffrement de {args.input_config}: {e}")
+            sys.exit(1)
+    else:
+        # Ce cas ne devrait pas être atteint à cause des vérifications précédentes
+        logger.error("Aucune source de configuration (fichier JSON, chaîne JSON, ou fichier chiffré) n'a été traitée. Arrêt.")
+        sys.exit(1)
+
+    # 5. Traiter chaque source_info
+    updated_sources_count = 0
+    sources_with_errors_count = 0
+
+    for i, source_info in enumerate(extract_definitions):
+        source_id = source_info.get('id', f"Source_{i+1}")
+        logger.info(f"Traitement de la source: {source_id} ({source_info.get('type', 'N/A')}: {source_info.get('path', 'N/A')})")
+
+        if source_info.get('full_text') and source_info['full_text'].strip():
+            logger.info(f"  Le texte complet est déjà présent pour la source {source_id}.")
+        else:
+            logger.info(f"  Texte complet manquant pour la source {source_id}. Tentative de récupération...")
+            try:
+                # fetch_method = source_info.get("fetch_method", source_info.get("source_type")) # Ancienne logique
+                current_source_type = source_info.get("source_type")
+                current_fetch_method = source_info.get("fetch_method", current_source_type) # Garde la logique originale pour fetch_method si source_type n'est pas tika
+
+                full_text_content = None
+                logger.info(f"  Détermination de la méthode de récupération pour {source_id}: source_type='{current_source_type}', fetch_method='{current_fetch_method}'")
+
+                if current_source_type == "tika":
+                    logger.info(f"  Source {source_id} est de type 'tika'. Utilisation de get_full_text_for_source pour traitement Tika (même si fetch_method est '{current_fetch_method}').")
+                    # On s'attend à ce que get_full_text_for_source utilise le 'path' si disponible pour les sources 'tika' locales
+                    full_text_content = get_full_text_for_source(source_info)
+                elif current_fetch_method == "file": # Gère les fichiers non-Tika (txt, md)
+                    file_path_str = source_info.get("path")
+                    if file_path_str:
+                        document_path = Path(file_path_str)
+                        if not document_path.is_absolute():
+                            document_path = PROJECT_ROOT / file_path_str
+                        document_path = document_path.resolve()
+                        logger.info(f"  Utilisation de load_document_content pour le fichier texte/markdown : {document_path}")
+                        full_text_content = load_document_content(document_path) # load_document_content ne gère pas Tika
+                    else:
+                        logger.error(f"  Champ 'path' manquant pour la source locale de type 'file': {source_id}.")
+                else: # Gère les autres types (web, jina, etc. qui ne sont pas 'tika' et pas 'file')
+                    logger.info(f"  Utilisation de get_full_text_for_source pour la source {source_id} (type/méthode: {current_fetch_method}).")
+                    full_text_content = get_full_text_for_source(source_info)
+
+                if full_text_content:
+                    source_info['full_text'] = full_text_content
+                    logger.info(f"  Texte complet récupéré et mis à jour pour la source {source_id} (longueur: {len(full_text_content)}).")
+                    updated_sources_count += 1
+                else:
+                    logger.warning(f"  Impossible de récupérer le texte complet pour la source {source_id} (méthode: {current_fetch_method}). full_text reste vide.")
+                    sources_with_errors_count += 1
+            except Exception as e:
+                logger.error(f"  Erreur lors de la récupération du texte pour la source {source_id}: {e}")
+                sources_with_errors_count += 1
+        
+        # LOG SPÉCIFIQUE POUR SOURCE_4
+        if source_id == "Source_4":
+            logger.info(f"--- DEBUG Source_4 ---")
+            logger.info(f"  ID: {source_id}")
+            logger.info(f"  Type: {source_info.get('source_type')}")
+            logger.info(f"  Fetch Method: {source_info.get('fetch_method')}")
+            logger.info(f"  Path: {source_info.get('path')}")
+            retrieved_text = source_info.get('full_text') # Ne pas mettre de valeur par défaut ici pour voir si c'est None
+            logger.info(f"  Full_text récupéré (premiers 300 caractères): {str(retrieved_text)[:300] if retrieved_text else 'VIDE ou None'}")
+            logger.info(f"  Longueur full_text: {len(retrieved_text) if retrieved_text else 0}")
+            logger.info(f"--- FIN DEBUG Source_4 ---")
+
+    logger.info(f"Traitement des sources terminé. {updated_sources_count} sources mises à jour, {sources_with_errors_count} erreurs de récupération.")
+
+    # 6. Sauvegarder la version non chiffrée pour débogage
+    unencrypted_output_path = PROJECT_ROOT / "_temp" / "final_processed_config_unencrypted.json"
+    try:
+        logger.info(f"Création du répertoire _temp s'il n'existe pas: {unencrypted_output_path.parent}")
+        unencrypted_output_path.parent.mkdir(parents=True, exist_ok=True)
+        logger.info(f"Sauvegarde des définitions traitées (non chiffrées) dans {unencrypted_output_path}...")
+        with open(unencrypted_output_path, 'w', encoding='utf-8') as f_unencrypted:
+            json.dump(extract_definitions, f_unencrypted, indent=2, ensure_ascii=False)
+        logger.info(f"Définitions traitées (non chiffrées) sauvegardées avec succès dans {unencrypted_output_path}.")
+    except Exception as e:
+        logger.error(f"Erreur lors de la sauvegarde du fichier JSON non chiffré {unencrypted_output_path}: {e}")
+        # Continuer même si cette sauvegarde échoue, car la sauvegarde chiffrée est prioritaire.
 
+    # 7. Sauvegarder les définitions mises à jour (chiffrées)
+    # Toujours tenter de sauvegarder, même si extract_definitions est vide, pour créer le fichier de sortie.
+    # La fonction save_extract_definitions gérera une liste vide.
     try:
-        run_embedding_generation_pipeline(
-            input_config_path=args.input_config,
-            json_string=args.json_string,
-            input_json_file_path=args.input_json_file,
-            output_config_path=args.output_config,
-            generate_embeddings_model=args.generate_embeddings,
-            force_overwrite=args.force,
-            log_level=args.log_level, 
-            passphrase=args.passphrase
+        logger.info(f"Sauvegarde des définitions d'extraits (mises à jour ou vides) dans {args.output_config}...")
+        # Note: la fonction save_extract_definitions dans file_operations attend 'config_file' et 'encryption_key'
+        save_success = save_extract_definitions(
+            extract_definitions=extract_definitions, # Peut être une liste vide
+            config_file=args.output_config,
+            b64_derived_key=encryption_key_to_use, # Utilisation de la clé de ui.config
+            embed_full_text=True # embed_full_text=True est important pour que le script tente d'ajouter les textes
         )
-        logger.info(f"Pipeline d'embedding terminé. Le script '{Path(__file__).name}' a fini son exécution.")
-    except SystemExit:
-        logger.warning(f"Le pipeline s'est terminé prématurément (SystemExit). Vérifiez les logs du pipeline.")
+        if save_success:
+            logger.info(f"Définitions d'extraits sauvegardées avec succès dans {args.output_config}.")
+        else:
+            # L'erreur aura déjà été logguée par save_extract_definitions
+            logger.error(f"Échec de la sauvegarde des définitions dans {args.output_config}.")
+            # sys.exit(1) # On pourrait choisir de sortir ici si la sauvegarde est critique même pour un fichier vide
     except Exception as e:
-        logger.error(f"Une erreur s'est produite lors de l'exécution du pipeline d'embedding: {e}", exc_info=True)
+        logger.error(f"Erreur majeure lors de la tentative de sauvegarde des définitions dans {args.output_config}: {e}")
         sys.exit(1)
 
+    logger.info("Script d'embarquement des sources terminé avec succès.")
 
 if __name__ == "__main__":
     main()
\ No newline at end of file

==================== COMMIT: 1046ac0b99ce59890292e3ca9d7cfc3ad72b0fb0 ====================
commit 1046ac0b99ce59890292e3ca9d7cfc3ad72b0fb0
Merge: 67f9a25b eeb909ab
Author: jsboigeEpita <jean-sylvain.boige@epita.fr>
Date:   Mon Jun 16 02:09:41 2025 +0200

    Merge branch 'main' of https://github.com/jsboigeEpita/2025-Epita-Intelligence-Symbolique

diff --cc argumentation_analysis/orchestration/analysis_runner.py
index 8866e26e,a201ed33..1468f20c
--- a/argumentation_analysis/orchestration/analysis_runner.py
+++ b/argumentation_analysis/orchestration/analysis_runner.py
@@@ -26,12 -26,24 +26,22 @@@ from semantic_kernel.connectors.ai.open
  from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion
  from semantic_kernel.contents.chat_message_content import ChatMessageContent as SKChatMessageContent # Alias pour éviter conflit
  from semantic_kernel.kernel import Kernel as SKernel # Alias pour éviter conflit avec Kernel de SK
+ # KernelArguments est déjà importé plus bas
+  # Imports Semantic Kernel
+ import semantic_kernel as sk
+ from semantic_kernel.contents import ChatMessageContent, ChatRole as AuthorRole
+ # CORRECTIF COMPATIBILITÉ: Utilisation du module de compatibilité
+ from argumentation_analysis.utils.semantic_kernel_compatibility import AgentGroupChat, ChatCompletionAgent, Agent
+ from semantic_kernel_compatibility import AgentChatException
+ from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion, AzureChatCompletion # Pour type hint
+ from semantic_kernel.connectors.ai.function_choice_behavior import FunctionChoiceBehavior
+ from semantic_kernel.functions.kernel_arguments import KernelArguments
  
  # Imports Semantic Kernel
 -from semantic_kernel.agents import AgentGroupChat, Agent
  import semantic_kernel as sk
 -from semantic_kernel.contents import ChatMessageContent
 -from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion, AzureChatCompletion
 -from semantic_kernel.contents.utils.author_role import AuthorRole
  from semantic_kernel.contents.chat_history import ChatHistory
 +from semantic_kernel.contents.chat_message_content import ChatMessageContent
 +from semantic_kernel.contents.role import Role
  
  # Correct imports
  from argumentation_analysis.core.shared_state import RhetoricalAnalysisState

==================== COMMIT: 67f9a25bd7fd0b0205e7780cf25c5efb6782b4ea ====================
commit 67f9a25bd7fd0b0205e7780cf25c5efb6782b4ea
Author: jsboigeEpita <jean-sylvain.boige@epita.fr>
Date:   Mon Jun 16 02:03:15 2025 +0200

    feat: Intégration des stashes et réparation de l'orchestrateur
    
    Applique une série de stashes contenant des refactorings majeurs et des corrections. Corrige une erreur d'appel de méthode dans l'orchestrateur web et nettoie le backend manager.

diff --git a/.gitignore b/.gitignore
index 29f9895e..7c6ec735 100644
--- a/.gitignore
+++ b/.gitignore
@@ -189,7 +189,7 @@ test-results/
 node_modules/
 
 # Temporary files
-# .temp/
+.temp/
 environment_evaluation_report.json
 
 # Fichiers temporaires de tests
diff --git a/activate_project_env.ps1 b/activate_project_env.ps1
index 7c2bfd8c..2a9baa2d 100644
--- a/activate_project_env.ps1
+++ b/activate_project_env.ps1
@@ -25,6 +25,7 @@ param(
 )
 
 # Configuration
+Write-Host "[DEBUG] Activating environment..."
 $ProjectRoot = $PSScriptRoot
 $PythonModule = "project_core/core_from_scripts/environment_manager.py"
 
@@ -116,4 +117,5 @@ try {
 } catch {
     Write-Log "Erreur critique: $($_.Exception.Message)" "ERROR"
     exit 1
-}
\ No newline at end of file
+}
+Write-Host "[DEBUG] Environment script finished."
\ No newline at end of file
diff --git a/argumentation_analysis/agents/core/extract/extract_agent.py b/argumentation_analysis/agents/core/extract/extract_agent.py
index 337d0dc2..abb3ac6c 100644
--- a/argumentation_analysis/agents/core/extract/extract_agent.py
+++ b/argumentation_analysis/agents/core/extract/extract_agent.py
@@ -26,118 +26,47 @@ from pathlib import Path
 from typing import List, Dict, Any, Tuple, Optional, Union, Callable, ClassVar
 
 import semantic_kernel as sk
-from semantic_kernel.contents import ChatMessageContent, AuthorRole
+from semantic_kernel.contents import ChatMessageContent
 from semantic_kernel.functions.kernel_arguments import KernelArguments
 from semantic_kernel.prompt_template.prompt_template_config import PromptTemplateConfig
 
-# Import de la classe de base
 from ..abc.agent_bases import BaseAgent
 
 # Import des définitions et prompts locaux
-from .extract_definitions import ExtractAgentPlugin, ExtractResult # ExtractAgentPlugin sera utilisé comme plugin natif
+from .extract_definitions import ExtractAgentPlugin, ExtractResult
 from .prompts import (
-    EXTRACT_AGENT_INSTRUCTIONS, # Utilisé comme system_prompt
-    # VALIDATION_AGENT_INSTRUCTIONS, # Plus nécessaire si validation_agent est supprimé
+    EXTRACT_AGENT_INSTRUCTIONS,
     EXTRACT_FROM_NAME_PROMPT,
     VALIDATE_EXTRACT_PROMPT,
-    # REPAIR_EXTRACT_PROMPT # Non utilisé actuellement
 )
 
 # Fonction d'importation paresseuse pour éviter les importations circulaires
 def _lazy_imports() -> None:
     """
     Importe les modules de manière paresseuse pour éviter les importations circulaires.
-
-    Cette fonction est appelée une fois au chargement du module `extract_agent`.
-    Elle peuple les variables globales nécessaires à partir d'autres modules,
-    principalement `argumentation_analysis.ui.config`,
-    `argumentation_analysis.ui.utils`, et `argumentation_analysis.ui.extract_utils`.
-    Cela permet à `ExtractAgent` d'accéder à des configurations et des utilitaires
-    sans créer de dépendances d'importation directes au niveau supérieur du module,
-    ce qui pourrait causer des problèmes si ces modules importent eux-mêmes `ExtractAgent`.
-
-    Les variables globales peuplées incluent `ENCRYPTION_KEY`, `CONFIG_FILE`,
-    `CONFIG_FILE_JSON`, `load_from_cache`, `reconstruct_url`, `load_source_text`,
-    `extract_text_with_markers`, `find_similar_text`,
-    `load_extract_definitions_safely`, et `save_extract_definitions_safely`.
     """
-    global ENCRYPTION_KEY, CONFIG_FILE, CONFIG_FILE_JSON
-    global load_from_cache, reconstruct_url
     global load_source_text, extract_text_with_markers, find_similar_text
-    global load_extract_definitions_safely, save_extract_definitions_safely
-    # global create_llm_service # create_llm_service ne sera plus appelé ici
-
     try:
-        # Import relatif depuis le package agents/core
-        # Corrigé de '....' à '...' car ui est un sous-répertoire de argumentation_analysis
-        from ...ui.config import ENCRYPTION_KEY, CONFIG_FILE, CONFIG_FILE_JSON
-        from ...ui.utils import load_from_cache, reconstruct_url
         from ...ui.extract_utils import (
             load_source_text, extract_text_with_markers, find_similar_text,
-            load_extract_definitions_safely, save_extract_definitions_safely
         )
-        # from ...core.llm_service import create_llm_service # Déplacé
     except ImportError:
-        # Fallback pour les imports absolus
-        from argumentation_analysis.ui.config import ENCRYPTION_KEY, CONFIG_FILE, CONFIG_FILE_JSON
-        from argumentation_analysis.ui.utils import load_from_cache, reconstruct_url
         from argumentation_analysis.ui.extract_utils import (
             load_source_text, extract_text_with_markers, find_similar_text,
-            load_extract_definitions_safely, save_extract_definitions_safely
         )
-        # from argumentation_analysis.core.llm_service import create_llm_service # Déplacé
-
-# Appeler la fonction d'importation paresseuse avec diagnostic JVM
-try:
-    import jpype
-    print(f"[DIAGNOSTIC] extract_agent.py: État JVM AVANT _lazy_imports(): started={jpype.isJVMStarted()}")
-except ImportError:
-    print("[DIAGNOSTIC] extract_agent.py: jpype non disponible avant _lazy_imports()")
 
 _lazy_imports()
 
-try:
-    import jpype
-    print(f"[DIAGNOSTIC] extract_agent.py: État JVM APRÈS _lazy_imports(): started={jpype.isJVMStarted()}")
-except ImportError:
-    print("[DIAGNOSTIC] extract_agent.py: jpype non disponible après _lazy_imports()")
-
-# Configuration du logging - BaseAgent s'en charge, mais on peut garder le handler spécifique
-# logger = logging.getLogger(__name__) # Sera initialisé par BaseAgent
-
-# Création d'un handler pour écrire les logs dans un fichier
-# file_handler = logging.FileHandler("extract_agent.log") # Peut être configuré au niveau du projet
-# file_handler.setFormatter(logging.Formatter('%(asctime)s [%(levelname)s] [%(name)s] %(message)s', datefmt='%H:%M:%S'))
-# logger.addHandler(file_handler) # Le logger de BaseAgent peut être utilisé
-
 
 class ExtractAgent(BaseAgent):
     """
     Agent spécialisé dans l'extraction d'informations pertinentes de textes sources.
-
-    Hérite de `BaseAgent` et implémente des fonctionnalités pour proposer,
-    valider, réparer et gérer des extraits de texte. Utilise des fonctions
-    sémantiques pour l'interaction avec les LLMs et un plugin natif pour
-    des opérations textuelles spécifiques.
-
-    Attributes:
-        EXTRACT_SEMANTIC_FUNCTION_NAME (str): Nom de la fonction sémantique pour la proposition d'extraits.
-        VALIDATE_SEMANTIC_FUNCTION_NAME (str): Nom de la fonction sémantique pour la validation d'extraits.
-        NATIVE_PLUGIN_NAME (str): Nom du plugin natif contenant les fonctions d'aide à l'extraction.
-        find_similar_text_func (Callable): Fonction pour trouver du texte similaire.
-        extract_text_func (Callable): Fonction pour extraire du texte basé sur des marqueurs.
-        _native_extract_plugin (Optional[ExtractAgentPlugin]): Instance du plugin natif.
     """
     
-    # Noms pour les fonctions sémantiques
     EXTRACT_SEMANTIC_FUNCTION_NAME: ClassVar[str] = "extract_from_name_semantic"
     VALIDATE_SEMANTIC_FUNCTION_NAME: ClassVar[str] = "validate_extract_semantic"
     NATIVE_PLUGIN_NAME: ClassVar[str] = "ExtractNativePlugin"
 
-    find_similar_text_func: Optional[Callable] = None
-    extract_text_func: Optional[Callable] = None
-    _native_extract_plugin: Optional[ExtractAgentPlugin] = None
-
     def __init__(
         self,
         kernel: sk.Kernel,
@@ -145,97 +74,64 @@ class ExtractAgent(BaseAgent):
         find_similar_text_func: Optional[Callable] = None,
         extract_text_func: Optional[Callable] = None
     ):
-        """
-        Initialise l'agent d'extraction.
-
-        :param kernel: Le kernel Semantic Kernel à utiliser par l'agent.
-        :type kernel: sk.Kernel
-        :param agent_name: Nom de cet agent. Par défaut "ExtractAgent".
-        :type agent_name: str
-        :param find_similar_text_func: Fonction optionnelle pour trouver du texte similaire.
-                                       Si None, utilise `find_similar_text` de `ui.utils`.
-        :type find_similar_text_func: Optional[Callable]
-        :param extract_text_func: Fonction optionnelle pour extraire du texte avec des marqueurs.
-                                  Si None, utilise `extract_text_with_markers` de `ui.utils`.
-        :type extract_text_func: Optional[Callable]
-        """
         super().__init__(kernel, agent_name, EXTRACT_AGENT_INSTRUCTIONS)
-        
-        # Fonctions helper spécifiques à cet agent
+        self.plugin_name = "extract_plugin"
         self.find_similar_text_func = find_similar_text_func or find_similar_text
         self.extract_text_func = extract_text_func or extract_text_with_markers
 
     def get_agent_capabilities(self) -> Dict[str, Any]:
-        """Décrit les capacités principales de l'agent d'extraction.
-
-        :return: Un dictionnaire mappant les noms des capacités à leurs descriptions.
-        :rtype: Dict[str, Any]
-        """
         return {
             "extract_from_name": "Extrait un passage pertinent à partir de la dénomination de l'extrait.",
             "repair_extract": "Répare un extrait existant en utilisant sa dénomination.",
-            "update_extract_markers": "Met à jour les marqueurs d'un extrait avec les résultats d'une extraction.",
-            "add_new_extract": "Ajoute un nouvel extrait à une source existante.",
-            # Les fonctions du plugin natif pourraient aussi être listées ici si elles sont exposées publiquement
-            "find_similar_markers_native": "Trouve des marqueurs similaires dans le texte source (fonction native).",
-            "search_text_dichotomically_native": "Recherche un terme dans le texte en utilisant une approche dichotomique (fonction native).",
-            "extract_blocks_native": "Extrait des blocs de texte avec chevauchement pour l'analyse (fonction native)."
         }
 
     def setup_agent_components(self, llm_service_id: str) -> None:
-        """Configure les composants spécifiques de l'agent d'extraction dans le kernel Semantic Kernel.
-
-        Enregistre le plugin natif `ExtractAgentPlugin` et les fonctions sémantiques
-        pour l'extraction et la validation des extraits.
-
-        :param llm_service_id: L'ID du service LLM à utiliser pour les fonctions sémantiques.
-        :type llm_service_id: str
-        :return: None
-        :rtype: None
-        """
         super().setup_agent_components(llm_service_id)
         self.logger.info(f"Configuration des composants pour {self.name} avec le service LLM ID: {llm_service_id}")
-
-        # 1. Initialiser et enregistrer le plugin natif
         self._native_extract_plugin = ExtractAgentPlugin()
-        self.kernel.add_plugin(self._native_extract_plugin, plugin_name=self.NATIVE_PLUGIN_NAME)
+        self.sk_kernel.add_plugin(self._native_extract_plugin, plugin_name=self.NATIVE_PLUGIN_NAME)
         self.logger.info(f"Plugin natif '{self.NATIVE_PLUGIN_NAME}' enregistré.")
-
-        # 2. Enregistrer les fonctions sémantiques
-        # Fonction sémantique pour l'extraction
-        # Nouvelle méthode, plus directe, pour enregistrer les fonctions sémantiques.
-        # Il semble que la substitution de variables ne fonctionnait pas
-        # correctement avec PromptTemplateConfig dans ce contexte.
-
-        # Fonction sémantique pour l'extraction
-        self.kernel.add_function(
-            prompt=EXTRACT_FROM_NAME_PROMPT,
+        extract_prompt_template_config = PromptTemplateConfig(
+            template=EXTRACT_FROM_NAME_PROMPT,
+            name=self.EXTRACT_SEMANTIC_FUNCTION_NAME,
+            description="Propose des marqueurs de début et de fin pour un extrait basé sur son nom.",
+            input_variables=[
+                {"name": "extract_name", "description": "Nom de l'extrait à trouver", "is_required": True},
+                {"name": "source_name", "description": "Nom de la source", "is_required": True},
+                {"name": "extract_context", "description": "Texte source dans lequel chercher", "is_required": True}
+            ],
+            execution_settings=self.sk_kernel.get_prompt_execution_settings_from_service_id(llm_service_id)
+        )
+        self.sk_kernel.add_function(
             function_name=self.EXTRACT_SEMANTIC_FUNCTION_NAME,
-            plugin_name=self.name,
-            description="Propose des bornes (marqueurs de début et de fin) pour un extrait.",
-            execution_settings=self.kernel.get_prompt_execution_settings_from_service_id(llm_service_id)
+            prompt_template_config=extract_prompt_template_config,
+            plugin_name=self.name
         )
         self.logger.info(f"Fonction sémantique '{self.EXTRACT_SEMANTIC_FUNCTION_NAME}' enregistrée dans le plugin '{self.name}'.")
-
-        # Fonction sémantique pour la validation
-        self.kernel.add_function(
-            prompt=VALIDATE_EXTRACT_PROMPT,
+        validate_prompt_template_config = PromptTemplateConfig(
+            template=VALIDATE_EXTRACT_PROMPT,
+            name=self.VALIDATE_SEMANTIC_FUNCTION_NAME,
+            description="Valide si un extrait de texte est pertinent par rapport à sa dénomination.",
+            input_variables=[
+                {"name": "extract_name", "description": "Nom de l'extrait", "is_required": True},
+                {"name": "source_name", "description": "Nom de la source", "is_required": True},
+                {"name": "start_marker", "description": "Marqueur de début", "is_required": True},
+                {"name": "end_marker", "description": "Marqueur de fin", "is_required": True},
+                {"name": "template_start", "description": "Modèle de début (optionnel)", "is_required": False},
+                {"name": "extracted_text", "description": "Texte extrait", "is_required": True},
+                {"name": "explanation", "description": "Explication de l'extraction LLM", "is_required": True}
+            ],
+            execution_settings=self.sk_kernel.get_prompt_execution_settings_from_service_id(llm_service_id)
+        )
+        self.sk_kernel.add_function(
             function_name=self.VALIDATE_SEMANTIC_FUNCTION_NAME,
-            plugin_name=self.name,
-            description="Valide un extrait proposé.",
-            execution_settings=self.kernel.get_prompt_execution_settings_from_service_id(llm_service_id)
+            prompt_template_config=validate_prompt_template_config,
+            plugin_name=self.name
         )
         self.logger.info(f"Fonction sémantique '{self.VALIDATE_SEMANTIC_FUNCTION_NAME}' enregistrée dans le plugin '{self.name}'.")
 
     @property
     def native_extract_plugin(self) -> ExtractAgentPlugin:
-        """Retourne l'instance du plugin natif d'extraction.
-
-        :return: L'instance de `ExtractAgentPlugin`.
-        :rtype: ExtractAgentPlugin
-        :raises RuntimeError: Si le plugin natif n'a pas été initialisé (c'est-à-dire
-                              si `setup_agent_components` n'a pas été appelé).
-        """
         if self._native_extract_plugin is None:
             raise RuntimeError("Plugin natif d'extraction non initialisé. Appelez setup_agent_components.")
         return self._native_extract_plugin
@@ -243,516 +139,98 @@ class ExtractAgent(BaseAgent):
     async def extract_from_name(
         self,
         source_info: Dict[str, Any],
-        extract_name: str
+        extract_name: str,
+        source_text: Optional[str] = None
     ) -> ExtractResult:
-        """
-        Propose et valide des marqueurs pour un extrait basé sur son nom et le texte source.
-
-        Utilise une fonction sémantique pour proposer des marqueurs (`start_marker`, `end_marker`,
-        `template_start`) et une autre pour valider la pertinence du texte extrait.
-        Gère les textes volumineux en utilisant une approche dichotomique pour le contexte.
-
-        :param source_info: Dictionnaire contenant les informations de la source
-                            (nom, chemin, etc.).
-        :type source_info: Dict[str, Any]
-        :param extract_name: Le nom ou la description de l'extrait à trouver.
-        :type extract_name: str
-        :return: Un objet `ExtractResult` contenant les marqueurs proposés, le texte extrait,
-                 le statut de l'extraction ("valid", "rejected", "error"), un message
-                 et une explication.
-        :rtype: ExtractResult
-        """
         source_name = source_info.get("source_name", "Source inconnue")
+        self.logger.info(f"Extraction à partir du nom '{extract_name}' dans la source '{source_name}'...")
         
-        self.logger.info(f"Extraction à partir du nom '{extract_name}' dans la source '{source_info.get('source_name', 'Source inconnue')}'...")
-        
-        source_name = source_info.get("source_name", "Source inconnue")
-        source_text, url = load_source_text(source_info)
-        if not source_text:
-            self.logger.error(f"Impossible de charger le texte source pour '{source_name}': {url}")
-            return ExtractResult(
-                source_name=source_name,
-                extract_name=extract_name,
-                status="error",
-                message=f"Impossible de charger le texte source: {url}"
-            )
-        
-        is_large_text = len(source_text) > 10000
-        extract_context_for_llm = ""
-
-        if is_large_text:
-            self.logger.info(f"Texte volumineux détecté. Utilisation d'une approche dichotomique...")
-            blocks = self.native_extract_plugin.extract_blocks(source_text, block_size=500, overlap=50)
-            keywords = extract_name.lower().split()
-            relevant_blocks_data = []
-            
-            for keyword in keywords:
-                if len(keyword) > 3:
-                    search_results = self.native_extract_plugin.search_text_dichotomically(source_text, keyword)
-                    for result in search_results:
-                        block_idx = result["block_start"] // 450
-                        if block_idx < len(blocks):
-                            relevant_blocks_data.append(blocks[block_idx])
-            
-            extract_context_for_llm = f"TEXTE SOURCE VOLUMINEUX ({len(source_text)} caractères).\n\n"
-            extract_context_for_llm += f"Dénomination de l'extrait: {extract_name}\n\n"
-            
-            if relevant_blocks_data:
-                extract_context_for_llm += "BLOCS PERTINENTS TROUVÉS:\n\n"
-                for i, block_data in enumerate(relevant_blocks_data[:5]):
-                    extract_context_for_llm += f"--- BLOC {i+1} (positions {block_data['start_pos']}-{block_data['end_pos']}) ---\n"
-                    extract_context_for_llm += block_data["block"] + "\n\n"
-            else:
-                extract_context_for_llm += f"Début du texte:\n{source_text[:2500]}...\n\n"
-                extract_context_for_llm += f"Fin du texte:\n...{source_text[-2500:]}"
-        else:
-            extract_context_for_llm = source_text
+        if source_text is None:
+            self.logger.debug("Aucun texte source direct fourni. Tentative de chargement depuis source_info.")
+            source_text, url = load_source_text(source_info)
+            if not source_text:
+                return ExtractResult(source_name=source_name, extract_name=extract_name, status="error", message=f"Impossible de charger le texte source: {url}")
         
         arguments = KernelArguments(
             extract_name=extract_name,
             source_name=source_name,
-            extract_context=extract_context_for_llm
+            extract_context=source_text # On passe le texte complet
         )
-        
-        extract_content_result = ""
         try:
-            # Utilisation de self.kernel.invoke pour appeler la fonction sémantique
-            response = await self.kernel.invoke(
+            response = await self.sk_kernel.invoke(
                 plugin_name=self.name,
                 function_name=self.EXTRACT_SEMANTIC_FUNCTION_NAME,
                 arguments=arguments
             )
-            extract_content_result = str(response) # La réponse est un KernelContent ou similaire
+            extract_content_result = str(response)
         except Exception as e:
-            self.logger.error(f"Erreur lors de l'invocation de la fonction sémantique '{self.EXTRACT_SEMANTIC_FUNCTION_NAME}': {e}")
-            return ExtractResult(
-                source_name=source_name,
-                extract_name=extract_name,
-                status="error",
-                message=f"Erreur LLM lors de la proposition de bornes: {str(e)}"
-            )
+            return ExtractResult(source_name=source_name, extract_name=extract_name, status="error", message=f"Erreur LLM lors de la proposition de bornes: {str(e)}")
         
-        # Extraire la réponse JSON de l'agent
-        extract_content = extract_content_result
         try:
-            json_match = re.search(r'```json\s*([\s\S]*?)\s*```', extract_content)
-            if json_match:
-                extract_json = json.loads(json_match.group(1))
-            else:
-                extract_json = json.loads(extract_content)
+            json_match = re.search(r'```json\s*([\s\S]*?)\s*```', extract_content_result)
+            extract_json = json.loads(json_match.group(1) if json_match else extract_content_result)
         except json.JSONDecodeError:
-            self.logger.warning(f"Réponse non JSON de l'extraction pour '{extract_name}': {extract_content}")
-            start_match = re.search(r'"start_marker"\s*:\s*"([^"]*)"', extract_content)
-            end_match = re.search(r'"end_marker"\s*:\s*"([^"]*)"', extract_content)
-            template_match = re.search(r'"template_start"\s*:\s*"([^"]*)"', extract_content)
-            explanation_match = re.search(r'"explanation"\s*:\s*"([^"]*)"', extract_content)
-            extract_json = {
-                "start_marker": start_match.group(1) if start_match else "",
-                "end_marker": end_match.group(1) if end_match else "",
-                "template_start": template_match.group(1) if template_match else "",
-                "explanation": explanation_match.group(1) if explanation_match else "Explication non disponible due à une réponse malformée."
-            }
+            self.logger.warning(f"Réponse non JSON de l'extraction pour '{extract_name}': {extract_content_result}")
+            return ExtractResult(source_name=source_name, extract_name=extract_name, status="error", message="Réponse non-JSON de l'agent LLM.", explanation=extract_content_result)
 
         start_marker = extract_json.get("start_marker", "")
         end_marker = extract_json.get("end_marker", "")
-        template_start = extract_json.get("template_start", "")
-        explanation = extract_json.get("explanation", "Aucune explication fournie par l'agent d'extraction.")
+        explanation = extract_json.get("explanation", "Aucune explication fournie.")
 
         if not start_marker or not end_marker:
-            self.logger.warning(f"Bornes invalides ou manquantes proposées pour l'extrait '{extract_name}'. Start: '{start_marker}', End: '{end_marker}'")
-            return ExtractResult(
-                source_name=source_name,
-                extract_name=extract_name,
-                status="error",
-                message="Bornes invalides ou manquantes proposées par l'agent.",
-                explanation=explanation
-            )
-
-        extracted_text, status_msg, start_found, end_found = self.extract_text_func(
-            source_text, start_marker, end_marker, template_start
-        )
+            return ExtractResult(source_name=source_name, extract_name=extract_name, status="error", message="Bornes invalides ou manquantes proposées par l'agent.", explanation=explanation)
 
+        extracted_text, status_msg, start_found, end_found = self.extract_text_func(source_text, start_marker, end_marker, "")
         if not start_found or not end_found:
-            self.logger.warning(f"Bornes non trouvées dans le texte pour l'extrait '{extract_name}'. Status: {status_msg}")
-            return ExtractResult(
-                source_name=source_name,
-                extract_name=extract_name,
-                status="error",
-                message=f"Bornes non trouvées dans le texte: {status_msg}",
-                start_marker=start_marker,
-                end_marker=end_marker,
-                template_start=template_start,
-                explanation=explanation
-            )
+            return ExtractResult(source_name=source_name, extract_name=extract_name, status="error", message=f"Bornes non trouvées dans le texte: {status_msg}", start_marker=start_marker, end_marker=end_marker, explanation=explanation)
         
-        # Appel de la fonction sémantique de validation
-        validation_args = KernelArguments(
-            extract_name=extract_name,
+        return ExtractResult(
             source_name=source_name,
+            extract_name=extract_name,
+            status="valid",
+            message="Extraction réussie",
             start_marker=start_marker,
             end_marker=end_marker,
-            template_start=template_start,
-            extracted_text=extracted_text if extracted_text else "Aucun texte extrait",
-            explanation=explanation
+            explanation=explanation,
+            extracted_text=extracted_text
         )
-        
-        validation_content_result = ""
-        try:
-            response = await self.kernel.invoke(
-                plugin_name=self.name,
-                function_name=self.VALIDATE_SEMANTIC_FUNCTION_NAME,
-                arguments=validation_args
-            )
-            validation_content_result = str(response)
-        except Exception as e:
-            self.logger.error(f"Erreur lors de l'invocation de la fonction sémantique '{self.VALIDATE_SEMANTIC_FUNCTION_NAME}': {e}")
-            return ExtractResult(
-                source_name=source_name,
-                extract_name=extract_name,
-                status="error",
-                message=f"Erreur LLM lors de la validation: {str(e)}",
-                start_marker=start_marker,
-                end_marker=end_marker,
-                template_start=template_start,
-                explanation=explanation
-            )
-            
-        validation_content = validation_content_result
-        try:
-            json_match = re.search(r'```json\s*([\s\S]*?)\s*```', validation_content)
-            if json_match:
-                validation_json = json.loads(json_match.group(1))
-            else:
-                validation_json = json.loads(validation_content)
-        except json.JSONDecodeError:
-            self.logger.warning(f"Réponse non JSON de la validation pour '{extract_name}': {validation_content}")
-            valid_match = re.search(r'"valid"\s*:\s*(true|false)', validation_content, re.IGNORECASE)
-            reason_match = re.search(r'"reason"\s*:\s*"([^"]*)"', validation_content)
-            validation_json = {
-                "valid": valid_match.group(1).lower() == "true" if valid_match else False,
-                "reason": reason_match.group(1) if reason_match else "Raison non disponible due à une réponse malformée."
-            }
 
-        is_valid = validation_json.get("valid", False)
-        validation_reason = validation_json.get("reason", "Aucune raison de validation fournie.")
+    async def invoke_custom(self, history: list[ChatMessageContent]) -> ChatMessageContent:
+        self.logger.info(f"invoke_custom appelée pour {self.name}")
 
-        if is_valid:
-            self.logger.info(f"Extrait validé pour '{extract_name}'. Raison: {validation_reason}")
-            return ExtractResult(
-                source_name=source_name,
-                extract_name=extract_name,
-                status="valid",
-                message=f"Extrait validé: {validation_reason}",
-                start_marker=start_marker,
-                end_marker=end_marker,
-                template_start=template_start,
-                explanation=explanation,
-                extracted_text=extracted_text
-            )
-        else:
-            self.logger.warning(f"Extrait rejeté pour '{extract_name}'. Raison: {validation_reason}")
-            return ExtractResult(
-                source_name=source_name,
-                extract_name=extract_name,
-                status="rejected",
-                message=f"Extrait rejeté: {validation_reason}",
-                start_marker=start_marker,
-                end_marker=end_marker,
-                template_start=template_start,
-                explanation=explanation,
-                extracted_text=extracted_text # Inclure le texte même si rejeté pour analyse
-            )
-
-    async def repair_extract(
-        self,
-        extract_definitions: List[Dict[str, Any]],
-        source_idx: int,
-        extract_idx: int
-    ) -> ExtractResult:
-        """
-        Tente de réparer les marqueurs d'un extrait existant si ceux-ci sont invalides.
-
-        Si les marqueurs actuels de l'extrait ne permettent pas une extraction valide,
-        cette méthode appelle `extract_from_name` pour tenter de trouver de meilleurs
-        marqueurs basés sur le nom de l'extrait.
+        source_text = ""
+        if history:
+            user_message = next((msg for msg in history if msg.role == "user"), None)
+            if user_message and user_message.content:
+                source_text = str(user_message.content)
+            else:
+                first_message = history[0]
+                if first_message and first_message.content:
+                    source_text = str(first_message.content)
 
-        :param extract_definitions: La liste complète des définitions d'extraits.
-        :type extract_definitions: List[Dict[str, Any]]
-        :param source_idx: L'index de la source contenant l'extrait à réparer.
-        :type source_idx: int
-        :param extract_idx: L'index de l'extrait à réparer au sein de la source.
-        :type extract_idx: int
-        :return: Un objet `ExtractResult` indiquant le résultat de la tentative de réparation.
-                 Si l'extrait original était déjà valide, le statut sera "valid" avec un message.
-                 Si la réparation réussit, le résultat de la nouvelle extraction est retourné.
-                 Si la réparation échoue, le résultat de l'échec de la nouvelle extraction est retourné.
-        :rtype: ExtractResult
-        """
-        source_info = extract_definitions[source_idx]
-        source_name = source_info.get("source_name", f"Source #{source_idx}")
-        
-        extracts = source_info.get("extracts", [])
-        extract_info = extracts[extract_idx]
-        extract_name = extract_info.get("extract_name", f"Extrait #{extract_idx}")
-        
-        start_marker = extract_info.get("start_marker", "")
-        end_marker = extract_info.get("end_marker", "")
-        template_start = extract_info.get("template_start", "")
-        
-        self.logger.info(f"Réparation de l'extrait '{extract_name}' de la source '{source_name}'...")
-        
-        source_text, url = load_source_text(source_info)
         if not source_text:
-            self.logger.error(f"Impossible de charger le texte source pour '{source_name}': {url}")
-            return ExtractResult(
-                source_name=source_name,
-                extract_name=extract_name,
-                status="error",
-                message=f"Impossible de charger le texte source: {url}"
-            )
-        
-        extracted_text, status_msg, start_found, end_found = self.extract_text_func(
-            source_text, start_marker, end_marker, template_start
-        )
-        
-        if start_found and end_found:
-            self.logger.info(f"Extrait '{extract_name}' déjà valide. Aucune correction nécessaire.")
-            return ExtractResult(
-                source_name=source_name,
-                extract_name=extract_name,
-                status="valid",
-                message="Extrait déjà valide. Aucune correction nécessaire.",
-                start_marker=start_marker,
-                end_marker=end_marker,
-                template_start=template_start,
-                extracted_text=extracted_text
-            )
-        
-        self.logger.info(f"Extrait '{extract_name}' invalide (start_found: {start_found}, end_found: {end_found}). Tentative d'extraction à partir du nom...")
-        
-        # Réutiliser la logique d'extraction principale
-        new_extract_result = await self.extract_from_name(source_info, extract_name)
-        
-        if new_extract_result.status == "valid":
-            self.logger.info(f"Réparation réussie pour l'extrait '{extract_name}' via nouvelle extraction.")
-            return new_extract_result
-        else:
-            self.logger.warning(f"Échec de la réparation pour l'extrait '{extract_name}'. L'extraction à partir du nom a échoué avec le statut: {new_extract_result.status}")
-            # Retourner le résultat de la tentative de réparation, même si c'est une erreur, pour informer l'appelant
-            return new_extract_result
-    
-    async def update_extract_markers(
-        self,
-        extract_definitions: List[Dict[str, Any]],
-        source_idx: int,
-        extract_idx: int,
-        result: ExtractResult
-    ) -> bool:
-        """
-        Met à jour les marqueurs d'un extrait spécifique dans la liste des définitions
-        avec les marqueurs d'un `ExtractResult` validé.
-
-        Enregistre également l'action de mise à jour dans le plugin natif si disponible.
-
-        :param extract_definitions: La liste complète des définitions d'extraits (sera modifiée en place).
-        :type extract_definitions: List[Dict[str, Any]]
-        :param source_idx: L'index de la source de l'extrait.
-        :type source_idx: int
-        :param extract_idx: L'index de l'extrait à mettre à jour.
-        :type extract_idx: int
-        :param result: L'objet `ExtractResult` contenant les nouveaux marqueurs validés.
-        :type result: ExtractResult
-        :return: True si la mise à jour a été effectuée, False si le résultat n'était pas valide
-                 ou si les index étaient invalides.
-        :rtype: bool
-        """
-        if result.status != "valid":
-            self.logger.warning(f"Impossible de mettre à jour les marqueurs avec un résultat non valide: {result.status}")
-            return False
-        
-        if not (0 <= source_idx < len(extract_definitions)):
-            self.logger.error(f"Index de source invalide: {source_idx}")
-            return False
-
-        source_info = extract_definitions[source_idx]
-        extracts = source_info.get("extracts", [])
-
-        if not (0 <= extract_idx < len(extracts)):
-            self.logger.error(f"Index d'extrait invalide: {extract_idx} pour la source '{source_info.get('source_name', '')}'")
-            return False
-            
-        current_extract = extracts[extract_idx]
-        old_start = current_extract.get("start_marker", "")
-        old_end = current_extract.get("end_marker", "")
-        old_template = current_extract.get("template_start", "")
-        
-        current_extract["start_marker"] = result.start_marker
-        current_extract["end_marker"] = result.end_marker
-        if result.template_start:
-            current_extract["template_start"] = result.template_start
-        elif "template_start" in current_extract and not result.template_start:
-            del current_extract["template_start"] # Supprimer si vide et existait
-        
-        # Enregistrer les modifications dans le plugin natif (si nécessaire pour le suivi)
-        # Note: self.extract_plugin n'existe plus, utiliser self.native_extract_plugin
-        if self.native_extract_plugin:
-            self.native_extract_plugin.extract_results.append({
-                "source_name": source_info.get("source_name", f"Source #{source_idx}"),
-                "extract_name": current_extract.get("extract_name", f"Extrait #{extract_idx}"),
-                "action": "updated_markers",
-                "old_start_marker": old_start,
-                "new_start_marker": result.start_marker,
-                "old_end_marker": old_end,
-                "new_end_marker": result.end_marker,
-                "old_template_start": old_template,
-                "new_template_start": result.template_start or ""
-            })
-        
-        self.logger.info(f"Marqueurs mis à jour pour l'extrait '{current_extract.get('extract_name', '')}' dans '{source_info.get('source_name', '')}'.")
-        return True
+            return ChatMessageContent(role="assistant", content="ERREUR: Impossible de trouver le texte source dans l'historique.", name=self.name)
 
-    async def add_new_extract(
-        self,
-        extract_definitions: List[Dict[str, Any]],
-        source_idx: int,
-        extract_name: str,
-        result: ExtractResult
-    ) -> Tuple[bool, int]:
-        """
-        Ajoute un nouvel extrait à une source existante dans la liste des définitions.
-
-        Utilise les marqueurs d'un `ExtractResult` validé pour créer le nouvel extrait.
-        Enregistre également l'action d'ajout dans le plugin natif si disponible.
+        extract_name = "Le point principal de l'auteur"
+        source_info = {"source_name": "Source Conversationnelle"}
 
-        :param extract_definitions: La liste complète des définitions d'extraits (sera modifiée en place).
-        :type extract_definitions: List[Dict[str, Any]]
-        :param source_idx: L'index de la source à laquelle ajouter l'extrait.
-        :type source_idx: int
-        :param extract_name: Le nom à donner au nouvel extrait.
-        :type extract_name: str
-        :param result: L'objet `ExtractResult` contenant les marqueurs validés pour le nouvel extrait.
-        :type result: ExtractResult
-        :return: Un tuple contenant un booléen indiquant le succès de l'ajout et l'index
-                 du nouvel extrait (-1 en cas d'échec).
-        :rtype: Tuple[bool, int]
-        """
-        if result.status != "valid":
-            self.logger.warning(f"Impossible d'ajouter un extrait avec un résultat non valide: {result.status}")
-            return False, -1
-        
-        if not (0 <= source_idx < len(extract_definitions)):
-            self.logger.error(f"Index de source invalide: {source_idx}")
-            return False, -1
-
-        source_info = extract_definitions[source_idx]
-        extracts = source_info.get("extracts", [])
-            
-        new_extract_data = {
-            "extract_name": extract_name,
-            "start_marker": result.start_marker,
-            "end_marker": result.end_marker
-        }
-        if result.template_start:
-            new_extract_data["template_start"] = result.template_start
-            
-        extracts.append(new_extract_data)
-        new_extract_idx = len(extracts) - 1
-        source_info["extracts"] = extracts # Mettre à jour la liste dans le dictionnaire source_info
-            
-        # Enregistrer l'ajout dans le plugin natif (si nécessaire pour le suivi)
-        if self.native_extract_plugin:
-            self.native_extract_plugin.extract_results.append({
-                "source_name": source_info.get("source_name", f"Source #{source_idx}"),
-                "extract_name": extract_name,
-                "action": "added_extract",
-                "start_marker": result.start_marker,
-                "end_marker": result.end_marker,
-                "template_start": result.template_start or ""
-            })
-        
-        self.logger.info(f"Nouvel extrait '{extract_name}' ajouté à '{source_info.get('source_name', '')}' à l'index {new_extract_idx}.")
-        return True, new_extract_idx
-
-    async def invoke_single(self, action: str = "extract_from_name", **kwargs) -> Any:
-        """
-        Méthode d'invocation principale pour l'agent d'extraction.
-
-        :param action: L'action à effectuer (par exemple, 'extract_from_name').
-        :type action: str
-        :param kwargs: Arguments variables pour l'action.
-                       Par exemple, `source_info` et `extract_name`.
-        :type kwargs: Any
-        :return: Le résultat de l'action, généralement un objet `ExtractResult`.
-        :rtype: Any
-        :raises ValueError: Si une action inconnue est demandée.
-        """
-        self.logger.info(f"invoke_single appelée avec l'action: {action}")
-        
-        if action == "extract_from_name":
-            source_info = kwargs.get("source_info")
-            extract_name = kwargs.get("extract_name")
-            if not source_info or not extract_name:
-                raise ValueError("Les arguments 'source_info' et 'extract_name' sont requis pour l'action 'extract_from_name'.")
-            return await self.extract_from_name(source_info, extract_name)
-        
-        elif action == "repair_extract":
-            extract_definitions = kwargs.get("extract_definitions")
-            source_idx = kwargs.get("source_idx")
-            extract_idx = kwargs.get("extract_idx")
-            if extract_definitions is None or source_idx is None or extract_idx is None:
-                raise ValueError("Les arguments 'extract_definitions', 'source_idx' et 'extract_idx' sont requis pour 'repair_extract'.")
-            return await self.repair_extract(extract_definitions, source_idx, extract_idx)
-
-        elif action == "update_extract_markers":
-            extract_definitions = kwargs.get("extract_definitions")
-            source_idx = kwargs.get("source_idx")
-            extract_idx = kwargs.get("extract_idx")
-            result = kwargs.get("result")
-            if extract_definitions is None or source_idx is None or extract_idx is None or result is None:
-                raise ValueError("Arguments manquants pour 'update_extract_markers'.")
-            return await self.update_extract_markers(extract_definitions, source_idx, extract_idx, result)
-
-        elif action == "add_new_extract":
-            extract_definitions = kwargs.get("extract_definitions")
-            source_idx = kwargs.get("source_idx")
-            extract_name = kwargs.get("extract_name")
-            result = kwargs.get("result")
-            if extract_definitions is None or source_idx is None or extract_name is None or result is None:
-                raise ValueError("Arguments manquants pour 'add_new_extract'.")
-            return await self.add_new_extract(extract_definitions, source_idx, extract_name, result)
-        
-        # Action par défaut ou
-        else:
-            self.logger.warning(f"Action '{action}' non reconnue dans invoke_single. Retourne les capacités de l'agent.")
-            capabilities = self.get_agent_capabilities()
-            return f"ExtractAgent '{self.name}' prêt. Action non reconnue. Capacités: {', '.join(capabilities.keys())}"
-
-
-# La fonction setup_extract_agent n'est plus nécessaire ici,
-# car l'initialisation du kernel et du service LLM se fait à l'extérieur
-# et l'agent est initialisé avec un kernel existant.
-# Les fonctions sémantiques sont maintenant configurées dans setup_agent_components.
-
-# Exemple de méthodes natives qui pourraient être appelées directement ou via le kernel si enregistrées
-# Ces méthodes sont maintenant dans ExtractAgentPlugin et accessibles via self.native_extract_plugin
-
-# async def find_similar_markers_wrapper(self, text: str, marker: str, max_results: int = 5):
-#     # Wrapper si on veut l'exposer comme une capacité de l'agent directement
-#     if not self.native_extract_plugin:
-#         self.logger.error("Plugin natif non initialisé pour find_similar_markers_wrapper.")
-#         return []
-#     return self.native_extract_plugin.find_similar_markers(
-#         text, marker, max_results, self.find_similar_text_func
-#     )
-
-# async def search_text_dichotomically_wrapper(self, text: str, search_term: str, block_size: int = 500, overlap: int = 50):
-#     if not self.native_extract_plugin:
-#         self.logger.error("Plugin natif non initialisé pour search_text_dichotomically_wrapper.")
-#         return []
-#     return self.native_extract_plugin.search_text_dichotomically(text, search_term, block_size, overlap)
-
-# async def extract_blocks_wrapper(self, text: str, block_size: int = 500, overlap: int = 50):
-#     if not self.native_extract_plugin:
-#         self.logger.error("Plugin natif non initialisé pour extract_blocks_wrapper.")
-#         return []
-#     return self.native_extract_plugin.extract_blocks(text, block_size, overlap)
\ No newline at end of file
+        try:
+            result = await self.extract_from_name(source_info, extract_name, source_text=source_text)
+            response_content = result.to_json()
+            return ChatMessageContent(role="assistant", content=response_content, name=self.name)
+        except Exception as e:
+            self.logger.error(f"Erreur dans invoke_custom de ExtractAgent: {e}", exc_info=True)
+            return ChatMessageContent(role="assistant", content=f"Erreur lors de l'extraction: {e}", name=self.name)
+
+    async def get_response(self, request: str, context: str = "", **kwargs) -> str:
+        self.logger.warning("La méthode 'get_response' est dépréciée. Redirection vers 'invoke_custom'.")
+        history = [ChatMessageContent(role="user", content=request)]
+        response_message = await self.invoke_custom(history)
+        return str(response_message.content)
+
+    async def invoke(self, function_name: str = "extract_from_name", **kwargs) -> str:
+        self.logger.warning(f"La méthode 'invoke' est dépréciée. Redirection vers 'invoke_custom' pour la fonction '{function_name}'.")
+        raw_text = kwargs.get("source_text", "")
+        history = [ChatMessageContent(role="user", content=raw_text)]
+        response_message = await self.invoke_custom(history)
+        return str(response_message.content)
\ No newline at end of file
diff --git a/argumentation_analysis/agents/core/extract/extract_definitions.py b/argumentation_analysis/agents/core/extract/extract_definitions.py
index 66c02aa6..af44925c 100644
--- a/argumentation_analysis/agents/core/extract/extract_definitions.py
+++ b/argumentation_analysis/agents/core/extract/extract_definitions.py
@@ -123,6 +123,14 @@ class ExtractResult: # De la version HEAD (Updated upstream)
             "extracted_text": self.extracted_text
         }
     
+    def to_json(self) -> str:
+        """Convertit l'instance `ExtractResult` en une chaîne JSON.
+
+        :return: Une chaîne JSON représentant l'objet.
+        :rtype: str
+        """
+        return json.dumps(self.to_dict(), indent=2)
+
     @classmethod
     def from_dict(cls, data: Dict[str, Any]) -> 'ExtractResult':
         """Crée une instance de `ExtractResult` à partir d'un dictionnaire.
diff --git a/argumentation_analysis/agents/core/informal/informal_agent.py b/argumentation_analysis/agents/core/informal/informal_agent.py
index f85bcbf9..c3efb982 100644
--- a/argumentation_analysis/agents/core/informal/informal_agent.py
+++ b/argumentation_analysis/agents/core/informal/informal_agent.py
@@ -22,10 +22,11 @@ L'agent est conçu pour :
 
 import logging
 import json
-from typing import Dict, List, Any, Optional, AsyncGenerator
+from typing import Dict, List, Any, Optional
 import semantic_kernel as sk
 from semantic_kernel.functions.kernel_arguments import KernelArguments
-from semantic_kernel.contents import ChatMessageContent, AuthorRole
+from semantic_kernel.contents.chat_message_content import ChatMessageContent
+from semantic_kernel.contents.role import Role
 
 # Import de la classe de base
 from ..abc.agent_bases import BaseAgent
@@ -732,31 +733,46 @@ class InformalAnalysisAgent(BaseAgent):
                 "analysis_timestamp": self._get_timestamp()
             }
 
-    async def invoke_single(
-        self,
-        messages: List[ChatMessageContent],
-        **kwargs: Any,
-    ) -> ChatMessageContent:
+    async def invoke_custom(self, history: list[ChatMessageContent]) -> ChatMessageContent:
         """
-        Implémentation de la logique de l'agent pour une seule réponse, conforme à BaseAgent.
+        Logique d'invocation principale de l'agent, utilisant l'historique de chat.
         """
-        self.logger.info(f"Informal Agent invoke_single called with: {len(messages)} messages.")
-        
-        # Le dernier message de l'utilisateur est généralement celui qu'on traite.
-        user_message = next((m.content for m in reversed(messages) if m.role == AuthorRole.USER), None)
+        self.logger.info(f"invoke_custom called for {self.name}")
 
-        if not user_message:
-            self.logger.warning("Aucun message utilisateur trouvé dans l'historique pour l'analyse informelle.")
-            error_content = json.dumps({"error": "No user message to analyze."})
-            return ChatMessageContent(role=AuthorRole.ASSISTANT, content=error_content)
+        # Extraire le contenu du dernier message utilisateur
+        # ou de la dernière réponse d'un autre agent comme entrée principale.
+        input_text = next((m.content for m in reversed(history) if m.role in [Role.USER, Role.ASSISTANT] and m.content), None)
 
-        self.logger.info(f"Déclenchement de 'perform_complete_analysis' sur le texte: '{user_message[:100]}...'")
-        analysis_result = await self.perform_complete_analysis(user_message)
-        
-        # Encodage du résultat en JSON pour la réponse
-        response_content = json.dumps(analysis_result, indent=2, ensure_ascii=False)
+        if not isinstance(input_text, str) or not input_text.strip():
+            self.logger.warning("Aucun contenu textuel valide trouvé dans l'historique récent pour l'analyse.")
+            error_msg = {"error": "No valid text content found in recent history to analyze."}
+            return ChatMessageContent(role=Role.ASSISTANT, content=json.dumps(error_msg), name=self.name)
+
+        self.logger.info(f"Déclenchement de l'analyse et catégorisation pour le texte : '{input_text[:100]}...'")
         
-        return ChatMessageContent(role=AuthorRole.ASSISTANT, content=response_content)
+        try:
+            # Utiliser une des méthodes d'analyse principales de l'agent
+            analysis_result = await self.analyze_and_categorize(input_text)
+            response_content = json.dumps(analysis_result, indent=2, ensure_ascii=False)
+            
+            return ChatMessageContent(role=Role.ASSISTANT, content=response_content, name=self.name)
+
+        except Exception as e:
+            self.logger.error(f"Erreur durant 'analyze_and_categorize' dans invoke_custom: {e}", exc_info=True)
+            error_msg = {"error": f"An unexpected error occurred during analysis: {e}"}
+            return ChatMessageContent(role=Role.ASSISTANT, content=json.dumps(error_msg), name=self.name)
+
+    async def invoke(self, history: list[ChatMessageContent]) -> ChatMessageContent:
+        """Méthode dépréciée, utilisez invoke_custom."""
+        import warnings
+        warnings.warn("The 'invoke' method is deprecated, use 'invoke_custom' instead.", DeprecationWarning)
+        return await self.invoke_custom(history)
+
+    async def get_response(self, history: list[ChatMessageContent]) -> ChatMessageContent:
+        """Méthode dépréciée, utilisez invoke_custom."""
+        import warnings
+        warnings.warn("The 'get_response' method is deprecated, use 'invoke_custom' instead.", DeprecationWarning)
+        return await self.invoke_custom(history)
 
 # Log de chargement
 # logging.getLogger(__name__).debug("Module agents.core.informal.informal_agent chargé.") # Géré par BaseAgent
diff --git a/argumentation_analysis/agents/core/pm/pm_agent.py b/argumentation_analysis/agents/core/pm/pm_agent.py
index a08bf1b9..6aa67726 100644
--- a/argumentation_analysis/agents/core/pm/pm_agent.py
+++ b/argumentation_analysis/agents/core/pm/pm_agent.py
@@ -4,7 +4,8 @@ from typing import Dict, Any, Optional
 
 from semantic_kernel import Kernel # type: ignore
 from semantic_kernel.functions.kernel_arguments import KernelArguments # type: ignore
-from semantic_kernel.contents import ChatMessageContent, AuthorRole
+from semantic_kernel.contents.chat_message_content import ChatMessageContent
+from semantic_kernel.contents.role import Role
 
 
 from ..abc.agent_bases import BaseAgent
@@ -159,40 +160,56 @@ class ProjectManagerAgent(BaseAgent):
             # Retourner une chaîne d'erreur ou lever une exception spécifique
             return f"ERREUR: Impossible d'écrire la conclusion. Détails: {e}"
 
-    async def get_response(self, message: str, **kwargs) -> str:
-        """
-        Méthode générique pour obtenir une réponse, non utilisée pour les appels spécifiques.
-        """
-        self.logger.info(f"get_response non implémenté pour l'appel générique, retour des capacités.")
-        capabilities = self.get_agent_capabilities()
-        return f"Agent ProjectManager prêt. Capacités: {', '.join(capabilities.keys())}"
+    async def invoke(self, history: list[ChatMessageContent]) -> ChatMessageContent:
+        """Méthode dépréciée, utilisez invoke_custom."""
+        import warnings
+        warnings.warn("The 'invoke' method is deprecated, use 'invoke_custom' instead.", DeprecationWarning)
+        return await self.invoke_custom(history)
+
+    async def get_response(self, history: list[ChatMessageContent]) -> ChatMessageContent:
+        """Méthode dépréciée, utilisez invoke_custom."""
+        import warnings
+        warnings.warn("The 'get_response' method is deprecated, use 'invoke_custom' instead.", DeprecationWarning)
+        return await self.invoke_custom(history)
 
-    async def invoke_single(self, *args, **kwargs) -> str:
+    async def invoke_custom(self, history: list[ChatMessageContent]) -> ChatMessageContent:
         """
-        Implémentation de la logique de l'agent pour une seule réponse, appelée par la méthode `invoke` de la classe de base.
+        Logique d'invocation principale du PM, qui décide de la prochaine action.
         """
-        self.logger.info(f"PM Agent invoke_single called with: args={args}, kwargs={kwargs}")
+        self.logger.info(f"invoke_custom called for {self.name} with {len(history)} messages.")
 
-        # Le framework AgentGroupChat passe le `chat_history` comme premier argument positionnel.
-        # Nous l'extrayons pour récupérer le contexte et le texte.
-        # C'est une heuristique basée sur le fonctionnement actuel de SK.
-        raw_text = ""
-        analysis_state_snapshot = "{}" # Default empty state
+        # Extraire le texte brut initial (typiquement le premier message utilisateur)
+        raw_text = next((m.content for m in history if m.role == Role.USER and m.content), "")
         
-        if args and isinstance(args[0], list) and len(args[0]) > 0:
-            # L'historique (chat avec les messages précédents) semble être dans args[0]
-            # Le message initial de l'utilisateur est souvent le premier.
-            for msg in args[0]:
-                if msg.role.value.lower() == 'user':
-                    raw_text = msg.content
-                    break # On prend le premier
-            self.logger.info(f"Texte brut extrait de l'historique: '{raw_text[:100]}...'")
-
-        # Pour le state_snapshot, c'est plus complexe.
-        # Sans une convention claire, on va appeler define_tasks avec l'état par défaut.
-        # C'est le rôle du PM de démarrer le processus.
-        self.logger.info("Déclenchement de 'define_tasks_and_delegate' depuis l'appel invoke_single générique.")
-        return await self.define_tasks_and_delegate(analysis_state_snapshot, raw_text)
+        # Extraire l'état d'analyse (typiquement un résumé JSON dans un message d'assistant récent)
+        analysis_state_snapshot = next((m.content for m in reversed(history) if m.role == Role.ASSISTANT and m.content and 'tasks_defined' in m.content), "{}")
+
+        if not raw_text:
+            self.logger.warning("Aucun texte brut (message utilisateur initial) trouvé dans l'historique.")
+            return ChatMessageContent(role=Role.ASSISTANT, content='{"error": "Initial text (user message) not found in history."}', name=self.name)
+
+        # Décider de l'action : écrire la conclusion ou définir la prochaine tâche.
+        # Cette logique est simplifiée. Une vraie implémentation analyserait `analysis_state_snapshot`
+        # pour voir si toutes les tâches sont complétées.
+        # Si le prompt v11 est assez intelligent, il peut faire ce choix lui-même.
+        action_to_perform = "conclusion" if '"final_conclusion": null' not in analysis_state_snapshot and len(analysis_state_snapshot) > 10 else "define_tasks"
+
+        self.logger.info(f"PM Agent সিদ্ধান্ত (decision): {action_to_perform}")
+
+        try:
+            if action_to_perform == "conclusion" and '"conclusion"' in self.system_prompt: # Vérifie si la conclusion est une étape attendue
+                self.logger.info("Tentative de rédaction de la conclusion.")
+                result_str = await self.write_conclusion(analysis_state_snapshot, raw_text)
+            else:
+                self.logger.info("Définition de la prochaine tâche.")
+                result_str = await self.define_tasks_and_delegate(analysis_state_snapshot, raw_text)
+            
+            return ChatMessageContent(role=Role.ASSISTANT, content=result_str, name=self.name)
+
+        except Exception as e:
+            self.logger.error(f"Erreur durant l'invocation du PM Agent: {e}", exc_info=True)
+            error_msg = f'{{"error": "An unexpected error occurred in ProjectManagerAgent: {e}"}}'
+            return ChatMessageContent(role=Role.ASSISTANT, content=error_msg, name=self.name)
 
     # D'autres méthodes métiers pourraient être ajoutées ici si nécessaire,
     # par exemple, une méthode qui encapsule la logique de décision principale du PM
diff --git a/argumentation_analysis/orchestration/analysis_runner.py b/argumentation_analysis/orchestration/analysis_runner.py
index b39cc4ed..8866e26e 100644
--- a/argumentation_analysis/orchestration/analysis_runner.py
+++ b/argumentation_analysis/orchestration/analysis_runner.py
@@ -28,12 +28,10 @@ from semantic_kernel.contents.chat_message_content import ChatMessageContent as
 from semantic_kernel.kernel import Kernel as SKernel # Alias pour éviter conflit avec Kernel de SK
 
 # Imports Semantic Kernel
-from semantic_kernel.agents import AgentGroupChat, Agent
 import semantic_kernel as sk
-from semantic_kernel.contents import ChatMessageContent
-from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion, AzureChatCompletion
-from semantic_kernel.contents.utils.author_role import AuthorRole
 from semantic_kernel.contents.chat_history import ChatHistory
+from semantic_kernel.contents.chat_message_content import ChatMessageContent
+from semantic_kernel.contents.role import Role
 
 # Correct imports
 from argumentation_analysis.core.shared_state import RhetoricalAnalysisState
@@ -161,29 +159,66 @@ async def _run_analysis_conversation(
             run_logger.critical("Aucun agent actif n'a pu être initialisé. Annulation de l'analyse.")
             return {"status": "error", "message": "Aucun agent actif."}
 
-        run_logger.info(f"Création du AgentGroupChat avec les agents: {[agent.name for agent in active_agents]}")
+        run_logger.info(f"Agents actifs pour la conversation: {[agent.name for agent in active_agents]}")
 
-        # Créer le groupe de chat
-        # Message initial pour lancer la conversation
-        initial_message_text = (
-            "Vous êtes une équipe d'analystes experts en argumentation. "
-            "Votre mission est d'analyser le texte suivant de manière collaborative. "
-            "Le Project Manager (PM) doit initier et coordonner. "
-            "Les autres agents attendent les instructions du PM. "
-            f"Voici le texte à analyser:\n\n---\n{local_state.raw_text}\n---"
+        # Création de l'historique de chat et message initial
+        chat = ChatHistory()
+        initial_user_message = (
+            "Vous êtes une équipe d'analystes experts en argumentation. Votre mission est d'analyser le texte suivant. "
+            "Le ProjectManagerAgent doit commencer par définir les tâches. Les autres agents attendent ses instructions. "
+            f"Le texte à analyser est:\n\n---\n{texte_a_analyser}\n---"
         )
-        
-        # Créer un historique de chat et y ajouter le message initial
-        chat_history_for_group = ChatHistory()
-        chat_history_for_group.add_user_message(initial_message_text)
+        chat.add_user_message(initial_user_message)
+        run_logger.info("Historique de chat initialisé avec le message utilisateur.")
 
-        # Créer le groupe de chat avec l'historique pré-rempli
-        group_chat = AgentGroupChat(agents=active_agents, chat_history=chat_history_for_group)
+        full_history: List[ChatMessageContent] = [chat.messages[-1]]
         
-        run_logger.info("Démarrage de l'invocation du groupe de chat...")
-        # L'invocation se fait sans argument car le premier message est déjà dans l'historique.
-        full_history = [message async for message in group_chat.invoke()]
-        run_logger.info("Conversation terminée.")
+        # Boucle de conversation manuelle
+        for i in range(15): # Limite de sécurité de 15 tours
+            run_logger.info(f"--- Tour de Conversation {i+1}/15 ---")
+
+            # 1. Utiliser le PM pour déterminer le prochain agent
+            run_logger.debug("Invocation du ProjectManagerAgent pour désigner le prochain agent...")
+            pm_response = await pm_agent_refactored.invoke_custom(history=full_history)
+            full_history.append(pm_response)
+
+            try:
+                # Le PM doit répondre avec un JSON contenant le nom de l'agent
+                response_data = json.loads(pm_response.content or "{}")
+                next_agent_name = response_data.get("next_agent")
+                
+                if next_agent_name == "FINISH":
+                    run_logger.info("Le ProjectManagerAgent a signalé la fin de l'analyse.")
+                    break
+                
+                if not next_agent_name:
+                    run_logger.warning("Le PM n'a pas désigné de prochain agent. Fin de la boucle.")
+                    break
+
+                # 2. Trouver l'agent désigné
+                next_agent = next((agent for agent in active_agents if agent.name == next_agent_name), None)
+                if not next_agent:
+                    run_logger.error(f"Agent désigné '{next_agent_name}' non trouvé. Fin de la boucle.")
+                    break
+                
+                run_logger.info(f"Agent désigné par le PM: {next_agent.name}")
+
+                # 3. Invoquer l'agent désigné
+                run_logger.debug(f"Invocation de l'agent '{next_agent.name}'...")
+                agent_response = await next_agent.invoke_custom(history=full_history)
+                full_history.append(agent_response)
+                run_logger.info(f"Réponse reçue de {next_agent.name}.")
+
+            except json.JSONDecodeError:
+                run_logger.error("Réponse du PM non-JSON. Fin de la boucle.")
+                break
+            except Exception as e:
+                run_logger.error(f"Erreur pendant le tour de conversation: {e}", exc_info=True)
+                break
+        else:
+            run_logger.warning("Limite de 15 tours de conversation atteinte.")
+
+        run_logger.info("Cycle de conversation terminé.")
         
         # Logger l'historique complet pour le débogage
         if full_history:
diff --git a/argumentation_analysis/ui/file_operations.py b/argumentation_analysis/ui/file_operations.py
index 8ae710f9..0e7e6ae1 100644
--- a/argumentation_analysis/ui/file_operations.py
+++ b/argumentation_analysis/ui/file_operations.py
@@ -17,8 +17,9 @@ file_ops_logger = utils_logger
 
 def load_extract_definitions(
     config_file: Path,
-    b64_derived_key: Optional[str], 
-    app_config: Optional[Dict[str, Any]] = None 
+    b64_derived_key: Optional[str],
+    app_config: Optional[Dict[str, Any]] = None,
+    raise_on_decrypt_error: bool = False
 ) -> list:
     """Charge, déchiffre et décompresse les définitions depuis le fichier chiffré."""
     # Utiliser uniquement DEFAULT_EXTRACT_SOURCES comme fallback pour éviter le cycle avec EXTRACT_SOURCES
@@ -35,16 +36,19 @@ def load_extract_definitions(
             with open(config_file, 'rb') as f: encrypted_data = f.read()
             decrypted_compressed_data = decrypt_data_with_fernet(encrypted_data, b64_derived_key)
             
-            if not decrypted_compressed_data: # decrypt_data_with_fernet retourne None en cas d'InvalidToken ou autre erreur de déchiffrement
-                file_ops_logger.warning(f"[WARN] Echec du dechiffrement pour '{config_file}' (decrypt_data_with_fernet a retourne None). Utilisation des definitions par defaut.")
-                return [item.copy() for item in fallback_definitions]
-            
+            if not decrypted_compressed_data:
+                file_ops_logger.error(f"Échec du déchiffrement pour '{config_file}'. Le token est peut-être invalide.")
+                raise InvalidToken(f"Échec du déchiffrement pour '{config_file}'.")
+
             decompressed_data = gzip.decompress(decrypted_compressed_data)
             definitions = json.loads(decompressed_data.decode('utf-8'))
-            file_ops_logger.info("[OK] Definitions chargees et dechiffrees.")
+            file_ops_logger.info("✅ Définitions chargées et déchiffrées.")
 
         except InvalidToken:
-            file_ops_logger.error(f"[FAIL] InvalidToken explicitement levee lors du dechiffrement de '{config_file}'. Utilisation definitions par defaut.", exc_info=True)
+            # Ce bloc est spécifiquement pour quand decrypt_data_with_fernet lève InvalidToken
+            file_ops_logger.error(f"❌ Token invalide (InvalidToken) lors du déchiffrement de '{config_file}'.", exc_info=True)
+            if raise_on_decrypt_error:
+                raise
             return [item.copy() for item in fallback_definitions]
         except Exception as e:
             file_ops_logger.error(f"[FAIL] Erreur chargement/dechiffrement '{config_file}': {e}. Utilisation definitions par defaut.", exc_info=True)
diff --git a/generateur_donnees_synthetiques_llm.py b/generateur_donnees_synthetiques_llm.py
new file mode 100644
index 00000000..9ef50a8d
--- /dev/null
+++ b/generateur_donnees_synthetiques_llm.py
@@ -0,0 +1,156 @@
+#!/usr/bin/env python3
+"""
+Générateur de données synthétiques avec vrais LLMs OpenRouter
+===========================================================
+"""
+
+import requests
+import json
+import os
+from datetime import datetime
+from pathlib import Path
+
+def load_env_file():
+    """Charge le fichier .env"""
+    env_file = Path('.env')
+    if env_file.exists():
+        with open(env_file, 'r', encoding='utf-8') as f:
+            for line in f:
+                line = line.strip()
+                if line and not line.startswith('#') and '=' in line:
+                    key, value = line.split('=', 1)
+                    value = value.strip('"').strip("'")
+                    os.environ[key] = value
+
+load_env_file()
+
+def generate_synthetic_datasets():
+    """Génère des datasets argumentatifs complexes"""
+    
+    prompts = [
+        {
+            "name": "arguments_ethique_ia",
+            "prompt": "Générez 3 arguments structurés sur l'éthique de l'IA, avec prémisses, conclusions et contre-arguments."
+        },
+        {
+            "name": "logique_modale_complexe", 
+            "prompt": "Créez un raisonnement modal avec nécessité, possibilité et contingence sur la justice sociale."
+        },
+        {
+            "name": "paradoxes_logiques",
+            "prompt": "Formulez 2 paradoxes logiques originaux avec analyse de leur structure argumentative."
+        },
+        {
+            "name": "sophismes_detectes",
+            "prompt": "Rédigez des exemples de 5 sophismes différents (appel à l'autorité, ad hominem, etc.) dans un débat politique."
+        }
+    ]
+    
+    results = []
+    
+    for i, dataset in enumerate(prompts, 1):
+        print(f"[*] Génération dataset {i}/4: {dataset['name']}")
+        
+        try:
+            response = requests.post(
+                'http://localhost:3000/analyze',
+                json={
+                    'text': dataset['prompt'],
+                    'analysis_type': 'comprehensive',
+                    'options': {'generate_examples': True}
+                },
+                timeout=30
+            )
+            
+            if response.status_code == 200:
+                result = response.json()
+                
+                synthetic_data = {
+                    'dataset_name': dataset['name'],
+                    'generation_time': datetime.now().isoformat(),
+                    'prompt': dataset['prompt'],
+                    'llm_response': result,
+                    'analysis_id': result.get('analysis_id'),
+                    'quality_metrics': {
+                        'response_length': len(str(result)),
+                        'structure_completeness': 'results' in result and 'metadata' in result,
+                        'processing_time': result.get('metadata', {}).get('duration', 0)
+                    }
+                }
+                
+                results.append(synthetic_data)
+                print(f"[OK] Dataset généré - ID: {result.get('analysis_id', 'N/A')}")
+            else:
+                print(f"[ERREUR] Génération échouée: {response.status_code}")
+                
+        except Exception as e:
+            print(f"[ERREUR] Dataset {dataset['name']}: {e}")
+    
+    return results
+
+def save_synthetic_datasets(datasets):
+    """Sauvegarde les datasets"""
+    
+    # Création du répertoire de données
+    data_dir = Path('data/synthetic_datasets')
+    data_dir.mkdir(parents=True, exist_ok=True)
+    
+    # Sauvegarde globale
+    with open(data_dir / 'synthetic_datasets_llm.json', 'w', encoding='utf-8') as f:
+        json.dump({
+            'generation_timestamp': datetime.now().isoformat(),
+            'llm_provider': 'OpenRouter gpt-4o-mini',
+            'total_datasets': len(datasets),
+            'datasets': datasets
+        }, f, indent=2, ensure_ascii=False)
+    
+    # Sauvegarde individuelle
+    for dataset in datasets:
+        filename = f"{dataset['dataset_name']}.json"
+        with open(data_dir / filename, 'w', encoding='utf-8') as f:
+            json.dump(dataset, f, indent=2, ensure_ascii=False)
+    
+    print(f"[OK] {len(datasets)} datasets sauvegardés dans {data_dir}")
+    return data_dir
+
+def main():
+    """Point d'entrée"""
+    print("=" * 70)
+    print("  GÉNÉRATION DONNÉES SYNTHÉTIQUES AVEC VRAIS LLMs")
+    print("=" * 70)
+    
+    # Vérification connexion Flask
+    try:
+        response = requests.get('http://localhost:3000/status', timeout=5)
+        if response.status_code == 200:
+            print("[OK] Interface Flask active")
+        else:
+            print("[ERREUR] Interface Flask non accessible")
+            return 1
+    except:
+        print("[ERREUR] Interface Flask non accessible")
+        return 1
+    
+    # Génération
+    datasets = generate_synthetic_datasets()
+    
+    if datasets:
+        data_dir = save_synthetic_datasets(datasets)
+        
+        print(f"\n[SUCCÈS] {len(datasets)} datasets synthétiques générés")
+        print(f"[INFO] Répertoire: {data_dir}")
+        
+        # Statistiques
+        total_chars = sum(d['quality_metrics']['response_length'] for d in datasets)
+        avg_time = sum(d['quality_metrics']['processing_time'] for d in datasets) / len(datasets)
+        
+        print(f"[STATS] Volume total: {total_chars:,} caractères")
+        print(f"[STATS] Temps moyen: {avg_time:.2f}s par dataset")
+        
+        return 0
+    else:
+        print("\n[ÉCHEC] Aucun dataset généré")
+        return 1
+
+if __name__ == "__main__":
+    exit(main())
\ No newline at end of file
diff --git a/project_core/webapp_from_scripts/frontend_manager.py b/project_core/webapp_from_scripts/frontend_manager.py
index 90caada3..7bb84fe7 100644
--- a/project_core/webapp_from_scripts/frontend_manager.py
+++ b/project_core/webapp_from_scripts/frontend_manager.py
@@ -120,7 +120,7 @@ class FrontendManager:
                 try:
                     self.frontend_stdout_log_file.close()
                 except Exception:
-                    pass # Ignorer les erreurs de fermeture
+                    pass
             if self.frontend_stderr_log_file:
                 try:
                     self.frontend_stderr_log_file.close()
@@ -141,7 +141,7 @@ class FrontendManager:
             self.process = subprocess.Popen(
                 cmd,
                 stdout=subprocess.PIPE,  # Capture de la sortie standard
-                stderr=subprocess.STDOUT, # Fusionner stdout et stderr pour tout capturer
+                stderr=self.frontend_stderr_log_file,
                 cwd=self.frontend_path,
                 env=frontend_env,
                 shell=shell,
@@ -317,7 +317,7 @@ class FrontendManager:
         """Tâche asynchrone pour lire stdout ligne par ligne."""
         loop = asyncio.get_event_loop()
          # La chaîne à rechercher. Peut être adaptée si les logs de react-scripts changent.
-        success_strings = ["Compiled successfully!", "webpack compiled successfully", "webpack compiled with"]
+        success_strings = ["Compiled successfully!", "webpack compiled successfully"]
         
         while True:
             if not self.process or not self.process.stdout:
diff --git a/project_core/webapp_from_scripts/playwright_runner.py b/project_core/webapp_from_scripts/playwright_runner.py
index 34a8e3b7..6a9d7d38 100644
--- a/project_core/webapp_from_scripts/playwright_runner.py
+++ b/project_core/webapp_from_scripts/playwright_runner.py
@@ -82,186 +82,89 @@ class PlaywrightRunner:
             return False
 
     def _merge_runtime_config(self, runtime_config: Dict[str, Any]) -> Dict[str, Any]:
-        """Fusionne configuration par défaut avec runtime"""
+        """Fusionne la configuration par défaut avec celle fournie à l'exécution."""
+        # Priorité : runtime_config > self.config > valeurs par défaut
         effective_config = {
-            'backend_url': 'http://localhost:5003',
-            'frontend_url': 'http://localhost:3000',
+            'backend_url': 'http://localhost:5003', # Valeur par défaut
+            'frontend_url': 'http://localhost:3000', # Valeur par défaut
             'headless': self.headless,
             'browser': self.browser,
             'timeout_ms': self.timeout_ms,
         }
-        effective_config.update(runtime_config)
+        effective_config.update(self.config) # Appliquer la config de l'instance
+        effective_config.update(runtime_config) # Écraser avec la config runtime
         return effective_config
 
     async def _prepare_test_environment(self, config: Dict[str, Any]):
-        """Prépare l'environnement pour les tests"""
+        """Prépare l'environnement d'exécution pour les tests Playwright."""
         env_vars = {
-            'API_BASE_URL': config['backend_url'], # Correction du nom de la variable
             'BACKEND_URL': config['backend_url'],
             'FRONTEND_URL': config['frontend_url'],
-            'PLAYWRIGHT_BASE_URL': config.get('frontend_url') or config['backend_url'],
-            'HEADLESS': str(config['headless']).lower(),
-            'BROWSER': config['browser'],
-            'SCREENSHOTS_DIR': str(self.screenshots_dir),
-            'TRACES_DIR': str(self.traces_dir)
+            'PLAYWRIGHT_BASE_URL': config.get('frontend_url', config['backend_url']),
+            # Les variables spécifiques à Playwright comme BROWSER, HEADLESS, etc.,
+            # sont passées en ligne de commande plutôt que via les variables d'environnement
+            # pour éviter les conflits avec le fichier de configuration Playwright.
         }
         for key, value in env_vars.items():
-            os.environ[key] = value
+            if value:
+                os.environ[key] = str(value)
         self.logger.info(f"Variables test configurées: {env_vars}")
 
-    def _build_command(self,
-                         test_type: str,
-                         test_paths: List[str],
-                         config: Dict[str, Any],
-                         pytest_args: List[str],
-                         playwright_config_path: Optional[str]) -> List[str]:
-        """Construit dynamiquement la commande de test en fonction du type."""
-        if test_type == 'javascript':
-            return self._build_js_command(test_paths, config, playwright_config_path)
-        elif test_type == 'python':
-            return self._build_python_command(test_paths, config, pytest_args)
-        else:
-            raise ValueError(f"Type de test non supporté: {test_type}")
-
-    def _build_js_command(self, test_paths: List[str], config: Dict[str, Any], playwright_config_path: Optional[str]) -> List[str]:
-        """Construit la commande 'npx playwright test ...'."""
-        self.logger.info("Construction de la commande pour les tests JavaScript.")
+    def _build_playwright_command_string(self, test_paths: List[str],
+                                         config: Dict[str, Any]) -> List[str]:
+        """Construit la liste de commande 'npx playwright test ...'."""
         node_home = os.getenv('NODE_HOME')
         if not node_home:
-            raise EnvironmentError("NODE_HOME non défini, impossible de trouver npx.")
+            raise RuntimeError("NODE_HOME n'est pas défini. Impossible de trouver npx.")
         
-        npx_executable = Path(node_home) / 'npx.cmd'
-        if not npx_executable.is_file():
-            raise FileNotFoundError(f"npx.cmd non trouvé: {npx_executable}")
-
-        self.logger.info(f"Utilisation de npx: {npx_executable}")
-
-        parts = [str(npx_executable), 'playwright', 'test']
-        parts.extend(test_paths)
+        npx_executable = str(Path(node_home) / 'npx.cmd')
         
-        # Toujours utiliser notre configuration unifiée pour assurer la génération des traces
-        config_path = playwright_config_path or 'tests/e2e/playwright.config.js'
-        parts.extend(['--config', config_path])
+        parts = [npx_executable, 'playwright', 'test']
+        parts.extend(test_paths)
         
         if not config.get('headless', True):
             parts.append('--headed')
             
+        # Lorsque le fichier de configuration utilise des "projets",
+        # il faut utiliser --project au lieu de --browser.
         parts.append(f"--project={config['browser']}")
-        parts.append('--reporter=list,html')
-
-        if os.environ.get('PLAYWRIGHT_DEBUG') == '1':
-            parts.append('--debug')
-        
-        self.logger.info(f"Commande JS construite: {parts}")
-        return parts
-
-    def _build_python_command(self, test_paths: List[str], config: Dict[str, Any], pytest_args: List[str]) -> List[str]:
-        """Construit la commande 'pytest ...'."""
-        self.logger.info("Construction de la commande pour les tests Python (pytest).")
-        
-        # Chercher pytest dans le PATH système, qui est configuré par l'environnement activé.
-        pytest_executable = shutil.which("pytest")
-        
-        if not pytest_executable:
-            # Essayer de construire le chemin manuellement comme fallback pour certains setups Conda
-            conda_prefix = os.getenv('CONDA_PREFIX')
-            if conda_prefix:
-                manual_path = Path(conda_prefix) / 'Scripts' / 'pytest.exe'
-                if manual_path.is_file():
-                    pytest_executable = str(manual_path)
-
-        if not pytest_executable:
-            raise FileNotFoundError("Exécutable 'pytest' non trouvé dans le PATH ou l'environnement Conda.")
-        
-        self.logger.info(f"Utilisation de pytest: {pytest_executable}")
-
-        parts = [pytest_executable]
-        # Aplatir la liste des chemins de test au cas où certains seraient des répertoires
-        all_test_files = []
-        for path in test_paths:
-            p = Path(path)
-            if p.is_dir():
-                all_test_files.extend(str(f) for f in p.glob('**/test_*.py'))
-            elif p.is_file():
-                all_test_files.append(str(p))
-        
-        if not all_test_files:
-            self.logger.warning(f"Aucun fichier de test trouvé dans les chemins: {test_paths}")
-            # Retourner une commande qui échouera avec un message clair
-            return ["pytest", "--collect-only", "-m", "not marker_that_does_not_exist"]
-
-        parts.extend(all_test_files)
-        parts.extend(pytest_args)
-        
-        parts.append(f"--browser={config['browser']}")
-        if not config.get('headless', True):
-            parts.append('--headed')
-            
-        parts.append(f"--screenshot=only-on-failure")
-        parts.append(f"--output={self.screenshots_dir / 'pytest'}")
-        parts.append(f"--trace")
+        parts.append(f"--timeout={config['timeout_ms']}")
 
-        self.logger.info(f"Commande Python construite: {parts}")
+        self.logger.info(f"Construction de la commande 'npx playwright': {parts}")
         return parts
 
-    async def _execute_tests(self, command_parts: List[str],
+    async def _execute_tests(self, playwright_command_parts: List[str],
                            config: Dict[str, Any]) -> subprocess.CompletedProcess:
-        """Exécute les tests en utilisant asyncio.create_subprocess_exec pour un contrôle total."""
         
-        self.logger.info(f"Commande à exécuter (via asyncio): {' '.join(command_parts)}")
+        self.logger.info(f"Commande à exécuter: {' '.join(playwright_command_parts)}")
         
-        proc = None
-        stdout_log = Path("logs") / "runner_stdout.log"
-        stderr_log = Path("logs") / "runner_stderr.log"
-        self.logger.info(f"Redirection stdout vers: {stdout_log}")
-        self.logger.info(f"Redirection stderr vers: {stderr_log}")
+        # Le répertoire de travail doit être la racine du projet
+        test_dir = '.'
 
         try:
-            with open(stdout_log, 'wb') as stdout_file, \
-                 open(stderr_log, 'wb') as stderr_file:
-                
-                proc = await asyncio.create_subprocess_exec(
-                    *command_parts,
-                    stdout=stdout_file,
-                    stderr=stderr_file
-                )
-
-                # Attendre la fin du processus
-                return_code = await proc.wait()
-
-                self.logger.info(f"Tests terminés - Code retour: {return_code}")
+            # Utilisation de asyncio.create_subprocess_shell pour une meilleure gestion async
+            process = await asyncio.create_subprocess_shell(
+                ' '.join(playwright_command_parts),
+                cwd=test_dir,
+                stdout=asyncio.subprocess.PIPE,
+                stderr=asyncio.subprocess.PIPE
+            )
             
-            # Lire le contenu des logs pour le retour
-            stdout = stdout_log.read_text(encoding='utf-8', errors='ignore')
-            stderr = stderr_log.read_text(encoding='utf-8', errors='ignore')
-
-            # Retourner un objet compatible avec l'analyseur de résultats
-            return subprocess.CompletedProcess(
-                args=command_parts,
-                returncode=return_code,
-                stdout=stdout,
-                stderr=stderr
+            stdout, stderr = await process.communicate()
+            
+            result = subprocess.CompletedProcess(
+                args=playwright_command_parts,
+                returncode=process.returncode,
+                stdout=stdout.decode('utf-8', errors='ignore'),
+                stderr=stderr.decode('utf-8', errors='ignore')
             )
-
-        except asyncio.CancelledError:
-            self.logger.warning("L'exécution des tests a été annulée (probablement par timeout).")
-            if proc:
-                self.logger.info(f"Tentative de terminaison forcée du processus Playwright (PID: {proc.pid})...")
-                try:
-                    proc.terminate()
-                    await asyncio.wait_for(proc.wait(), timeout=5.0)
-                    self.logger.info("Processus Playwright terminé.")
-                except Exception:
-                    self.logger.error(f"Échec de la terminaison, tentative de kill du processus Playwright (PID: {proc.pid})...")
-                    proc.kill()
-                    await proc.wait()
-                    self.logger.warning("Processus Playwright tué.")
-            # Il est essentiel de relancer CancelledError pour que le timeout de l'orchestrateur fonctionne.
-            raise
-
+            
+            self.logger.info(f"Tests terminés - Code retour: {result.returncode}")
+            return result
+            
         except Exception as e:
-            self.logger.error(f"Erreur majeure lors de l'exécution de la commande Playwright avec asyncio: {e}", exc_info=True)
-            return subprocess.CompletedProcess(args=' '.join(command_parts), returncode=1, stdout="", stderr=str(e))
+            self.logger.error(f"Erreur majeure lors de l'exécution de la commande Playwright: {e}", exc_info=True)
+            return subprocess.CompletedProcess(args=' '.join(playwright_command_parts), returncode=1, stdout="", stderr=str(e))
 
     async def _analyze_results(self, result: subprocess.CompletedProcess) -> bool:
         success = result.returncode == 0
diff --git a/project_core/webapp_from_scripts/unified_web_orchestrator.py b/project_core/webapp_from_scripts/unified_web_orchestrator.py
index 69b57bfc..d544b16d 100644
--- a/project_core/webapp_from_scripts/unified_web_orchestrator.py
+++ b/project_core/webapp_from_scripts/unified_web_orchestrator.py
@@ -1,6 +1,3 @@
-# Auto-activation de l'environnement intelligent
-import project_core.core_from_scripts.auto_env
-# ---
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 """
@@ -29,7 +26,6 @@ import asyncio
 import logging
 import argparse
 import subprocess
-import shutil # Ajout pour shutil.which
 import threading
 import socket
 import signal
@@ -116,17 +112,9 @@ class UnifiedWebOrchestrator:
         self.enable_trace = not args.no_trace
 
         # Gestionnaires spécialisés
-        # Récupérer le chemin de l'environnement Conda avant d'initialiser BackendManager
-        self.conda_env_name = self.config.get('backend', {}).get('conda_env', 'projet-is')
-        self.conda_env_path = self._find_conda_env_path(self.conda_env_name)
-
-        self.backend_manager = BackendManager(
-            self.config.get('backend', {}),
-            self.logger,
-            conda_env_path=self.conda_env_path # Passer le chemin ici
-        )
+        self.backend_manager = BackendManager(self.config.get('backend', {}), self.logger)
         self.frontend_manager: Optional[FrontendManager] = None  # Sera instancié plus tard
- 
+
         playwright_config = self.config.get('playwright', {})
         # Le timeout CLI surcharge la config YAML
         playwright_config['timeout_ms'] = self.timeout_minutes * 60 * 1000
@@ -175,22 +163,6 @@ class UnifiedWebOrchestrator:
                 self.logger.info(f"Port {port} détecté comme étant utilisé.")
             return is_used
             
-    def _find_free_port(self, start_port: int, max_attempts: int = 100) -> Optional[int]:
-        """Trouve un port TCP libre en commençant à partir de start_port."""
-        self.logger.debug(f"Recherche d'un port libre à partir de {start_port}")
-        for i in range(max_attempts):
-            port = start_port + i
-            try:
-                with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
-                    s.bind(('localhost', port))
-                    self.logger.info(f"Port {port} trouvé et est libre.")
-                    return port
-            except OSError:
-                self.logger.debug(f"Port {port} est déjà utilisé, tentative suivante.")
-                continue
-        self.logger.error(f"Impossible de trouver un port libre après {max_attempts} tentatives.")
-        return None
-    
     def _load_config(self) -> Dict[str, Any]:
         """Charge la configuration depuis le fichier YAML"""
         print("[DEBUG] unified_web_orchestrator.py: _load_config()")
@@ -262,16 +234,11 @@ class UnifiedWebOrchestrator:
             },
             'playwright': {
                 'enabled': True,
-                'test_type': 'python',  # Type de test par défaut
                 'browser': 'chromium',
                 'headless': True,
-                'timeout_ms': 30000,
-                'process_timeout_s': 600,
-                'test_paths': {
-                    'python': ['tests/e2e/python/'],
-                    'javascript': ['tests/e2e/js/'],
-                    'demos': ['tests/e2e/demos/']
-                },
+                'timeout_ms': 10000,
+                'slow_timeout_ms': 20000,
+                'test_paths': ['tests/functional/'],
                 'screenshots_dir': 'logs/screenshots',
                 'traces_dir': 'logs/traces'
             },
@@ -350,7 +317,7 @@ class UnifiedWebOrchestrator:
         self.app_info.start_time = datetime.now()
         
         self.add_trace("[START] DEMARRAGE APPLICATION WEB",
-                       f"Mode: {'Headless' if headless else 'Visible'}",
+                       f"Mode: {'Headless' if headless else 'Visible'}", 
                        "Initialisation orchestrateur")
         
         try:
@@ -397,7 +364,7 @@ class UnifiedWebOrchestrator:
             self.app_info.status = WebAppStatus.ERROR
             return False
     
-    async def run_tests(self, test_type: str = None, test_paths: List[str] = None, **kwargs) -> bool:
+    async def run_tests(self, test_paths: List[str] = None, **kwargs) -> bool:
         """
         Exécute les tests Playwright avec le support natif.
         """
@@ -438,18 +405,10 @@ class UnifiedWebOrchestrator:
                       f"BASE_URL={base_url}",
                       f"BACKEND_URL={backend_url}")
 
-        # Le test_type est passé en priorité, sinon celui de la config
-        effective_test_type = test_type or self.playwright_runner.test_type
-
-        # Si les chemins ne sont pas fournis, utiliser ceux par défaut pour le type de test
-        paths_for_type = self.config.get('playwright', {}).get('test_paths', {})
-        effective_paths = test_paths or paths_for_type.get(effective_test_type)
-
-        return await self.playwright_runner.run_tests(
-            test_type=effective_test_type,
-            test_paths=effective_paths,
-            runtime_config=test_config
-        )
+        # L'ancienne gestion de subprocess.TimeoutExpired n'est plus nécessaire car
+        # le runner utilise maintenant create_subprocess_exec.
+        # Le timeout est géré plus haut par asyncio.wait_for.
+        return await self.playwright_runner.run_tests(test_paths, test_config)
     
     async def stop_webapp(self):
         """Arrête l'application web et nettoie les ressources de manière gracieuse."""
@@ -488,7 +447,6 @@ class UnifiedWebOrchestrator:
     
     async def full_integration_test(self, headless: bool = True,
                                    frontend_enabled: bool = None,
-                                   test_type: str = None,
                                    test_paths: List[str] = None,
                                    **kwargs) -> bool:
         """
@@ -504,23 +462,23 @@ class UnifiedWebOrchestrator:
         try:
             self.add_trace("[TEST] INTEGRATION COMPLETE",
                           "Démarrage orchestration complète")
-
+            
             # 1. Démarrage application
             if not await self.start_webapp(headless, frontend_enabled):
                 return False
-
+            
             # 2. Attente stabilisation
             await asyncio.sleep(2)
-
+            
             # 3. Exécution tests
-            test_success = False
             try:
                 # Utilisation d'un timeout asyncio global comme filet de sécurité ultime.
+                # Cela garantit que l'orchestrateur ne restera jamais bloqué indéfiniment.
                 test_timeout_s = self.timeout_minutes * 60
                 self.add_trace("[TEST] Lancement avec timeout global", f"{test_timeout_s}s")
-
-                test_success = await asyncio.wait_for(
-                    self.run_tests(test_type=test_type, test_paths=test_paths, **kwargs),
+                
+                success = await asyncio.wait_for(
+                    self.run_tests(test_paths, **kwargs),
                     timeout=test_timeout_s
                 )
             except asyncio.TimeoutError:
@@ -528,65 +486,26 @@ class UnifiedWebOrchestrator:
                               f"L'étape de test a dépassé le timeout de {self.timeout_minutes} minutes.",
                               "Le processus est probablement bloqué.",
                               status="error")
-                test_success = False
-
-            # 4. Analyse des traces Playwright JS après l'exécution
-            # Cette étape est exécutée même si les tests échouent pour fournir un rapport de débogage.
-            effective_test_type = test_type or self.playwright_runner.test_type
-            if effective_test_type == 'javascript':
-                await self._analyze_playwright_traces()
-
-            if test_success:
+                success = False
+            
+            if success:
                 self.add_trace("[SUCCESS] INTEGRATION REUSSIE",
-                              "Tous les tests ont passé",
+                              "Tous les tests ont passé", 
                               "Application web validée")
             else:
                 self.add_trace("[ERROR] ECHEC INTEGRATION",
-                              "Certains tests ont échoué",
+                              "Certains tests ont échoué", 
                               "Voir logs détaillés", status="error")
             
-            success = test_success # Le succès global dépend de la réussite des tests
-
         finally:
-            # 5. Nettoyage systématique
+            # 4. Nettoyage systématique
             await self.stop_webapp()
-
-            # 6. Sauvegarde trace
+            
+            # 5. Sauvegarde trace
             await self._save_trace_report()
-
-        return success
-
-    def _find_conda_env_path(self, env_name: str) -> Optional[str]:
-        """Trouve le chemin complet d'un environnement Conda."""
-        self.logger.debug(f"Recherche du chemin pour l'environnement Conda: {env_name}")
-        conda_exe = shutil.which("conda")
-        if not conda_exe:
-            self.logger.error("Exécutable Conda non trouvé avec shutil.which.")
-            return None
         
-        try:
-            result = subprocess.run(
-                [conda_exe, "env", "list", "--json"],
-                capture_output=True, text=True, check=True, encoding='utf-8'
-            )
-            envs_data = json.loads(result.stdout)
-            for env_path_str in envs_data.get("envs", []):
-                if Path(env_path_str).name == env_name:
-                    self.logger.info(f"Chemin de l'environnement Conda '{env_name}' trouvé: {env_path_str}")
-                    return str(env_path_str)
-            self.logger.error(f"Environnement Conda '{env_name}' non trouvé dans la liste.")
-            return None
-        except subprocess.CalledProcessError as e:
-            self.logger.error(f"Erreur lors de l'exécution de 'conda env list --json': {e}")
-            self.logger.error(f"Stderr: {e.stderr}")
-            return None
-        except json.JSONDecodeError as e:
-            self.logger.error(f"Erreur de décodage JSON pour 'conda env list --json': {e}")
-            return None
-        except Exception as e:
-            self.logger.error(f"Erreur inattendue dans _find_conda_env_path: {e}")
-            return None
-
+        return success
+    
     # ========================================================================
     # MÉTHODES PRIVÉES
     # ========================================================================
@@ -651,110 +570,58 @@ class UnifiedWebOrchestrator:
             self.playwright = None
 
     async def _start_backend(self) -> bool:
-        """Démarre le backend en lui allouant un port dynamique."""
-        self.add_trace("[BACKEND] DEMARRAGE BACKEND", "Recherche d'un port libre et lancement.")
-
-        backend_config = self.config.get('backend', {})
-        preferred_port = backend_config.get('start_port', 5003)
-
-        # 1. Trouver un port libre
-        free_port = self._find_free_port(preferred_port)
-        if not free_port:
-            self.add_trace("[ERROR] ECHEC BACKEND", "Aucun port libre trouvé pour le backend.", status="error")
-            return False
-
-        # 2. Préparer l'environnement
-        backend_env = os.environ.copy()
-        # Flask lit automatiquement FLASK_RUN_PORT
-        backend_env['FLASK_RUN_PORT'] = str(free_port)
+        """Démarre le backend avec failover de ports"""
+        print("[DEBUG] unified_web_orchestrator.py: _start_backend()")
+        self.add_trace("[BACKEND] DEMARRAGE BACKEND", "Lancement avec failover de ports")
         
-        self.add_trace("[BACKEND] ENV VARS",
-                       f"FLASK_RUN_PORT={free_port}",
-                       "Variables d'environnement pour le processus backend")
-
-        # 3. Ré-instancier le BackendManager avec l'environnement dynamique
-        self.backend_manager = BackendManager(
-            backend_config,
-            self.logger,
-            conda_env_path=self.conda_env_path,
-            env=backend_env
-        )
-
         result = await self.backend_manager.start()
-        if result.get('success'):
+        if result['success']:
             self.app_info.backend_url = result['url']
             self.app_info.backend_port = result['port']
             self.app_info.backend_pid = result['pid']
             
             self.add_trace("[OK] BACKEND OPERATIONNEL",
-                          f"Port: {result['port']} | PID: {result['pid']}",
+                          f"Port: {result['port']} | PID: {result['pid']}", 
                           f"URL: {result['url']}")
             return True
         else:
-            error_details = result.get('error', 'Erreur inconnue lors du démarrage du backend.')
-            self.add_trace("[ERROR] ECHEC BACKEND", error_details, status="error")
+            self.add_trace("[ERROR] ECHEC BACKEND", result['error'], "", status="error")
             return False
     
     async def _start_frontend(self) -> bool:
-        """Démarre le frontend React avec un port dynamique."""
+        """Démarre le frontend React"""
         print("[DEBUG] unified_web_orchestrator.py: _start_frontend()")
-        self.add_trace("[FRONTEND] DEMARRAGE FRONTEND", "Recherche d'un port libre et lancement de l'interface React")
-    
-        frontend_config = self.config.get('frontend', {})
-        preferred_port = frontend_config.get('port', 8081)
-        
-        # 1. Trouver un port libre pour le frontend
-        free_port = self._find_free_port(preferred_port)
-        if not free_port:
-            self.add_trace("[ERROR] ECHEC FRONTEND", "Aucun port libre trouvé pour le serveur de développement.", status="error")
-            return False  # Bloquant car le port est essentiel
-    
-        self.app_info.frontend_port = free_port
-        self.app_info.frontend_url = f"http://localhost:{free_port}"
-        
-        # 2. Préparer les variables d'environnement pour le frontend
-        #    - PORT: le port sur lequel le serveur de dev doit démarrer
-        #    - REACT_APP_API_URL: l'URL complète du backend que l'app React utilisera
-        frontend_env = os.environ.copy()
-        frontend_env['PORT'] = str(free_port)
-        frontend_env['REACT_APP_API_URL'] = self.app_info.backend_url
+        # La décision de démarrer a déjà été prise en amont
+        self.add_trace("[FRONTEND] DEMARRAGE FRONTEND", "Lancement interface React")
         
-        self.add_trace("[FRONTEND] ENV VARS",
-                       f"PORT={free_port}, REACT_APP_API_URL={self.app_info.backend_url}",
-                       "Variables d'environnement pour le process frontend")
-    
-        # 3. Instancier et démarrer le FrontendManager
+        # Instanciation tardive du FrontendManager pour lui passer l'URL du backend
         self.frontend_manager = FrontendManager(
-            frontend_config,
+            self.config.get('frontend', {}),
             self.logger,
-            backend_url=self.app_info.backend_url,
-            env=frontend_env  # Passer l'environnement complet
+            backend_url=self.app_info.backend_url
         )
-    
+
         result = await self.frontend_manager.start()
         if result['success']:
-            # Assigner les URLs et ports
-            if result['url']: # Cas serveur de dev
-                self.app_info.frontend_url = result['url']
-                self.app_info.frontend_port = result['port']
-                self.app_info.frontend_pid = result['pid']
-                self.add_trace("[OK] FRONTEND (DEV SERVER) OPERATIONNEL", f"URL: {result['url']}")
-            else: # Cas statique servi par le backend
-                self.app_info.frontend_url = self.app_info.backend_url
-                self.app_info.frontend_port = self.app_info.backend_port
-                self.add_trace("[OK] FRONTEND (STATIQUE) PRÊT", f"Servi par backend: {self.app_info.frontend_url}")
+            self.app_info.frontend_url = result['url']
+            self.app_info.frontend_port = result['port']
+            self.app_info.frontend_pid = result['pid']
+            
+            self.add_trace("[OK] FRONTEND OPERATIONNEL",
+                          f"Port: {result['port']}", 
+                          f"URL: {result['url']}")
 
-            # Écrire l'URL du frontend dans tous les cas pour signaler au script parent
-            # self.app_info.frontend_url aura toujours une valeur ici.
+            # Sauvegarde l'URL du frontend pour que les tests puissent la lire
+            print("[DEBUG] unified_web_orchestrator.py: Saving frontend URL")
             try:
                 log_dir = Path("logs")
                 log_dir.mkdir(exist_ok=True)
-                url_to_write = self.app_info.frontend_url
-                with open(log_dir / "frontend_url.txt", "w", encoding='utf-8') as f:
-                    f.write(url_to_write)
-                self.add_trace("[SYNC] FICHIER URL ECRIT", f"Fichier: logs/frontend_url.txt, URL: {url_to_write}")
+                with open(log_dir / "frontend_url.txt", "w") as f:
+                    f.write(result['url'])
+                self.add_trace("[SAVE] URL FRONTEND SAUVEGARDEE", f"URL {result['url']} écrite dans logs/frontend_url.txt")
+                print(f"[DEBUG] unified_web_orchestrator.py: Frontend URL saved to logs/frontend_url.txt: {result['url']}")
             except Exception as e:
-                self.add_trace("[ERROR] ECRITURE FICHIER URL", str(e), status="error")
+                self.add_trace("[ERROR] SAUVEGARDE URL FRONTEND", str(e), status="error")
             
             return True
         else:
@@ -927,64 +794,6 @@ class UnifiedWebOrchestrator:
         
         return content
 
-    async def _analyze_playwright_traces(self):
-        """Lance l'analyseur de traces en tant que sous-processus et logue le résultat."""
-        self.add_trace("[ANALYZE] ANALYSE DES TRACES PLAYWRIGHT", "Lancement du script trace_analyzer.py")
-        analyzer_script_path = "services/web_api/trace_analyzer.py"
-        
-        # Le répertoire de traces pour Playwright JS est défini dans sa config
-        # et est relatif au répertoire de test, donc 'tests/e2e/test-results/'
-        # Playwright génère un dossier par test qui contient 'trace.zip'
-        # Le trace_analyzer.py doit être adapté pour chercher ces .zip, les extraire, et lire le contenu.
-        # Pour l'instant, on pointe vers le dossier où Playwright génère ses rapports
-        # La refactorisation du trace_analyzer est une tâche future
-        trace_dir = Path("tests/e2e/test-results/")
-
-        if not Path(analyzer_script_path).exists():
-            self.add_trace("[ERROR] Script d'analyse non trouvé", f"Chemin: {analyzer_script_path}", status="error")
-            return
-            
-        try:
-            # Utiliser le même interpréteur Python que celui qui exécute l'orchestrateur
-            python_executable = sys.executable
-            
-            command_to_run = [
-                python_executable,
-                analyzer_script_path,
-                '--mode=summary',
-                # On passe le répertoire où sont générés les rapports Playwright
-                '--trace-dir', str(trace_dir)
-            ]
-
-            self.logger.debug(f"Lancement de l'analyseur de trace avec la commande : {' '.join(command_to_run)}")
-            
-            proc = await asyncio.create_subprocess_exec(
-                *command_to_run,
-                stdout=asyncio.subprocess.PIPE,
-                stderr=asyncio.subprocess.PIPE
-            )
-            
-            stdout, stderr = await proc.communicate()
-            
-            stdout_str = stdout.decode('utf-8', errors='ignore')
-            stderr_str = stderr.decode('utf-8', errors='ignore')
-            
-            if proc.returncode == 0:
-                self.add_trace("[OK] ANALYSE DE TRACE TERMINÉE", "Détails ci-dessous")
-                # Loggue le résumé directement dans la trace de l'orchestrateur
-                self.logger.info("\n--- DEBUT RAPPORT D'ANALYSE DE TRACE ---\n"
-                                f"{stdout_str}"
-                                "\n--- FIN RAPPORT D'ANALYSE DE TRACE ---")
-            else:
-                self.add_trace("[ERROR] ECHEC ANALYSE DE TRACE", "Le script a retourné une erreur.", status="error")
-                self.logger.error(f"Erreur lors de l'exécution de {analyzer_script_path}:")
-                self.logger.error("STDOUT:\n" + stdout_str)
-                self.logger.error("STDERR:\n" + stderr_str)
-                
-        except Exception as e:
-            self.add_trace("[ERROR] ERREUR CRITIQUE ANALYSEUR", str(e), status="error")
-
-
 def main():
     """Point d'entrée principal en ligne de commande"""
     print("[DEBUG] unified_web_orchestrator.py: main()")
@@ -999,26 +808,23 @@ def main():
                        help='Force activation frontend')
     parser.add_argument('--tests', nargs='*',
                        help='Chemins spécifiques des tests à exécuter.')
-    parser.add_argument('--test-type', type=str,
-                       choices=['python', 'javascript', 'demos'],
-                       help='Type de tests à exécuter (python, javascript, demos).')
     parser.add_argument('--timeout', type=int, default=20,
-                           help="Timeout global en minutes pour l'orchestration.")
+                           help='Timeout global en minutes pour l\'orchestration.')
     parser.add_argument('--log-level', type=str, default='INFO',
                            choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
                            help='Niveau de log pour la console et le fichier.')
     parser.add_argument('--no-trace', action='store_true',
                            help='Désactive la génération du rapport de trace Markdown.')
     parser.add_argument('--no-playwright', action='store_true',
-                        help="Désactive l'exécution des tests Playwright.")
+                        help='Désactive l\'exécution des tests Playwright.')
     parser.add_argument('--exit-after-start', action='store_true',
                         help='Démarre les serveurs puis quitte sans lancer les tests.')
 
     # Commandes
-    parser.add_argument('--start', action='store_true', help="Démarre seulement l'application.")
-    parser.add_argument('--stop', action='store_true', help="Arrête l'application.")
-    parser.add_argument('--test', action='store_true', help="Exécute seulement les tests sur une app déjà démarrée ou en la démarrant.")
-    parser.add_argument('--integration', action='store_true', default=True, help="Test d'intégration complet (défaut).")
+    parser.add_argument('--start', action='store_true', help='Démarre seulement l\'application.')
+    parser.add_argument('--stop', action='store_true', help='Arrête l\'application.')
+    parser.add_argument('--test', action='store_true', help='Exécute seulement les tests sur une app déjà démarrée ou en la démarrant.')
+    parser.add_argument('--integration', action='store_true', default=True, help='Test d\'intégration complet (défaut).')
 
     args, unknown = parser.parse_known_args()
 
@@ -1051,10 +857,10 @@ def main():
             elif args.test:
                 # Pour les tests seuls, on fait un cycle complet mais sans arrêt entre les étapes.
                 if await orchestrator.start_webapp(orchestrator.headless, args.frontend):
-                    success = await orchestrator.run_tests(test_type=args.test_type, test_paths=args.tests)
+                    success = await orchestrator.run_tests(args.tests)
             else:  # --integration par défaut
                 success = await orchestrator.full_integration_test(
-                    orchestrator.headless, args.frontend, args.test_type, args.tests)
+                    orchestrator.headless, args.frontend, args.tests)
         except KeyboardInterrupt:
             print("\n🛑 Interruption utilisateur détectée. Arrêt en cours...")
             # L'arrêt est géré par le signal handler
@@ -1073,48 +879,6 @@ def main():
     orchestrator.logger.info(f"Code de sortie final : {exit_code}")
     sys.exit(exit_code)
 
-    async def _analyze_playwright_traces(self):
-        """Lance l'analyseur de traces en tant que sous-processus et logue le résultat."""
-        self.add_trace("[ANALYZE] ANALYSE DES TRACES PLAYWRIGHT", "Lancement du script trace_analyzer.py")
-        analyzer_script_path = "services/web_api/trace_analyzer.py"
-        
-        if not Path(analyzer_script_path).exists():
-            self.add_trace("[ERROR] Script d'analyse non trouvé", f"Chemin: {analyzer_script_path}", status="error")
-            return
-            
-        try:
-            # Utiliser le même interpréteur Python que celui qui exécute l'orchestrateur
-            python_executable = sys.executable
-            
-            self.logger.debug(f"Lancement de l'analyseur de trace avec la commande : {[python_executable, analyzer_script_path, '--mode=summary']}")
-            
-            proc = await asyncio.create_subprocess_exec(
-                python_executable, analyzer_script_path, '--mode=summary',
-                stdout=asyncio.subprocess.PIPE,
-                stderr=asyncio.subprocess.PIPE
-            )
-            
-            stdout, stderr = await proc.communicate()
-            
-            # Décoder la sortie
-            stdout_str = stdout.decode('utf-8', errors='ignore')
-            stderr_str = stderr.decode('utf-8', errors='ignore')
-            
-            if proc.returncode == 0:
-                self.add_trace("[OK] ANALYSE DE TRACE TERMINÉE", "Détails ci-dessous")
-                # Loggue le résumé directement dans la trace de l'orchestrateur
-                self.logger.info("\n--- DEBUT RAPPORT D'ANALYSE DE TRACE ---\n"
-                                f"{stdout_str}"
-                                "\n--- FIN RAPPORT D'ANALYSE DE TRACE ---")
-            else:
-                self.add_trace("[ERROR] ECHEC ANALYSE DE TRACE", "Le script a retourné une erreur.", status="error")
-                self.logger.error("Erreur lors de l'exécution de trace_analyzer.py:")
-                self.logger.error("STDOUT:\n" + stdout_str)
-                self.logger.error("STDERR:\n" + stderr_str)
-                
-        except Exception as e:
-            self.add_trace("[ERROR] ERREUR CRITIQUE ANALYSEUR", str(e), status="error")
-
 if __name__ == "__main__":
     from project_core.core_from_scripts import auto_env
     auto_env.ensure_env()
diff --git a/scripts/webapp/config/webapp_config.yml b/scripts/webapp/config/webapp_config.yml
index 85970705..131f555e 100644
--- a/scripts/webapp/config/webapp_config.yml
+++ b/scripts/webapp/config/webapp_config.yml
@@ -38,12 +38,7 @@ playwright:
   process_timeout_s: 600
   timeout_ms: 30000
   test_paths:
-    python:
-    - tests/e2e/python/
-    javascript:
-    - tests/e2e/js/
-    demos:
-    - tests/e2e/demos/
+  - tests_playwright/tests/
   screenshots_dir: logs/screenshots
   traces_dir: logs/traces
 webapp:
diff --git a/tests/conftest.py b/tests/conftest.py
index 9b3e07d9..4e2a2242 100644
--- a/tests/conftest.py
+++ b/tests/conftest.py
@@ -1,12 +1,3 @@
-import sys
-import os
-from pathlib import Path
-
-# Ajoute la racine du projet au sys.path pour résoudre les problèmes d'import
-# causés par le `rootdir` de pytest qui interfère avec la résolution des modules.
-project_root = Path(__file__).parent.parent.resolve()
-if str(project_root) not in sys.path:
-    sys.path.insert(0, str(project_root))
 """
 Configuration pour les tests pytest.
 
@@ -15,207 +6,48 @@ Il configure les mocks nécessaires pour les tests et utilise les vraies bibliot
 lorsqu'elles sont disponibles. Pour Python 3.12 et supérieur, le mock JPype1 est
 automatiquement utilisé en raison de problèmes de compatibilité.
 """
-import project_core.core_from_scripts.auto_env
 import sys
 import os
 import pytest
 from unittest.mock import patch, MagicMock
-import importlib.util
-import logging
-import threading # Ajout de l'import pour l'inspection des threads
-# --- Configuration globale du Logging pour les tests ---
-# Le logger global pour conftest est déjà défini plus bas,
-# mais nous avons besoin de configurer basicConfig tôt.
-# Nous allons utiliser un logger temporaire ici ou le logger racine.
-_conftest_setup_logger = logging.getLogger("conftest.setup")
-
-if not logging.getLogger().handlers: # Si le root logger n'a pas de handlers, basicConfig n'a probablement pas été appelé efficacement.
-    logging.basicConfig(
-        level=logging.INFO, # Ou un autre niveau pertinent pour les tests globaux
-        format='%(asctime)s [%(levelname)s] [%(name)s] %(message)s',
-        datefmt='%H:%M:%S'
-    )
-    _conftest_setup_logger.info("Configuration globale du logging appliquée.")
-else:
-    _conftest_setup_logger.info("Configuration globale du logging déjà présente ou appliquée par un autre module.")
-# --- Début Patching JPype Mock au niveau module si nécessaire ---
-os.environ['USE_REAL_JPYPE'] = 'false'
-_SHOULD_USE_REAL_JPYPE = os.environ.get('USE_REAL_JPYPE', 'false').lower() in ('true', '1')
-_conftest_setup_logger.info(f"conftest.py: USE_REAL_JPYPE={os.environ.get('USE_REAL_JPYPE', 'false')}, _SHOULD_USE_REAL_JPYPE={_SHOULD_USE_REAL_JPYPE}")
-
-if not _SHOULD_USE_REAL_JPYPE:
-    _conftest_setup_logger.info("conftest.py: Application du mock JPype au niveau module dans sys.modules.")
-    try:
-        # S'assurer que le répertoire des mocks est dans le path pour les imports suivants
-        _current_dir_for_jpype_mock_patch = os.path.dirname(os.path.abspath(__file__))
-        _mocks_dir_for_jpype_mock_patch = os.path.join(_current_dir_for_jpype_mock_patch, 'mocks')
-        # if _mocks_dir_for_jpype_mock_patch not in sys.path:
-        #     sys.path.insert(0, _mocks_dir_for_jpype_mock_patch)
-        #     _conftest_setup_logger.info(f"Ajout de {_mocks_dir_for_jpype_mock_patch} à sys.path pour jpype_mock.")
-
-        from .mocks import jpype_mock # Importer le module mock principal
-        from .mocks.jpype_components.imports import imports_module as actual_mock_jpype_imports_module
-
-        # Préparer l'objet mock principal pour 'jpype'
-        _jpype_module_mock_obj = MagicMock(name="jpype_module_mock_from_conftest")
-        _jpype_module_mock_obj.__path__ = [] # Nécessaire pour simuler un package
-        _jpype_module_mock_obj.isJVMStarted = jpype_mock.isJVMStarted
-        _jpype_module_mock_obj.startJVM = jpype_mock.startJVM
-        _jpype_module_mock_obj.getJVMPath = jpype_mock.getJVMPath
-        _jpype_module_mock_obj.getJVMVersion = jpype_mock.getJVMVersion
-        _jpype_module_mock_obj.getDefaultJVMPath = jpype_mock.getDefaultJVMPath
-        _jpype_module_mock_obj.JClass = jpype_mock.JClass
-        _jpype_module_mock_obj.JException = jpype_mock.JException
-        _jpype_module_mock_obj.JObject = jpype_mock.JObject
-        _jpype_module_mock_obj.JVMNotFoundException = jpype_mock.JVMNotFoundException
-        _jpype_module_mock_obj.__version__ = getattr(jpype_mock, '__version__', '1.x.mock.conftest')
-        _jpype_module_mock_obj.imports = actual_mock_jpype_imports_module
-        # Simuler d'autres attributs/méthodes si nécessaire pour la collecte
-        _jpype_module_mock_obj.config = MagicMock(name="jpype.config_mock_from_conftest")
-        _jpype_module_mock_obj.config.destroy_jvm = True # Comportement par défaut sûr pour un mock
-
-        # Préparer le mock pour '_jpype' (le module C)
-        _mock_dot_jpype_module = jpype_mock._jpype
-
-        # Appliquer les mocks à sys.modules
-        sys.modules['jpype'] = _jpype_module_mock_obj
-        sys.modules['_jpype'] = _mock_dot_jpype_module 
-        sys.modules['jpype._core'] = _mock_dot_jpype_module 
-        sys.modules['jpype.imports'] = actual_mock_jpype_imports_module
-        sys.modules['jpype.config'] = _jpype_module_mock_obj.config
-        
-        _mock_types_module = MagicMock(name="jpype.types_mock_from_conftest")
-        for type_name in ["JString", "JArray", "JObject", "JBoolean", "JInt", "JDouble", "JLong", "JFloat", "JShort", "JByte", "JChar"]:
-             setattr(_mock_types_module, type_name, getattr(jpype_mock, type_name, MagicMock(name=f"Mock{type_name}")))
-        sys.modules['jpype.types'] = _mock_types_module
-        sys.modules['jpype.JProxy'] = MagicMock(name="jpype.JProxy_mock_from_conftest")
-
-        _conftest_setup_logger.info("Mock JPype appliqué à sys.modules DEPUIS conftest.py.")
-
-    except ImportError as e_mock_load:
-        _conftest_setup_logger.error(f"conftest.py: ERREUR CRITIQUE lors du chargement des mocks JPype (jpype_mock ou jpype_components): {e_mock_load}. Le mock JPype pourrait ne pas être actif.")
-    except Exception as e_patching:
-        _conftest_setup_logger.error(f"conftest.py: Erreur inattendue lors du patching de JPype: {e_patching}", exc_info=True)
-else:
-    _conftest_setup_logger.info("conftest.py: _SHOULD_USE_REAL_JPYPE est True. Aucun mock JPype appliqué au niveau module depuis conftest.py.")
-# --- Fin Patching JPype Mock ---
-# # --- Gestion des imports conditionnels NumPy et Pandas ---
-# _conftest_setup_logger.info("Début de la gestion des imports conditionnels pour NumPy et Pandas.")
-# try:
-#     import numpy
-#     import pandas
-#     _conftest_setup_logger.info("NumPy et Pandas réels importés avec succès.")
-# except ImportError:
-#     _conftest_setup_logger.warning("Échec de l'import de NumPy et/ou Pandas. Tentative d'utilisation des mocks.")
-    
-#     # Mock pour NumPy
-#     try:
-#         # Tenter d'importer le contenu spécifique du mock si disponible
-#         from tests.mocks.numpy_mock import array as numpy_array_mock # Importer un élément spécifique pour vérifier
-#         # Si l'import ci-dessus fonctionne, on peut supposer que le module mock est complet
-#         # et sera utilisé par les imports suivants dans le code testé.
-#         # Cependant, pour forcer l'utilisation du mock complet, on le met dans sys.modules.
-#         import tests.mocks.numpy_mock as numpy_mock_content
-#         sys.modules['numpy'] = numpy_mock_content
-#         _conftest_setup_logger.info("Mock pour NumPy (tests.mocks.numpy_mock) activé via sys.modules.")
-#     except ImportError:
-#         _conftest_setup_logger.error("Mock spécifique tests.mocks.numpy_mock non trouvé. Utilisation de MagicMock pour NumPy.")
-#         sys.modules['numpy'] = MagicMock()
-#     except Exception as e_numpy_mock:
-#         _conftest_setup_logger.error(f"Erreur inattendue lors du chargement du mock NumPy: {e_numpy_mock}. Utilisation de MagicMock.")
-#         sys.modules['numpy'] = MagicMock()
-
-#     # Mock pour Pandas
-#     try:
-#         # Tenter d'importer le contenu spécifique du mock
-#         from tests.mocks.pandas_mock import DataFrame as pandas_dataframe_mock # Importer un élément spécifique
-#         import tests.mocks.pandas_mock as pandas_mock_content
-#         sys.modules['pandas'] = pandas_mock_content
-#         _conftest_setup_logger.info("Mock pour Pandas (tests.mocks.pandas_mock) activé via sys.modules.")
-#     except ImportError:
-#         _conftest_setup_logger.error("Mock spécifique tests.mocks.pandas_mock non trouvé. Utilisation de MagicMock pour Pandas.")
-#         sys.modules['pandas'] = MagicMock()
-#     except Exception as e_pandas_mock:
-#         _conftest_setup_logger.error(f"Erreur inattendue lors du chargement du mock Pandas: {e_pandas_mock}. Utilisation de MagicMock.")
-#         sys.modules['pandas'] = MagicMock()
-# _conftest_setup_logger.info("Fin de la gestion des imports conditionnels pour NumPy et Pandas.")
-# # --- Fin Gestion des imports conditionnels ---
-# --- Fin Configuration globale du Logging ---
-
-# --- Gestion du Path pour les Mocks (déplacé ici AVANT les imports des mocks) ---
-current_dir_for_mock = os.path.dirname(os.path.abspath(__file__))
-mocks_dir_for_mock = os.path.join(current_dir_for_mock, 'mocks')
-# if mocks_dir_for_mock not in sys.path:
-#     sys.path.insert(0, mocks_dir_for_mock)
-#     _conftest_setup_logger.info(f"Ajout de {mocks_dir_for_mock} à sys.path pour l'accès aux mocks locaux.")
-
-from .mocks.jpype_setup import (
-    _REAL_JPYPE_MODULE,
-    _REAL_JPYPE_AVAILABLE, # Ajouté pour skipif
-    _JPYPE_MODULE_MOCK_OBJ_GLOBAL,
-    _MOCK_DOT_JPYPE_MODULE_GLOBAL,
-    activate_jpype_mock_if_needed,
-    pytest_sessionstart,
-    pytest_sessionfinish
-)
-from .mocks.numpy_setup import setup_numpy_for_tests_fixture
-
-from .fixtures.integration_fixtures import (
-    integration_jvm, dung_classes, dl_syntax_parser, fol_syntax_parser,
-    pl_syntax_parser, cl_syntax_parser, tweety_logics_classes,
-    tweety_string_utils, tweety_math_utils, tweety_probability,
-    tweety_conditional_probability, tweety_parser_exception,
-    tweety_io_exception, tweety_qbf_classes, belief_revision_classes,
-    dialogue_classes
-)
-
-# --- Configuration du Logger (déplacé avant la sauvegarde JPype pour l'utiliser) ---
-logger = logging.getLogger(__name__)
-
-# _REAL_JPYPE_MODULE, _JPYPE_MODULE_MOCK_OBJ_GLOBAL, _MOCK_DOT_JPYPE_MODULE_GLOBAL sont maintenant importés de jpype_setup.py
-
-# Nécessaire pour la fixture integration_jvm
-# La variable _integration_jvm_started_session_scope et les imports de jvm_setup
-# ne sont plus nécessaires ici, gérés dans integration_fixtures.py
-
-# Les sections de code commentées pour le mocking global de Matplotlib, NetworkX,
-# l'installation immédiate de Pandas, et ExtractDefinitions ont été supprimées.
-# Ces mocks, s'ils sont nécessaires, devraient être gérés par des fixtures spécifiques
-# ou une configuration au niveau du module mock lui-même, similaire à NumPy/Pandas.
-
-# Ajout du répertoire racine du projet à sys.path pour assurer la découverte des modules du projet.
-# Ceci est particulièrement utile si les tests sont exécutés d'une manière où le répertoire racine
-# n'est pas automatiquement inclus dans PYTHONPATH (par exemple, exécution directe de pytest
-# depuis un sous-répertoire ou avec certaines configurations d'IDE).
-parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
-if parent_dir not in sys.path:
-    sys.path.insert(0, parent_dir)
-    _conftest_setup_logger.info(f"Ajout du répertoire racine du projet ({parent_dir}) à sys.path.")
-# Décommenté car l'environnement de test actuel en a besoin pour trouver les modules locaux.
-
-# Les fixtures et hooks sont importés depuis leurs modules dédiés.
-# Les commentaires résiduels concernant les déplacements de code et les refactorisations
-# antérieures ont été supprimés pour améliorer la lisibilité.
-
-# --- Fixtures déplacées depuis tests/integration/webapp/conftest.py ---
-
-@pytest.fixture
-def webapp_config():
-    """Provides a basic webapp configuration dictionary."""
-    return {
-        "backend": {
-            "start_port": 8008,
-            "fallback_ports": [8009, 8010]
-        },
-        "frontend": {
-            "port": 3008
-        },
-        "playwright": {
-            "enabled": True
-        }
-    }
 
-@pytest.fixture
-def test_config_path(tmp_path):
-    """Provides a temporary path for a config file."""
-    return tmp_path / "test_config.yml"
+# --- Hooks Pytest ---
+
+def pytest_sessionstart(session):
+    """
+    S'exécute au début de la session de test.
+    Initialise le mock JPype si nécessaire.
+    """
+    # Exposer jpype_setup au reste de la session de test si nécessaire
+    # session.config.jpype_setup = jpype_setup
+    print("pytest_sessionstart: Démarrage de la session de test.")
+
+def pytest_sessionfinish(session, exitstatus):
+    """
+    S'exécute à la fin de la session de test.
+    Nettoie les ressources, comme la JVM.
+    """
+    print(f"pytest_sessionfinish: Fin de la session de test avec le statut {exitstatus}.")
+    # Si jpype_setup a ete utilise et a demarre la JVM, l'arreter.
+    # from .mocks import jpype_setup # Import local pour eviter les problemes de dependance circulaire
+    # jpype_setup.shutdown_jvm_if_needed()
+
+# --- Fixtures ---
+
+# Importer les fixtures depuis jpype_setup pour les rendre disponibles globalement
+# Les fixtures seront importées dynamiquement pour éviter les problèmes d'import circulaire.
+try:
+    from .mocks.jpype_setup import mock_jpype, restore_jpype, real_jpype_fixture
+    print("Fixtures JPype importées avec succès depuis tests.mocks.jpype_setup")
+except ImportError as e:
+    print(f"Avertissement: Impossible d'importer les fixtures JPype: {e}")
+    # Définir des fixtures vides pour éviter les erreurs si l'import échoue
+    @pytest.fixture(scope="session")
+    def mock_jpype():
+        yield
+    @pytest.fixture(scope="session")
+    def restore_jpype():
+        yield
+    @pytest.fixture(scope="session")
+    def real_jpype_fixture():
+        yield None
diff --git a/tests/mocks/jpype_setup.py b/tests/mocks/jpype_setup.py
index 62cad7cb..768af7e9 100644
--- a/tests/mocks/jpype_setup.py
+++ b/tests/mocks/jpype_setup.py
@@ -1,378 +1,130 @@
+# tests/mocks/jpype_setup.py
+"""
+Ce module fournit des fixtures pytest pour gérer le cycle de vie de JPype
+et de la JVM, permettant de basculer entre un mock complet et la vraie bibliothèque.
+"""
+
 import sys
 import os
 import pytest
-from unittest.mock import MagicMock
+from unittest.mock import MagicMock, patch
 import importlib.util
-from argumentation_analysis.core.jvm_setup import shutdown_jvm_if_needed
+from argumentation_analysis.core.jvm_setup import shutdown_jvm_if_needed, start_jvm_if_needed, is_jvm_started
 import logging
 
-# --- Configuration du Logger ---
+# Configuration du logging
 logger = logging.getLogger(__name__)
-# Configuration basique si le logger n'est pas déjà configuré par pytest ou autre
-if not logger.handlers:
-    handler = logging.StreamHandler(sys.stdout)
-    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
-    handler.setFormatter(formatter)
-    logger.addHandler(handler)
-    logger.setLevel(logging.INFO) # Ou logging.DEBUG pour plus de détails
-    logger.propagate = False
-
-# --- Détermination de la disponibilité du vrai JPype via variable d'environnement ---
-# Cette variable est utilisée par les décorateurs skipif dans les fichiers de test.
-logger.info(f"jpype_setup.py: Évaluation de _REAL_JPYPE_AVAILABLE...")
-logger.info(f"jpype_setup.py: Valeur brute de os.environ.get('USE_REAL_JPYPE', 'false'): '{os.environ.get('USE_REAL_JPYPE', 'false')}'")
-_REAL_JPYPE_AVAILABLE = os.environ.get('USE_REAL_JPYPE', 'false').lower() in ('true', '1')
-logger.info(f"jpype_setup.py: _REAL_JPYPE_AVAILABLE évalué à: {_REAL_JPYPE_AVAILABLE}")
-# Les prints de débogage précédents ont confirmé que _REAL_JPYPE_AVAILABLE est correctement évalué.
-# La cause du skip était une erreur dans la fixture integration_jvm (chemin des libs).
-
-
-# --- Sauvegarde du module JPype potentiellement pré-importé ou import frais ---
-_REAL_JPYPE_MODULE = None
-_PRE_EXISTING_JPYPE_IN_SYS_MODULES = sys.modules.get('jpype')
-
-if _PRE_EXISTING_JPYPE_IN_SYS_MODULES:
-    _REAL_JPYPE_MODULE = _PRE_EXISTING_JPYPE_IN_SYS_MODULES
-    logger.info(f"jpype_setup.py: _REAL_JPYPE_MODULE initialisé à partir de _PRE_EXISTING_JPYPE_IN_SYS_MODULES (ID: {id(_REAL_JPYPE_MODULE)}).")
-else:
-    logger.info("jpype_setup.py: JPype non préchargé, tentative d'import frais.")
-    try:
-        import jpype as r_jpype_fresh_import
-        _REAL_JPYPE_MODULE = r_jpype_fresh_import
-        logger.info(f"jpype_setup.py: Vrai module JPype importé fraîchement (ID: {id(_REAL_JPYPE_MODULE)}).")
-    except ImportError as e_fresh_import:
-        logger.warning(f"jpype_setup.py: Le vrai module JPype n'a pas pu être importé fraîchement: {e_fresh_import}")
-        _REAL_JPYPE_MODULE = None
-    except NameError as e_name_error_fresh_import:
-        logger.error(f"jpype_setup.py: NameError lors de l'import frais de JPype: {e_name_error_fresh_import}.")
-        _REAL_JPYPE_MODULE = None
-
-if _REAL_JPYPE_MODULE is None:
-    logger.error("jpype_setup.py: _REAL_JPYPE_MODULE EST NONE après la tentative d'initialisation.")
-else:
-    logger.info(f"jpype_setup.py: _REAL_JPYPE_MODULE est initialisé (ID: {id(_REAL_JPYPE_MODULE)}) avant la définition des fixtures.")
-
-# --- Mock JPype ---
-try:
-    from tests.mocks import jpype_mock # Importer le module via son chemin de package
-    # Importer le vrai module mock d'imports depuis le sous-package jpype_components
-    from tests.mocks.jpype_components.imports import imports_module as actual_mock_jpype_imports_module
 
-    jpype_module_mock_obj = MagicMock(name="jpype_module_mock")
-    jpype_module_mock_obj.__path__ = []
-    jpype_module_mock_obj.isJVMStarted = jpype_mock.isJVMStarted
-    jpype_module_mock_obj.startJVM = jpype_mock.startJVM
-    jpype_module_mock_obj.getJVMPath = jpype_mock.getJVMPath
-    jpype_module_mock_obj.getJVMVersion = jpype_mock.getJVMVersion
-    jpype_module_mock_obj.getDefaultJVMPath = jpype_mock.getDefaultJVMPath
-    jpype_module_mock_obj.JClass = jpype_mock.JClass
-    jpype_module_mock_obj.JException = jpype_mock.JException
-    jpype_module_mock_obj.JObject = jpype_mock.JObject
-    jpype_module_mock_obj.JVMNotFoundException = jpype_mock.JVMNotFoundException
-    jpype_module_mock_obj.__version__ = '1.4.1.mock' # ou jpype_mock.__version__ si défini
-    jpype_module_mock_obj.imports = actual_mock_jpype_imports_module
-    _JPYPE_MODULE_MOCK_OBJ_GLOBAL = jpype_module_mock_obj
-    _MOCK_DOT_JPYPE_MODULE_GLOBAL = jpype_mock._jpype # Accéder à _jpype depuis le module jpype_mock importé
-    logger.info("jpype_setup.py: Mock JPype préparé.")
-except ImportError as e_jpype:
-    logger.error(f"jpype_setup.py: ERREUR CRITIQUE lors de l'import de jpype_mock ou ses composants: {e_jpype}. Utilisation de mocks de fallback pour JPype.")
-    _fb_jpype_mock = MagicMock(name="jpype_fallback_mock")
-    _fb_jpype_mock.imports = MagicMock(name="jpype.imports_fallback_mock")
-    _fb_dot_jpype_mock = MagicMock(name="_jpype_fallback_mock")
+# --- Configuration Globale ---
 
-    _JPYPE_MODULE_MOCK_OBJ_GLOBAL = _fb_jpype_mock
-    _MOCK_DOT_JPYPE_MODULE_GLOBAL = _fb_dot_jpype_mock
-    logger.info("jpype_setup.py: Mock JPype de FALLBACK préparé et assigné aux variables globales de mock.")
+_use_real_jpype = os.environ.get('USE_REAL_JPYPE', 'false').lower() in ('true', '1')
+_jpype_mock = None
+_jpype_patcher = None
 
+# --- Fonctions de Hook Pytest (si utilisées directement dans ce module) ---
 
-@pytest.fixture(scope="function", autouse=True)
-def activate_jpype_mock_if_needed(request):
-    global _JPYPE_MODULE_MOCK_OBJ_GLOBAL, _MOCK_DOT_JPYPE_MODULE_GLOBAL, _REAL_JPYPE_MODULE
-
-    # Déterminer si le vrai JPype doit être utilisé
-    env_use_real_jpype = os.environ.get('USE_REAL_JPYPE', 'false').lower() in ('true', '1')
+def pytest_sessionstart(session):
+    """
+    Démarre la JVM si USE_REAL_JPYPE est activé, sinon prépare le mock.
+    """
+    global _jpype_mock, _jpype_patcher
     
-    use_real_jpype_marker = False
-    if request.node.get_closest_marker("real_jpype"):
-        use_real_jpype_marker = True
+    if _use_real_jpype:
+        logger.info("Démarrage de la JVM pour la session de test.")
+        try:
+            start_jvm_if_needed()
+        except Exception as e:
+            logger.error(f"Erreur lors du démarrage de la JVM: {e}")
+            pytest.exit(f"Impossible de démarrer la JVM: {e}")
+    else:
+        logger.info("Configuration du mock JPype pour la session de test.")
+        _jpype_mock = MagicMock()
         
-    use_real_jpype_path = False
-    path_str = str(request.node.fspath).replace(os.sep, '/')
-    if 'tests/integration/' in path_str or 'tests/minimal_jpype_tweety_tests/' in path_str:
-        use_real_jpype_path = True
+        # Simuler les comportements de base du module jpype
+        _jpype_mock.isJVMStarted.return_value = True
+        _jpype_mock.JClass.return_value = MagicMock()
+        _jpype_mock.JException = Exception # Simuler les exceptions Java
         
-    final_use_real_jpype = False
-    if env_use_real_jpype:
-        final_use_real_jpype = True
-        logger.info(f"Test {request.node.name}: REAL JPype forcé par la variable d'environnement USE_REAL_JPYPE.")
-    elif use_real_jpype_marker:
-        final_use_real_jpype = True
-        logger.info(f"Test {request.node.name}: REAL JPype demandé par le marqueur 'real_jpype'.")
-    elif use_real_jpype_path:
-        final_use_real_jpype = True
-        logger.info(f"Test {request.node.name}: REAL JPype activé par chemin ({path_str}).")
-    # else: final_use_real_jpype reste False
+        # Patcher le module jpype dans le système
+        _jpype_patcher = patch('sys.modules', {**sys.modules, 'jpype': _jpype_mock})
+        _jpype_patcher.start()
 
-    if final_use_real_jpype:
-        logger.info(f"Test {request.node.name} demande REAL JPype. Configuration de sys.modules pour utiliser le vrai JPype.")
-        if _REAL_JPYPE_MODULE:
-            sys.modules['jpype'] = _REAL_JPYPE_MODULE
-            if hasattr(_REAL_JPYPE_MODULE, '_jpype'):
-                sys.modules['_jpype'] = _REAL_JPYPE_MODULE._jpype
-            elif '_jpype' in sys.modules and sys.modules.get('_jpype') is not getattr(_REAL_JPYPE_MODULE, '_jpype', None) :
-                del sys.modules['_jpype']
-            if hasattr(_REAL_JPYPE_MODULE, 'imports'):
-                sys.modules['jpype.imports'] = _REAL_JPYPE_MODULE.imports
-            elif 'jpype.imports' in sys.modules and sys.modules.get('jpype.imports') is not getattr(_REAL_JPYPE_MODULE, 'imports', None):
-                del sys.modules['jpype.imports']
-            logger.debug(f"REAL JPype (ID: {id(_REAL_JPYPE_MODULE)}) est maintenant sys.modules['jpype'].")
-        else:
-            logger.error(f"Test {request.node.name} demande REAL JPype, mais _REAL_JPYPE_MODULE n'est pas disponible. Test échouera probablement.")
-        yield
-    else:
-        logger.info(f"Test {request.node.name} utilise MOCK JPype.")
-        try:
-            jpype_components_jvm_module = sys.modules.get('tests.mocks.jpype_components.jvm')
-            if jpype_components_jvm_module:
-                if hasattr(jpype_components_jvm_module, '_jvm_started'):
-                    jpype_components_jvm_module._jvm_started = False
-                if hasattr(jpype_components_jvm_module, '_jvm_path'):
-                    jpype_components_jvm_module._jvm_path = None
-                if hasattr(_JPYPE_MODULE_MOCK_OBJ_GLOBAL, 'config') and hasattr(_JPYPE_MODULE_MOCK_OBJ_GLOBAL.config, 'jvm_path'):
-                    _JPYPE_MODULE_MOCK_OBJ_GLOBAL.config.jvm_path = None
-                logger.info("État (_jvm_started, _jvm_path, config.jvm_path) du mock JPype réinitialisé pour le test.")
-            else:
-                logger.warning("Impossible de réinitialiser l'état du mock JPype: module 'tests.mocks.jpype_components.jvm' non trouvé.")
-        except Exception as e_reset_mock:
-            logger.error(f"Erreur lors de la réinitialisation de l'état du mock JPype: {e_reset_mock}")
-
-        original_modules = {}
-        modules_to_handle = ['jpype', '_jpype', 'jpype._core', 'jpype.imports', 'jpype.types', 'jpype.config', 'jpype.JProxy']
 
-        if 'jpype.imports' in sys.modules and \
-           hasattr(sys.modules['jpype.imports'], '_jpype') and \
-           _MOCK_DOT_JPYPE_MODULE_GLOBAL is not None and \
-           hasattr(_MOCK_DOT_JPYPE_MODULE_GLOBAL, 'isStarted'):
-            if sys.modules['jpype.imports']._jpype is not _MOCK_DOT_JPYPE_MODULE_GLOBAL:
-                if 'jpype.imports._jpype_original' not in original_modules:
-                     original_modules['jpype.imports._jpype_original'] = sys.modules['jpype.imports']._jpype
-                logger.debug(f"Patch direct de sys.modules['jpype.imports']._jpype avec notre mock _jpype.")
-                sys.modules['jpype.imports']._jpype = _MOCK_DOT_JPYPE_MODULE_GLOBAL
-            else:
-                logger.debug("sys.modules['jpype.imports']._jpype est déjà notre mock.")
-
-        for module_name in modules_to_handle:
-            if module_name in sys.modules:
-                is_current_module_our_mock = False
-                if module_name == 'jpype' and sys.modules[module_name] is _JPYPE_MODULE_MOCK_OBJ_GLOBAL: is_current_module_our_mock = True
-                elif module_name in ['_jpype', 'jpype._core'] and sys.modules[module_name] is _MOCK_DOT_JPYPE_MODULE_GLOBAL: is_current_module_our_mock = True
-                elif module_name == 'jpype.imports' and hasattr(_JPYPE_MODULE_MOCK_OBJ_GLOBAL, 'imports') and sys.modules[module_name] is _JPYPE_MODULE_MOCK_OBJ_GLOBAL.imports: is_current_module_our_mock = True
-                elif module_name == 'jpype.config' and hasattr(_JPYPE_MODULE_MOCK_OBJ_GLOBAL, 'config') and sys.modules[module_name] is _JPYPE_MODULE_MOCK_OBJ_GLOBAL.config: is_current_module_our_mock = True
-
-                if not is_current_module_our_mock and module_name not in original_modules:
-                    original_modules[module_name] = sys.modules.pop(module_name)
-                    logger.debug(f"Supprimé et sauvegardé sys.modules['{module_name}']")
-                elif module_name in sys.modules and is_current_module_our_mock:
-                    del sys.modules[module_name]
-                    logger.debug(f"Supprimé notre mock préexistant pour sys.modules['{module_name}'].")
-                elif module_name in sys.modules:
-                    del sys.modules[module_name]
-                    logger.debug(f"Supprimé sys.modules['{module_name}'] (sauvegarde prioritaire existante).")
+def pytest_sessionfinish(session, exitstatus):
+    """
+    Arrête la JVM si elle a été démarrée pour les tests.
+    """
+    global _jpype_patcher
+    
+    if _use_real_jpype:
+        logger.info("Arrêt de la JVM à la fin de la session de test.")
+        shutdown_jvm_if_needed()
+    elif _jpype_patcher:
+        logger.info("Arrêt du patcher JPype.")
+        _jpype_patcher.stop()
+
+# --- Fixtures Pytest ---
+
+@pytest.fixture(scope="session")
+def real_jpype_fixture():
+    """
+    Fixture qui gère le cycle de vie de la vraie JVM pour toute la session de test.
+    Ne fait rien si USE_REAL_JPYPE est désactivé.
+    """
+    if not _use_real_jpype:
+        yield None
+        return
+
+    # La gestion est faite par les hooks pytest_sessionstart/finish
+    yield
+    
 
-        sys.modules['jpype'] = _JPYPE_MODULE_MOCK_OBJ_GLOBAL
-        sys.modules['_jpype'] = _MOCK_DOT_JPYPE_MODULE_GLOBAL
-        sys.modules['jpype._core'] = _MOCK_DOT_JPYPE_MODULE_GLOBAL
-        if hasattr(_JPYPE_MODULE_MOCK_OBJ_GLOBAL, 'imports'):
-            sys.modules['jpype.imports'] = _JPYPE_MODULE_MOCK_OBJ_GLOBAL.imports
-        else:
-            sys.modules['jpype.imports'] = MagicMock(name="jpype.imports_fallback_in_fixture")
+@pytest.fixture(scope="function")
+def mock_jpype(request):
+    """
+    Fixture pour mocker JPype au niveau de la fonction, permettant une configuration
+    spécifique au test.
+    """
+    if _use_real_jpype:
+        pytest.skip("Test incompatible avec la vraie JVM. Utilise un mock.")
 
-        if hasattr(_JPYPE_MODULE_MOCK_OBJ_GLOBAL, 'config'):
-            sys.modules['jpype.config'] = _JPYPE_MODULE_MOCK_OBJ_GLOBAL.config
-        else:
-            sys.modules['jpype.config'] = MagicMock(name="jpype.config_fallback_in_fixture")
+    start_path = 'sys.modules'
+    
+    # Création du mock
+    jpype_mock = MagicMock()
+    jpype_mock.isJVMStarted.return_value = True
+    jpype_mock.JClass.return_value = MagicMock()
+    
+    # Patcher le module jpype
+    patcher = patch.dict(start_path, {'jpype': jpype_mock})
+    patcher.start()
+    
+    yield jpype_mock
+    
+    # Nettoyage après le test
+    patcher.stop()
 
-        mock_types_module = MagicMock(name="jpype.types_mock_module_dynamic_in_fixture")
-        for type_name in ["JString", "JArray", "JObject", "JBoolean", "JInt", "JDouble", "JLong", "JFloat", "JShort", "JByte", "JChar"]:
-            if hasattr(_JPYPE_MODULE_MOCK_OBJ_GLOBAL, type_name):
-                setattr(mock_types_module, type_name, getattr(_JPYPE_MODULE_MOCK_OBJ_GLOBAL, type_name))
-            else:
-                setattr(mock_types_module, type_name, MagicMock(name=f"Mock{type_name}_in_fixture"))
-        sys.modules['jpype.types'] = mock_types_module
 
-        sys.modules['jpype.JProxy'] = MagicMock(name="jpype.JProxy_mock_module_dynamic_in_fixture")
-        logger.debug(f"Mocks JPype (principal, _jpype/_core, imports, config, types, JProxy) mis en place.")
+@pytest.fixture(scope="session", autouse=True)
+def manage_global_jpype_mock():
+    """
+    Fixture auto-utilisée pour gérer le mock JPype global pour la session.
+    """
+    if _use_real_jpype:
         yield
-        logger.debug(f"Nettoyage après test {request.node.name} (utilisation du mock).")
+        return
 
-        if 'jpype.imports._jpype_original' in original_modules:
-            if 'jpype.imports' in sys.modules and hasattr(sys.modules['jpype.imports'], '_jpype'):
-                sys.modules['jpype.imports']._jpype = original_modules['jpype.imports._jpype_original']
-                logger.debug("Restauré jpype.imports._jpype à sa valeur originale.")
-            del original_modules['jpype.imports._jpype_original']
+    # Création et configuration du mock global
+    global_jpype_mock = MagicMock(name="GlobalJPypeMock")
+    global_jpype_mock.isJVMStarted.return_value = True
+    global_jpype_mock.JClass.return_value = MagicMock()
+    global_jpype_mock.JException = Exception
 
-        modules_we_set_up_in_fixture = ['jpype', '_jpype', 'jpype._core', 'jpype.imports', 'jpype.config', 'jpype.types', 'jpype.JProxy']
-        for module_name in modules_we_set_up_in_fixture:
-            current_module_in_sys = sys.modules.get(module_name)
-            is_our_specific_mock_from_fixture = False
-            if module_name == 'jpype' and current_module_in_sys is _JPYPE_MODULE_MOCK_OBJ_GLOBAL: is_our_specific_mock_from_fixture = True
-            elif module_name in ['_jpype', 'jpype._core'] and current_module_in_sys is _MOCK_DOT_JPYPE_MODULE_GLOBAL: is_our_specific_mock_from_fixture = True
-            elif module_name == 'jpype.imports' and hasattr(_JPYPE_MODULE_MOCK_OBJ_GLOBAL, 'imports') and current_module_in_sys is _JPYPE_MODULE_MOCK_OBJ_GLOBAL.imports: is_our_specific_mock_from_fixture = True
-            elif module_name == 'jpype.config' and hasattr(_JPYPE_MODULE_MOCK_OBJ_GLOBAL, 'config') and current_module_in_sys is _JPYPE_MODULE_MOCK_OBJ_GLOBAL.config: is_our_specific_mock_from_fixture = True
-            elif module_name == 'jpype.types' and current_module_in_sys is mock_types_module: is_our_specific_mock_from_fixture = True
-            elif module_name == 'jpype.JProxy' and isinstance(current_module_in_sys, MagicMock) and hasattr(current_module_in_sys, 'name') and "jpype.JProxy_mock_module_dynamic_in_fixture" in current_module_in_sys.name : is_our_specific_mock_from_fixture = True
+    # Patcher sys.modules pour que toute importation de 'jpype' retourne notre mock
+    patcher = patch.dict(sys.modules, {'jpype': global_jpype_mock})
+    patcher.start()
 
-            if is_our_specific_mock_from_fixture:
-                if module_name in sys.modules:
-                    del sys.modules[module_name]
-                    logger.debug(f"Supprimé notre mock pour sys.modules['{module_name}']")
+    yield global_jpype_mock
 
-        for module_name, original_module in original_modules.items():
-            sys.modules[module_name] = original_module
-            logger.debug(f"Restauré sys.modules['{module_name}'] à {original_module}")
-
-        logger.info(f"État de JPype restauré après test {request.node.name} (utilisation du mock).")
-
-def pytest_sessionstart(session):
-    global _REAL_JPYPE_MODULE, _JPYPE_MODULE_MOCK_OBJ_GLOBAL, logger
-    logger.info("jpype_setup.py: pytest_sessionstart hook triggered.")
-    if not hasattr(logger, 'info'):
-        import logging
-        logger = logging.getLogger(__name__)
-
-    if _REAL_JPYPE_MODULE and _REAL_JPYPE_MODULE is not _JPYPE_MODULE_MOCK_OBJ_GLOBAL:
-        logger.info("   pytest_sessionstart: Real JPype module is available.")
-        # try:
-            # La logique de configuration de destroy_jvm et l'import de jpype.config
-            # sont maintenant gérés de manière centralisée par initialize_jvm lors du premier démarrage réel.
-            # Commenter cette section pour éviter les conflits ou les configurations prématurées.
-            # original_sys_jpype_module = sys.modules.get('jpype')
-            # if sys.modules.get('jpype') is not _REAL_JPYPE_MODULE:
-            #     sys.modules['jpype'] = _REAL_JPYPE_MODULE
-            #     logger.info("   pytest_sessionstart: Temporarily set sys.modules['jpype'] to _REAL_JPYPE_MODULE for config import.")
-
-            # if not hasattr(_REAL_JPYPE_MODULE, 'config') or _REAL_JPYPE_MODULE.config is None:
-            #     logger.info("   pytest_sessionstart: Attempting to import jpype.config explicitly.")
-            #     import jpype.config # This might be problematic if called before JVM start or with wrong classpath context
-            
-            # if original_sys_jpype_module is not None and sys.modules.get('jpype') is not original_sys_jpype_module:
-            #     sys.modules['jpype'] = original_sys_jpype_module
-            #     logger.info("   pytest_sessionstart: Restored original sys.modules['jpype'].")
-            # elif original_sys_jpype_module is None and 'jpype' in sys.modules and sys.modules['jpype'] is _REAL_JPYPE_MODULE:
-            #     pass # It was correctly set
-            
-            # Tentative d'assurer que jpype.config est le vrai config, si possible.
-            # initialize_jvm s'occupera de mettre destroy_jvm à False.
-            # Bloc try/except correctement indenté :
-        try:
-            if not hasattr(_REAL_JPYPE_MODULE, 'config') or _REAL_JPYPE_MODULE.config is None:
-                logger.info("   pytest_sessionstart: _REAL_JPYPE_MODULE.config non trouvé, tentative d'import de jpype.config.")
-                _current_sys_jpype = sys.modules.get('jpype')
-                sys.modules['jpype'] = _REAL_JPYPE_MODULE
-                import jpype.config
-                sys.modules['jpype'] = _current_sys_jpype
-                logger.info(f"   pytest_sessionstart: Import de jpype.config tenté. hasattr(config): {hasattr(_REAL_JPYPE_MODULE, 'config')}")
-
-            if hasattr(_REAL_JPYPE_MODULE, 'config') and _REAL_JPYPE_MODULE.config is not None:
-                if 'jpype.config' not in sys.modules or sys.modules.get('jpype.config') is not _REAL_JPYPE_MODULE.config:
-                    sys.modules['jpype.config'] = _REAL_JPYPE_MODULE.config
-                    logger.info("   pytest_sessionstart: Assuré que sys.modules['jpype.config'] est _REAL_JPYPE_MODULE.config.")
-            else:
-                logger.warning("   pytest_sessionstart: _REAL_JPYPE_MODULE.config toujours non disponible après tentative d'import.")
-
-        except ImportError as e_cfg_imp_sess_start:
-            logger.error(f"   pytest_sessionstart: ImportError lors de la tentative d'import de jpype.config: {e_cfg_imp_sess_start}")
-        except Exception as e_sess_start_cfg:
-            logger.error(f"   pytest_sessionstart: Erreur inattendue lors de la manipulation de jpype.config: {e_sess_start_cfg}", exc_info=True)
-
-        logger.info("   pytest_sessionstart: La configuration de jpype.config.destroy_jvm est gérée par initialize_jvm.")
-    elif _JPYPE_MODULE_MOCK_OBJ_GLOBAL and _REAL_JPYPE_MODULE is _JPYPE_MODULE_MOCK_OBJ_GLOBAL:
-        logger.info("   pytest_sessionstart: JPype module is the MOCK. No changes to destroy_jvm needed for the mock.")
-    else:
-        logger.info("   pytest_sessionstart: Real JPype module not definitively available or identified as mock. La configuration de jpype.config est gérée par initialize_jvm.")
-
-def pytest_sessionfinish(session, exitstatus):
-    global _REAL_JPYPE_MODULE, _JPYPE_MODULE_MOCK_OBJ_GLOBAL, logger
-    logger.info(f"jpype_setup.py: pytest_sessionfinish hook triggered. Exit status: {exitstatus}")
-
-    # Déterminer si le vrai JPype a été utilisé pour la session ou le dernier test
-    # Cela est une heuristique. Idéalement, on saurait si la JVM a été démarrée par notre code.
-    real_jpype_was_potentially_used = False
-    if _REAL_JPYPE_MODULE and sys.modules.get('jpype') is _REAL_JPYPE_MODULE:
-        logger.info("   pytest_sessionfinish: sys.modules['jpype'] IS _REAL_JPYPE_MODULE. Le vrai JPype a potentiellement été utilisé.")
-        real_jpype_was_potentially_used = True
-    elif _REAL_JPYPE_MODULE and _REAL_JPYPE_MODULE is not _JPYPE_MODULE_MOCK_OBJ_GLOBAL:
-        logger.info("   pytest_sessionfinish: _REAL_JPYPE_MODULE est disponible et n'est pas le mock global. Le vrai JPype a potentiellement été utilisé.")
-        real_jpype_was_potentially_used = True
-    else:
-        logger.info("   pytest_sessionfinish: sys.modules['jpype'] n'est pas _REAL_JPYPE_MODULE ou _REAL_JPYPE_MODULE est le mock. Le mock JPype a probablement été utilisé.")
-
-    if real_jpype_was_potentially_used:
-        logger.info("   pytest_sessionfinish: Tentative d'arrêt de la JVM via shutdown_jvm_if_needed() car le vrai JPype a potentiellement été utilisé.")
-        try:
-            # S'assurer que le vrai jpype est dans sys.modules pour que shutdown_jvm_if_needed fonctionne correctement
-            original_jpype_in_sys = sys.modules.get('jpype')
-            if original_jpype_in_sys is not _REAL_JPYPE_MODULE and _REAL_JPYPE_MODULE is not None:
-                logger.info(f"   pytest_sessionfinish: Temporairement, sys.modules['jpype'] = _REAL_JPYPE_MODULE (ID: {id(_REAL_JPYPE_MODULE)}) pour shutdown.")
-                sys.modules['jpype'] = _REAL_JPYPE_MODULE
-            
-            shutdown_jvm_if_needed() # Appel de notre fonction centralisée
-            
-            # Restaurer l'état précédent de sys.modules['jpype'] si modifié
-            if original_jpype_in_sys is not None and sys.modules.get('jpype') is not original_jpype_in_sys:
-                logger.info(f"   pytest_sessionfinish: Restauration de sys.modules['jpype'] à son état original (ID: {id(original_jpype_in_sys)}).")
-                sys.modules['jpype'] = original_jpype_in_sys
-            elif original_jpype_in_sys is None and 'jpype' in sys.modules: # Si on l'a ajouté et qu'il n'y était pas
-                del sys.modules['jpype']
-                logger.info("   pytest_sessionfinish: sys.modules['jpype'] supprimé car il n'était pas là initialement.")
-
-        except Exception as e_shutdown:
-            logger.error(f"   pytest_sessionfinish: Erreur lors de l'appel à shutdown_jvm_if_needed(): {e_shutdown}", exc_info=True)
-        
-        # La logique ci-dessous pour restaurer sys.modules['jpype'] et sys.modules['jpype.config']
-        # est importante si la JVM n'est PAS arrêtée par JPype via atexit (destroy_jvm=False).
-        # Si shutdown_jvm_if_needed() a bien arrêté la JVM, cette partie est moins critique mais ne fait pas de mal.
-        logger.info("   pytest_sessionfinish: Vérification de l'état de la JVM après tentative d'arrêt.")
-        try:
-            jvm_still_started_after_shutdown_attempt = False
-            if _REAL_JPYPE_MODULE and hasattr(_REAL_JPYPE_MODULE, 'isJVMStarted'):
-                 # Assurer que _REAL_JPYPE_MODULE est utilisé pour la vérification
-                _current_jpype_for_check = sys.modules.get('jpype')
-                if _current_jpype_for_check is not _REAL_JPYPE_MODULE and _REAL_JPYPE_MODULE is not None:
-                    sys.modules['jpype'] = _REAL_JPYPE_MODULE
-                jvm_still_started_after_shutdown_attempt = _REAL_JPYPE_MODULE.isJVMStarted()
-                if _current_jpype_for_check is not None and _current_jpype_for_check is not _REAL_JPYPE_MODULE: # restaurer
-                    sys.modules['jpype'] = _current_jpype_for_check
-                elif _current_jpype_for_check is None and 'jpype' in sys.modules and sys.modules['jpype'] is _REAL_JPYPE_MODULE:
-                    del sys.modules['jpype']
-
-
-            logger.info(f"   pytest_sessionfinish: JVM encore démarrée après tentative d'arrêt: {jvm_still_started_after_shutdown_attempt}")
-
-            destroy_jvm_is_false = False # Valeur par défaut si config non accessible
-            if _REAL_JPYPE_MODULE and hasattr(_REAL_JPYPE_MODULE, 'config') and _REAL_JPYPE_MODULE.config is not None and hasattr(_REAL_JPYPE_MODULE.config, 'destroy_jvm'):
-                destroy_jvm_is_false = not _REAL_JPYPE_MODULE.config.destroy_jvm
-            logger.info(f"   pytest_sessionfinish: destroy_jvm est False (selon config): {destroy_jvm_is_false}")
-
-            if jvm_still_started_after_shutdown_attempt and destroy_jvm_is_false:
-                logger.info("   pytest_sessionfinish: JVM est toujours active et destroy_jvm est False. Assurer la présence des modules jpype pour atexit.")
-                current_sys_jpype = sys.modules.get('jpype')
-                if current_sys_jpype is not _REAL_JPYPE_MODULE and _REAL_JPYPE_MODULE is not None:
-                    logger.warning(f"   pytest_sessionfinish: sys.modules['jpype'] (ID: {id(current_sys_jpype)}) n'est pas _REAL_JPYPE_MODULE (ID: {id(_REAL_JPYPE_MODULE)}). Restauration de _REAL_JPYPE_MODULE.")
-                    sys.modules['jpype'] = _REAL_JPYPE_MODULE
-                
-                if _REAL_JPYPE_MODULE and hasattr(_REAL_JPYPE_MODULE, 'config') and _REAL_JPYPE_MODULE.config is not None:
-                    current_sys_jpype_config = sys.modules.get('jpype.config')
-                    if current_sys_jpype_config is not _REAL_JPYPE_MODULE.config:
-                        logger.warning(f"   pytest_sessionfinish: sys.modules['jpype.config'] (ID: {id(current_sys_jpype_config)}) n'est pas _REAL_JPYPE_MODULE.config (ID: {id(_REAL_JPYPE_MODULE.config)}). Restauration.")
-                        sys.modules['jpype.config'] = _REAL_JPYPE_MODULE.config
-                else:
-                    logger.warning("   pytest_sessionfinish: _REAL_JPYPE_MODULE.config non disponible, ne peut pas assurer sys.modules['jpype.config'].")
-            else:
-                logger.info("   pytest_sessionfinish: JVM non démarrée ou destroy_jvm est True. Pas de gestion spéciale de sys.modules pour atexit depuis ici.")
-        except AttributeError as ae:
-             logger.error(f"   pytest_sessionfinish: AttributeError (vérification post-arrêt): {ae}.", exc_info=True)
-        except Exception as e:
-            logger.error(f"   pytest_sessionfinish: Erreur inattendue (vérification post-arrêt): {type(e).__name__}: {e}", exc_info=True)
-    else:
-        logger.info("   pytest_sessionfinish: Le mock JPype a probablement été utilisé. Aucun arrêt de JVM nécessaire depuis ici.")
\ No newline at end of file
+    # Nettoyage à la fin de la session
+    patcher.stop()
\ No newline at end of file
diff --git a/tests/ui/test_extract_definition_persistence.py b/tests/ui/test_extract_definition_persistence.py
index 656ae62b..6cbb9c59 100644
--- a/tests/ui/test_extract_definition_persistence.py
+++ b/tests/ui/test_extract_definition_persistence.py
@@ -105,12 +105,12 @@ def test_load_definitions_encrypted_no_key(test_env):
 def test_load_definitions_encrypted_wrong_key(test_env):
     wrong_key = test_env['crypto_service'].generate_key() # bytes
     
-    # La fonction gère maintenant l'erreur et retourne une liste par défaut.
-    definitions = load_extract_definitions(
-        config_file=test_env['encrypted_definitions_file'],
-        b64_derived_key=wrong_key.decode('utf-8')
-    )
-    assert isinstance(definitions, list)
+    with pytest.raises(InvalidToken):
+        load_extract_definitions(
+            config_file=test_env['encrypted_definitions_file'],
+            b64_derived_key=wrong_key.decode('utf-8'),
+            raise_on_decrypt_error=True
+        )
 
 @pytest.mark.skip("La fonction save_extract_definitions chiffre toujours ; ce test pour la sauvegarde non chiffrée est obsolète.")
 def test_save_definitions_unencrypted(test_env):
@@ -167,6 +167,5 @@ def test_load_malformed_json(test_env):
     with open(malformed_encrypted_file, 'wb') as f:
         f.write(b"this is not encrypted data")
 
-    # La fonction gère maintenant l'erreur et retourne une liste par défaut.
-    definitions = load_extract_definitions(config_file=malformed_encrypted_file, b64_derived_key=test_env['key'].decode('utf-8'))
-    assert isinstance(definitions, list)
\ No newline at end of file
+    with pytest.raises(InvalidToken):
+        load_extract_definitions(config_file=malformed_encrypted_file, b64_derived_key=test_env['key'].decode('utf-8'), raise_on_decrypt_error=True)
\ No newline at end of file
diff --git a/tests/ui/test_utils.py b/tests/ui/test_utils.py
index 31a6fc84..86b0da3d 100644
--- a/tests/ui/test_utils.py
+++ b/tests/ui/test_utils.py
@@ -513,7 +513,7 @@ def test_load_extract_definitions_decryption_fails(mock_decrypt_data_with_fernet
     # Le message vient de load_extract_definitions quand InvalidToken est attrapée
     error_logged = False
     # Le message exact loggué par load_extract_definitions pour InvalidToken attrapée
-    expected_log_part = f"❌ InvalidToken explicitement levée lors du déchiffrement de '{config_file_path}'"
+    expected_log_part = f"❌ Token invalide (InvalidToken) lors du déchiffrement de '{config_file_path}'"
     for call_args_tuple in mock_logger.error.call_args_list: # C'est une erreur maintenant
         args, kwargs = call_args_tuple
         if args and isinstance(args[0], str) and expected_log_part in args[0] and kwargs.get('exc_info') is True:
diff --git a/validation_point2_llm_authentique.py b/validation_point2_llm_authentique.py
new file mode 100644
index 00000000..238cf2a1
--- /dev/null
+++ b/validation_point2_llm_authentique.py
@@ -0,0 +1,383 @@
+#!/usr/bin/env python3
+"""
+VALIDATION POINT 2 - Applications web Flask + React avec vrais LLMs
+================================================================
+
+Script de validation autonome pour Point 2 avec OpenRouter gpt-4o-mini authentique.
+"""
+
+import sys
+import subprocess
+import time
+import requests
+import json
+import os
+from pathlib import Path
+from datetime import datetime
+
+def load_env_file():
+    """Charge le fichier .env"""
+    env_file = Path('.env')
+    if env_file.exists():
+        with open(env_file, 'r', encoding='utf-8') as f:
+            for line in f:
+                line = line.strip()
+                if line and not line.startswith('#') and '=' in line:
+                    key, value = line.split('=', 1)
+                    # Nettoyer les guillemets
+                    value = value.strip('"').strip("'")
+                    os.environ[key] = value
+        print("[OK] Fichier .env chargé")
+    else:
+        print("[ATTENTION] Fichier .env non trouvé")
+
+# Charger les variables d'environnement
+load_env_file()
+
+class ValidationPoint2:
+    """Validateur Point 2 avec vrais LLMs"""
+    
+    def __init__(self):
+        self.project_root = Path(__file__).parent
+        self.results = {
+            'flask_interface': False,
+            'react_interface': False,
+            'llm_integration': False,
+            'synthetic_data': False,
+            'e2e_tests': False
+        }
+        self.flask_process = None
+        self.react_process = None
+        
+    def print_banner(self, title):
+        """Bannière formatée"""
+        print("\n" + "=" * 80)
+        print(f"  {title}")
+        print("=" * 80)
+    
+    def check_openrouter_config(self):
+        """Vérifie la configuration OpenRouter"""
+        self.print_banner("VERIFICATION CONFIGURATION OPENROUTER")
+        
+        api_key = os.getenv('OPENAI_API_KEY')
+        base_url = os.getenv('OPENAI_BASE_URL')
+        
+        if not api_key or not base_url:
+            print("[ERREUR] Configuration OpenRouter manquante")
+            return False
+            
+        if 'openrouter.ai' not in base_url:
+            print("[ERREUR] Base URL OpenRouter invalide")
+            return False
+            
+        print(f"[OK] API Key: {api_key[:20]}...")
+        print(f"[OK] Base URL: {base_url}")
+        return True
+    
+    def start_flask_interface(self):
+        """Démarre l'interface Flask"""
+        self.print_banner("DEMARRAGE INTERFACE FLASK")
+        
+        try:
+            # Démarrer Flask en arrière-plan
+            self.flask_process = subprocess.Popen([
+                sys.executable, "interface_web/app.py"
+            ], cwd=self.project_root, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
+            
+            # Attendre le démarrage
+            print("[*] Démarrage Flask en cours...")
+            time.sleep(8)
+            
+            # Tester la connexion
+            try:
+                response = requests.get('http://localhost:3000', timeout=10)
+                if response.status_code == 200:
+                    print("[OK] Interface Flask opérationnelle")
+                    return True
+                else:
+                    print(f"[ERREUR] Status code: {response.status_code}")
+                    return False
+            except requests.exceptions.RequestException as e:
+                print(f"[ERREUR] Connexion Flask: {e}")
+                return False
+                
+        except Exception as e:
+            print(f"[ERREUR] Démarrage Flask: {e}")
+            return False
+    
+    def test_flask_with_real_llm(self):
+        """Test Flask avec vrais appels LLM"""
+        self.print_banner("TEST FLASK AVEC VRAIS LLMS")
+        
+        test_cases = [
+            {
+                'text': 'Si il pleut, alors la route est mouillée. Il pleut. Donc la route est mouillée.',
+                'analysis_type': 'propositional'
+            },
+            {
+                'text': 'Il est nécessaire que tous les hommes soient mortels. Socrate est un homme.',
+                'analysis_type': 'modal'
+            },
+            {
+                'text': 'L\'intelligence artificielle présente des avantages et des inconvénients.',
+                'analysis_type': 'comprehensive'
+            }
+        ]
+        
+        success_count = 0
+        
+        for i, test_case in enumerate(test_cases, 1):
+            print(f"\n[*] Test {i}/3: {test_case['analysis_type']}")
+            
+            try:
+                response = requests.post(
+                    'http://localhost:3000/analyze',
+                    json=test_case,
+                    timeout=30
+                )
+                
+                if response.status_code == 200:
+                    result = response.json()
+                    print(f"[OK] Analyse réussie - ID: {result.get('analysis_id', 'N/A')}")
+                    
+                    # Vérifier la structure de réponse
+                    if 'results' in result and 'metadata' in result:
+                        print(f"[OK] Structure de réponse valide")
+                        success_count += 1
+                    else:
+                        print(f"[ATTENTION] Structure de réponse incomplète")
+                else:
+                    print(f"[ERREUR] Status: {response.status_code}")
+                    
+            except Exception as e:
+                print(f"[ERREUR] Test {i}: {e}")
+        
+        success_rate = success_count / len(test_cases)
+        print(f"\n[RESULTAT] Taux de succès Flask: {success_rate:.1%} ({success_count}/{len(test_cases)})")
+        
+        self.results['flask_interface'] = success_rate >= 0.7
+        return self.results['flask_interface']
+    
+    def test_react_interface(self):
+        """Test de l'interface React"""
+        self.print_banner("TEST INTERFACE REACT")
+        
+        react_path = self.project_root / "services/web_api/interface-web-argumentative"
+        
+        if not react_path.exists():
+            print("[ERREUR] Répertoire React non trouvé")
+            return False
+        
+        try:
+            # Vérifier package.json
+            package_json = react_path / "package.json"
+            if package_json.exists():
+                print("[OK] Package.json trouvé")
+            else:
+                print("[ERREUR] Package.json manquant")
+                return False
+            
+            # Vérifier les composants principaux
+            components_dir = react_path / "src/components"
+            if components_dir.exists():
+                components = list(components_dir.glob("*.js"))
+                print(f"[OK] {len(components)} composants React trouvés")
+                
+                # Compter les onglets attendus
+                expected_components = ['ArgumentAnalyzer', 'FallacyDetector', 'FrameworkBuilder', 'ValidationForm', 'LogicGraph']
+                found_components = [c.stem for c in components]
+                
+                tab_count = sum(1 for exp in expected_components if exp in found_components)
+                print(f"[OK] {tab_count}/5 onglets principaux détectés")
+                
+                self.results['react_interface'] = tab_count >= 4
+                return self.results['react_interface']
+            else:
+                print("[ERREUR] Répertoire composants manquant")
+                return False
+                
+        except Exception as e:
+            print(f"[ERREUR] Test React: {e}")
+            return False
+    
+    def generate_synthetic_data_with_llm(self):
+        """Génère des données synthétiques avec vrais LLMs"""
+        self.print_banner("GENERATION DONNEES SYNTHETIQUES")
+        
+        try:
+            # Test simple d'appel LLM direct
+            test_response = requests.post(
+                'http://localhost:3000/analyze',
+                json={
+                    'text': 'Générez un argument logique complexe sur l\'éthique de l\'IA.',
+                    'analysis_type': 'comprehensive',
+                    'options': {'generate_synthetic': True}
+                },
+                timeout=45
+            )
+            
+            if test_response.status_code == 200:
+                result = test_response.json()
+                print(f"[OK] Génération synthétique réussie")
+                print(f"[INFO] Durée: {result.get('metadata', {}).get('duration', 'N/A')}s")
+                
+                self.results['synthetic_data'] = True
+                return True
+            else:
+                print(f"[ERREUR] Génération échouée: {test_response.status_code}")
+                return False
+                
+        except Exception as e:
+            print(f"[ERREUR] Génération synthétique: {e}")
+            return False
+    
+    def run_quick_playwright_test(self):
+        """Exécute un test Playwright rapide"""
+        self.print_banner("TEST PLAYWRIGHT RAPIDE")
+        
+        try:
+            # Test simple avec timeout court
+            result = subprocess.run([
+                "npx", "playwright", "test", 
+                "--project=chromium",
+                "--timeout=30000",
+                "--max-failures=1",
+                "tests/flask-interface.spec.js"
+            ], cwd=self.project_root / "tests_playwright", 
+               capture_output=True, text=True, timeout=60)
+            
+            if result.returncode == 0:
+                print("[OK] Tests Playwright réussis")
+                self.results['e2e_tests'] = True
+                return True
+            else:
+                print(f"[ATTENTION] Tests Playwright partiels")
+                print(f"Stdout: {result.stdout[-200:]}")
+                return False
+                
+        except subprocess.TimeoutExpired:
+            print("[ATTENTION] Timeout tests Playwright")
+            return False
+        except Exception as e:
+            print(f"[ERREUR] Tests Playwright: {e}")
+            return False
+    
+    def cleanup(self):
+        """Nettoyage des processus"""
+        if self.flask_process:
+            try:
+                self.flask_process.terminate()
+                self.flask_process.wait(timeout=5)
+            except:
+                self.flask_process.kill()
+        
+        if self.react_process:
+            try:
+                self.react_process.terminate()
+                self.react_process.wait(timeout=5)
+            except:
+                self.react_process.kill()
+    
+    def generate_report(self):
+        """Génère le rapport final"""
+        self.print_banner("RAPPORT VALIDATION POINT 2")
+        
+        print("RESULTATS:")
+        print(f"  [+] Interface Flask avec LLMs: {'[OK]' if self.results['flask_interface'] else '[ECHEC]'}")
+        print(f"  [+] Interface React 5 onglets: {'[OK]' if self.results['react_interface'] else '[ECHEC]'}")
+        print(f"  [+] Intégration LLM authentique: {'[OK]' if self.results['llm_integration'] else '[ECHEC]'}")
+        print(f"  [+] Données synthétiques: {'[OK]' if self.results['synthetic_data'] else '[ECHEC]'}")
+        print(f"  [+] Tests E2E Playwright: {'[OK]' if self.results['e2e_tests'] else '[ECHEC]'}")
+        
+        success_count = sum(self.results.values())
+        total_tests = len(self.results)
+        success_rate = success_count / total_tests
+        
+        print(f"\nSCORE GLOBAL: {success_count}/{total_tests} ({success_rate:.1%})")
+        
+        if success_rate >= 0.8:
+            print("\n[SUCCES] POINT 2 VALIDÉ - Applications web opérationnelles avec vrais LLMs")
+            status = "VALIDÉ"
+        elif success_rate >= 0.6:
+            print("\n[ATTENTION] POINT 2 PARTIELLEMENT VALIDÉ")
+            status = "PARTIEL"
+        else:
+            print("\n[ECHEC] POINT 2 NON VALIDÉ")
+            status = "ECHEC"
+        
+        # Sauvegarde du rapport
+        report = {
+            'validation_date': datetime.now().isoformat(),
+            'point': 'Point 2 - Applications web Flask + React avec vrais LLMs',
+            'status': status,
+            'success_rate': success_rate,
+            'detailed_results': self.results,
+            'recommendations': self.get_recommendations()
+        }
+        
+        with open('rapport_validation_point2.json', 'w', encoding='utf-8') as f:
+            json.dump(report, f, indent=2, ensure_ascii=False)
+        
+        print(f"\n[INFO] Rapport sauvegardé: rapport_validation_point2.json")
+        return success_rate >= 0.6
+    
+    def get_recommendations(self):
+        """Recommandations selon les résultats"""
+        recommendations = []
+        
+        if not self.results['flask_interface']:
+            recommendations.append("Améliorer intégration ServiceManager dans Flask")
+        
+        if not self.results['react_interface']:
+            recommendations.append("Compléter développement composants React manquants")
+        
+        if not self.results['synthetic_data']:
+            recommendations.append("Optimiser génération données synthétiques avec LLMs")
+        
+        if not self.results['e2e_tests']:
+            recommendations.append("Stabiliser tests Playwright end-to-end")
+        
+        return recommendations
+    
+    def run_validation(self):
+        """Exécute la validation complète"""
+        self.print_banner("VALIDATION POINT 2 - APPLICATIONS WEB + VRAIS LLMS")
+        
+        try:
+            # Phase 1: Configuration
+            if not self.check_openrouter_config():
+                print("[ERREUR] Configuration invalide")
+                return False
+            
+            # Phase 2: Interface Flask
+            if self.start_flask_interface():
+                self.test_flask_with_real_llm()
+                self.generate_synthetic_data_with_llm()
+                self.results['llm_integration'] = True
+            
+            # Phase 3: Interface React
+            self.test_react_interface()
+            
+            # Phase 4: Tests E2E rapides
+            self.run_quick_playwright_test()
+            
+            # Phase 5: Rapport
+            return self.generate_report()
+            
+        except KeyboardInterrupt:
+            print("\n[INFO] Arrêt demandé")
+            return False
+        except Exception as e:
+            print(f"\n[ERREUR] Erreur critique: {e}")
+            return False
+        finally:
+            self.cleanup()
+
+def main():
+    """Point d'entrée"""
+    validator = ValidationPoint2()
+    success = validator.run_validation()
+    return 0 if success else 1
+
+if __name__ == "__main__":
+    sys.exit(main())
\ No newline at end of file

==================== COMMIT: d796d39bc79598a3f2a50f74a374806c8423e8d8 ====================
commit d796d39bc79598a3f2a50f74a374806c8423e8d8
Author: jsboigeEpita <jean-sylvain.boige@epita.fr>
Date:   Mon Jun 16 02:03:11 2025 +0200

    MERGE: Integrate stash with robust parsing and test infra refactor

diff --git a/argumentation_analysis/ui/fetch_utils.py b/argumentation_analysis/ui/fetch_utils.py
index a3532814..a5d3c78d 100644
--- a/argumentation_analysis/ui/fetch_utils.py
+++ b/argumentation_analysis/ui/fetch_utils.py
@@ -13,7 +13,7 @@ from .cache_utils import load_from_cache, save_to_cache
 # reconstruct_url sera importé depuis le utils.py principal ou un common_utils
 # from .utils import reconstruct_url # Cet import sera ajusté plus tard
 # L'import correct, maintenant que utils.py est l'index et contient reconstruct_url
-from .utils import reconstruct_url
+from . import utils as circular_utils_ref_fetch
 
 fetch_logger = logging.getLogger("App.UI.FetchUtils")
 if not fetch_logger.handlers and not fetch_logger.propagate:
@@ -242,7 +242,7 @@ def get_full_text_for_source(source_info: Dict[str, Any], app_config: Optional[D
     if not target_url:
         fetch_logger.debug(f"Champ 'url' non trouvé pour {source_name_for_log}, tentative de reconstruction avec schema/host/path.")
         # Utilisation du reconstruct_url défini localement en attendant la refonte
-        target_url = reconstruct_url(
+        target_url = circular_utils_ref_fetch.reconstruct_url(
             source_info.get("schema"), source_info.get("host_parts", []), source_info.get("path")
         )
     
diff --git a/argumentation_analysis/ui/verification_utils.py b/argumentation_analysis/ui/verification_utils.py
index cc228575..8f2e92f2 100644
--- a/argumentation_analysis/ui/verification_utils.py
+++ b/argumentation_analysis/ui/verification_utils.py
@@ -29,7 +29,7 @@ if not verification_logger.handlers and not verification_logger.propagate:
 # La version HEAD de verify_extract_definitions reconstruit l'URL pour le cache_key.
 
 # La fonction reconstruct_url est maintenant importée depuis .utils
-from .utils import reconstruct_url
+# from .utils import reconstruct_url # Commenté pour suspicion de circularité/inutilisé
 
 def verify_extract_definitions(definitions_list: List[Dict[str, Any]], app_config: Optional[Dict[str, Any]] = None) -> str:
     """Vérifie la présence des marqueurs de début et de fin pour chaque extrait défini. (Version HEAD)"""
diff --git a/tests/conftest.py b/tests/conftest.py
deleted file mode 100644
index ef30c532..00000000
--- a/tests/conftest.py
+++ /dev/null
@@ -1,221 +0,0 @@
-import sys
-import os
-from pathlib import Path
-
-# Ajoute la racine du projet au sys.path pour résoudre les problèmes d'import
-# causés par le `rootdir` de pytest qui interfère avec la résolution des modules.
-project_root = Path(__file__).parent.parent.resolve()
-if str(project_root) not in sys.path:
-    sys.path.insert(0, str(project_root))
-"""
-Configuration pour les tests pytest.
-
-Ce fichier est automatiquement chargé par pytest avant l'exécution des tests.
-Il configure les mocks nécessaires pour les tests et utilise les vraies bibliothèques
-lorsqu'elles sont disponibles. Pour Python 3.12 et supérieur, le mock JPype1 est
-automatiquement utilisé en raison de problèmes de compatibilité.
-"""
-import project_core.core_from_scripts.auto_env
-import sys
-import os
-import pytest
-from unittest.mock import patch, MagicMock
-import importlib.util
-import logging
-import threading # Ajout de l'import pour l'inspection des threads
-# --- Configuration globale du Logging pour les tests ---
-# Le logger global pour conftest est déjà défini plus bas,
-# mais nous avons besoin de configurer basicConfig tôt.
-# Nous allons utiliser un logger temporaire ici ou le logger racine.
-_conftest_setup_logger = logging.getLogger("conftest.setup")
-
-if not logging.getLogger().handlers: # Si le root logger n'a pas de handlers, basicConfig n'a probablement pas été appelé efficacement.
-    logging.basicConfig(
-        level=logging.INFO, # Ou un autre niveau pertinent pour les tests globaux
-        format='%(asctime)s [%(levelname)s] [%(name)s] %(message)s',
-        datefmt='%H:%M:%S'
-    )
-    _conftest_setup_logger.info("Configuration globale du logging appliquée.")
-else:
-    _conftest_setup_logger.info("Configuration globale du logging déjà présente ou appliquée par un autre module.")
-# --- Début Patching JPype Mock au niveau module si nécessaire ---
-os.environ['USE_REAL_JPYPE'] = 'false'
-_SHOULD_USE_REAL_JPYPE = os.environ.get('USE_REAL_JPYPE', 'false').lower() in ('true', '1')
-_conftest_setup_logger.info(f"conftest.py: USE_REAL_JPYPE={os.environ.get('USE_REAL_JPYPE', 'false')}, _SHOULD_USE_REAL_JPYPE={_SHOULD_USE_REAL_JPYPE}")
-
-if not _SHOULD_USE_REAL_JPYPE:
-    _conftest_setup_logger.info("conftest.py: Application du mock JPype au niveau module dans sys.modules.")
-    try:
-        # S'assurer que le répertoire des mocks est dans le path pour les imports suivants
-        _current_dir_for_jpype_mock_patch = os.path.dirname(os.path.abspath(__file__))
-        _mocks_dir_for_jpype_mock_patch = os.path.join(_current_dir_for_jpype_mock_patch, 'mocks')
-        # if _mocks_dir_for_jpype_mock_patch not in sys.path:
-        #     sys.path.insert(0, _mocks_dir_for_jpype_mock_patch)
-        #     _conftest_setup_logger.info(f"Ajout de {_mocks_dir_for_jpype_mock_patch} à sys.path pour jpype_mock.")
-
-        from .mocks import jpype_mock # Importer le module mock principal
-        from .mocks.jpype_components.imports import imports_module as actual_mock_jpype_imports_module
-
-        # Préparer l'objet mock principal pour 'jpype'
-        _jpype_module_mock_obj = MagicMock(name="jpype_module_mock_from_conftest")
-        _jpype_module_mock_obj.__path__ = [] # Nécessaire pour simuler un package
-        _jpype_module_mock_obj.isJVMStarted = jpype_mock.isJVMStarted
-        _jpype_module_mock_obj.startJVM = jpype_mock.startJVM
-        _jpype_module_mock_obj.getJVMPath = jpype_mock.getJVMPath
-        _jpype_module_mock_obj.getJVMVersion = jpype_mock.getJVMVersion
-        _jpype_module_mock_obj.getDefaultJVMPath = jpype_mock.getDefaultJVMPath
-        _jpype_module_mock_obj.JClass = jpype_mock.JClass
-        _jpype_module_mock_obj.JException = jpype_mock.JException
-        _jpype_module_mock_obj.JObject = jpype_mock.JObject
-        _jpype_module_mock_obj.JVMNotFoundException = jpype_mock.JVMNotFoundException
-        _jpype_module_mock_obj.__version__ = getattr(jpype_mock, '__version__', '1.x.mock.conftest')
-        _jpype_module_mock_obj.imports = actual_mock_jpype_imports_module
-        # Simuler d'autres attributs/méthodes si nécessaire pour la collecte
-        _jpype_module_mock_obj.config = MagicMock(name="jpype.config_mock_from_conftest")
-        _jpype_module_mock_obj.config.destroy_jvm = True # Comportement par défaut sûr pour un mock
-
-        # Préparer le mock pour '_jpype' (le module C)
-        _mock_dot_jpype_module = jpype_mock._jpype
-
-        # Appliquer les mocks à sys.modules
-        sys.modules['jpype'] = _jpype_module_mock_obj
-        sys.modules['_jpype'] = _mock_dot_jpype_module 
-        sys.modules['jpype._core'] = _mock_dot_jpype_module 
-        sys.modules['jpype.imports'] = actual_mock_jpype_imports_module
-        sys.modules['jpype.config'] = _jpype_module_mock_obj.config
-        
-        _mock_types_module = MagicMock(name="jpype.types_mock_from_conftest")
-        for type_name in ["JString", "JArray", "JObject", "JBoolean", "JInt", "JDouble", "JLong", "JFloat", "JShort", "JByte", "JChar"]:
-             setattr(_mock_types_module, type_name, getattr(jpype_mock, type_name, MagicMock(name=f"Mock{type_name}")))
-        sys.modules['jpype.types'] = _mock_types_module
-        sys.modules['jpype.JProxy'] = MagicMock(name="jpype.JProxy_mock_from_conftest")
-
-        _conftest_setup_logger.info("Mock JPype appliqué à sys.modules DEPUIS conftest.py.")
-
-    except ImportError as e_mock_load:
-        _conftest_setup_logger.error(f"conftest.py: ERREUR CRITIQUE lors du chargement des mocks JPype (jpype_mock ou jpype_components): {e_mock_load}. Le mock JPype pourrait ne pas être actif.")
-    except Exception as e_patching:
-        _conftest_setup_logger.error(f"conftest.py: Erreur inattendue lors du patching de JPype: {e_patching}", exc_info=True)
-else:
-    _conftest_setup_logger.info("conftest.py: _SHOULD_USE_REAL_JPYPE est True. Aucun mock JPype appliqué au niveau module depuis conftest.py.")
-# --- Fin Patching JPype Mock ---
-# # --- Gestion des imports conditionnels NumPy et Pandas ---
-# _conftest_setup_logger.info("Début de la gestion des imports conditionnels pour NumPy et Pandas.")
-# try:
-#     import numpy
-#     import pandas
-#     _conftest_setup_logger.info("NumPy et Pandas réels importés avec succès.")
-# except ImportError:
-#     _conftest_setup_logger.warning("Échec de l'import de NumPy et/ou Pandas. Tentative d'utilisation des mocks.")
-    
-#     # Mock pour NumPy
-#     try:
-#         # Tenter d'importer le contenu spécifique du mock si disponible
-#         from tests.mocks.numpy_mock import array as numpy_array_mock # Importer un élément spécifique pour vérifier
-#         # Si l'import ci-dessus fonctionne, on peut supposer que le module mock est complet
-#         # et sera utilisé par les imports suivants dans le code testé.
-#         # Cependant, pour forcer l'utilisation du mock complet, on le met dans sys.modules.
-#         import tests.mocks.numpy_mock as numpy_mock_content
-#         sys.modules['numpy'] = numpy_mock_content
-#         _conftest_setup_logger.info("Mock pour NumPy (tests.mocks.numpy_mock) activé via sys.modules.")
-#     except ImportError:
-#         _conftest_setup_logger.error("Mock spécifique tests.mocks.numpy_mock non trouvé. Utilisation de MagicMock pour NumPy.")
-#         sys.modules['numpy'] = MagicMock()
-#     except Exception as e_numpy_mock:
-#         _conftest_setup_logger.error(f"Erreur inattendue lors du chargement du mock NumPy: {e_numpy_mock}. Utilisation de MagicMock.")
-#         sys.modules['numpy'] = MagicMock()
-
-#     # Mock pour Pandas
-#     try:
-#         # Tenter d'importer le contenu spécifique du mock
-#         from tests.mocks.pandas_mock import DataFrame as pandas_dataframe_mock # Importer un élément spécifique
-#         import tests.mocks.pandas_mock as pandas_mock_content
-#         sys.modules['pandas'] = pandas_mock_content
-#         _conftest_setup_logger.info("Mock pour Pandas (tests.mocks.pandas_mock) activé via sys.modules.")
-#     except ImportError:
-#         _conftest_setup_logger.error("Mock spécifique tests.mocks.pandas_mock non trouvé. Utilisation de MagicMock pour Pandas.")
-#         sys.modules['pandas'] = MagicMock()
-#     except Exception as e_pandas_mock:
-#         _conftest_setup_logger.error(f"Erreur inattendue lors du chargement du mock Pandas: {e_pandas_mock}. Utilisation de MagicMock.")
-#         sys.modules['pandas'] = MagicMock()
-# _conftest_setup_logger.info("Fin de la gestion des imports conditionnels pour NumPy et Pandas.")
-# # --- Fin Gestion des imports conditionnels ---
-# --- Fin Configuration globale du Logging ---
-
-# --- Gestion du Path pour les Mocks (déplacé ici AVANT les imports des mocks) ---
-current_dir_for_mock = os.path.dirname(os.path.abspath(__file__))
-mocks_dir_for_mock = os.path.join(current_dir_for_mock, 'mocks')
-# if mocks_dir_for_mock not in sys.path:
-#     sys.path.insert(0, mocks_dir_for_mock)
-#     _conftest_setup_logger.info(f"Ajout de {mocks_dir_for_mock} à sys.path pour l'accès aux mocks locaux.")
-
-from .mocks.jpype_setup import (
-    _REAL_JPYPE_MODULE,
-    _REAL_JPYPE_AVAILABLE, # Ajouté pour skipif
-    _JPYPE_MODULE_MOCK_OBJ_GLOBAL,
-    _MOCK_DOT_JPYPE_MODULE_GLOBAL,
-    activate_jpype_mock_if_needed,
-    pytest_sessionstart,
-    pytest_sessionfinish
-)
-from .mocks.numpy_setup import setup_numpy_for_tests_fixture
-
-from .fixtures.integration_fixtures import (
-    integration_jvm, dung_classes, dl_syntax_parser, fol_syntax_parser,
-    pl_syntax_parser, cl_syntax_parser, tweety_logics_classes,
-    tweety_string_utils, tweety_math_utils, tweety_probability,
-    tweety_conditional_probability, tweety_parser_exception,
-    tweety_io_exception, tweety_qbf_classes, belief_revision_classes,
-    dialogue_classes
-)
-
-# --- Configuration du Logger (déplacé avant la sauvegarde JPype pour l'utiliser) ---
-logger = logging.getLogger(__name__)
-
-# _REAL_JPYPE_MODULE, _JPYPE_MODULE_MOCK_OBJ_GLOBAL, _MOCK_DOT_JPYPE_MODULE_GLOBAL sont maintenant importés de jpype_setup.py
-
-# Nécessaire pour la fixture integration_jvm
-# La variable _integration_jvm_started_session_scope et les imports de jvm_setup
-# ne sont plus nécessaires ici, gérés dans integration_fixtures.py
-
-# Les sections de code commentées pour le mocking global de Matplotlib, NetworkX,
-# l'installation immédiate de Pandas, et ExtractDefinitions ont été supprimées.
-# Ces mocks, s'ils sont nécessaires, devraient être gérés par des fixtures spécifiques
-# ou une configuration au niveau du module mock lui-même, similaire à NumPy/Pandas.
-
-# Ajout du répertoire racine du projet à sys.path pour assurer la découverte des modules du projet.
-# Ceci est particulièrement utile si les tests sont exécutés d'une manière où le répertoire racine
-# n'est pas automatiquement inclus dans PYTHONPATH (par exemple, exécution directe de pytest
-# depuis un sous-répertoire ou avec certaines configurations d'IDE).
-parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
-if parent_dir not in sys.path:
-    sys.path.insert(0, parent_dir)
-    _conftest_setup_logger.info(f"Ajout du répertoire racine du projet ({parent_dir}) à sys.path.")
-# Décommenté car l'environnement de test actuel en a besoin pour trouver les modules locaux.
-
-# Les fixtures et hooks sont importés depuis leurs modules dédiés.
-# Les commentaires résiduels concernant les déplacements de code et les refactorisations
-# antérieures ont été supprimés pour améliorer la lisibilité.
-
-# --- Fixtures déplacées depuis tests/integration/webapp/conftest.py ---
-
-@pytest.fixture
-def webapp_config():
-    """Provides a basic webapp configuration dictionary."""
-    return {
-        "backend": {
-            "start_port": 8008,
-            "fallback_ports": [8009, 8010]
-        },
-        "frontend": {
-            "port": 3008
-        },
-        "playwright": {
-            "enabled": True
-        }
-    }
-
-@pytest.fixture
-def test_config_path(tmp_path):
-    """Provides a temporary path for a config file."""
-    return tmp_path / "test_config.yml"
diff --git a/tests/mocks/jpype_components/imports.py b/tests/mocks/jpype_components/imports.py
index eba9d596..dfc16051 100644
--- a/tests/mocks/jpype_components/imports.py
+++ b/tests/mocks/jpype_components/imports.py
@@ -1,46 +1,58 @@
-import sys
-import types
-import logging
-
-# Configuration du logging pour ce module
-mock_logger = logging.getLogger(__name__)
-if not mock_logger.hasHandlers():
-    handler = logging.StreamHandler(sys.stdout)
-    formatter = logging.Formatter('[MOCK JPYPE IMPORTS LOG] %(asctime)s - %(levelname)s - %(message)s')
-    handler.setFormatter(formatter)
-    mock_logger.addHandler(handler)
-mock_logger.setLevel(logging.DEBUG)
-
-class MockJpypeImports:
-    def registerDomain(self, domain, alias=None):
-        """Simule jpype.imports.registerDomain()."""
-        mock_logger.info(f"jpype.imports.registerDomain('{domain}', alias='{alias}') appelé.")
-        # Ne fait rien, car c'est un mock.
-        pass
-
-# Crée une instance du mock d'imports. Cette instance sera utilisée pour peupler le module factice.
-_the_actual_imports_object = MockJpypeImports()
-mock_logger.info("Instance de MockJpypeImports créée (_the_actual_imports_object).")
-
-# Créer un objet module factice pour 'jpype.imports'
-# C'est cet objet 'imports_module' qui sera accessible via `from jpype_components import imports`
-# ou `jpype.imports` après le patch dans jpype_mock.py principal.
-imports_module = types.ModuleType('jpype.imports')
-mock_logger.debug(f"Module factice 'jpype.imports' créé: {imports_module}")
-
-# Attribuer les fonctionnalités de notre instance '_the_actual_imports_object' à ce module factice.
-if hasattr(_the_actual_imports_object, 'registerDomain'):
-    imports_module.registerDomain = _the_actual_imports_object.registerDomain
-    mock_logger.debug(f"Fonction 'registerDomain' attachée au module factice 'jpype.imports'.")
-
-# Mettre ce module factice dans sys.modules pour qu'il soit trouvable par 'import jpype.imports'
-# Ceci est crucial pour que les imports directs de jpype.imports fonctionnent comme attendu
-# après que ce module (jpype_components.imports) soit importé et exécuté.
-sys.modules['jpype.imports'] = imports_module
-mock_logger.info(f"Module factice 'jpype.imports' injecté dans sys.modules.")
-
-# Exposer le module factice comme 'imports' pour ce fichier,
-# afin que `from tests.mocks.jpype_components.imports import imports` fonctionne.
-imports = imports_module
-
+import sys
+import types
+import logging
+import os
+
+# Configuration du logging pour ce module
+mock_logger = logging.getLogger(__name__)
+if not mock_logger.hasHandlers():
+    handler = logging.StreamHandler(sys.stdout)
+    formatter = logging.Formatter('[MOCK JPYPE IMPORTS LOG] %(asctime)s - %(levelname)s - %(message)s')
+    handler.setFormatter(formatter)
+    mock_logger.addHandler(handler)
+mock_logger.setLevel(logging.DEBUG)
+
+imports = None # Initialisation par défaut
+
+if os.environ.get('USE_REAL_JPYPE', 'false').lower() != 'true':
+    class MockJpypeImports:
+        def registerDomain(self, domain, alias=None):
+            """Simule jpype.imports.registerDomain()."""
+            mock_logger.info(f"jpype.imports.registerDomain('{domain}', alias='{alias}') appelé.")
+            # Ne fait rien, car c'est un mock.
+            pass
+
+    # Crée une instance du mock d'imports. Cette instance sera utilisée pour peupler le module factice.
+    _the_actual_imports_object = MockJpypeImports()
+    mock_logger.info("Instance de MockJpypeImports créée (_the_actual_imports_object).")
+
+    # Créer un objet module factice pour 'jpype.imports'
+    # C'est cet objet 'imports_module' qui sera accessible via `from jpype_components import imports`
+    # ou `jpype.imports` après le patch dans jpype_mock.py principal.
+    imports_module = types.ModuleType('jpype.imports')
+    mock_logger.debug(f"Module factice 'jpype.imports' créé: {imports_module}")
+
+    # Attribuer les fonctionnalités de notre instance '_the_actual_imports_object' à ce module factice.
+    if hasattr(_the_actual_imports_object, 'registerDomain'):
+        imports_module.registerDomain = _the_actual_imports_object.registerDomain
+        mock_logger.debug(f"Fonction 'registerDomain' attachée au module factice 'jpype.imports'.")
+
+    # Mettre ce module factice dans sys.modules pour qu'il soit trouvable par 'import jpype.imports'
+    # Ceci est crucial pour que les imports directs de jpype.imports fonctionnent comme attendu
+    # après que ce module (jpype_components.imports) soit importé et exécuté.
+    sys.modules['jpype.imports'] = imports_module
+    mock_logger.info(f"MOCK: Module factice 'jpype.imports' injecté dans sys.modules.")
+
+    # Exposer le module factice comme 'imports' pour ce fichier,
+    # afin que `from tests.mocks.jpype_components.imports import imports` fonctionne.
+    imports = imports_module
+else:
+    mock_logger.info(f"REAL JPYPE: Injection du mock pour 'jpype.imports' ANNULÉE.")
+    try:
+        import jpype.imports as real_jpype_imports
+        imports = real_jpype_imports
+        mock_logger.info("REAL JPYPE: Le vrai 'jpype.imports' a été importé et assigné à 'imports'.")
+    except ImportError:
+        mock_logger.warning("REAL JPYPE: Impossible d'importer le vrai 'jpype.imports'. 'imports' restera None.")
+
 mock_logger.info("Module jpype_components.imports initialisé.")
\ No newline at end of file
diff --git a/tests/mocks/jpype_mock.py b/tests/mocks/jpype_mock.py
index 79fd2be5..60e1d6e1 100644
--- a/tests/mocks/jpype_mock.py
+++ b/tests/mocks/jpype_mock.py
@@ -1,191 +1,196 @@
-"""
-Mock de JPype1 pour la compatibilité avec Python 3.12+.
-Ce fichier sert de point d'entrée principal pour le mock JPype,
-important les composants spécifiques depuis le package jpype_components.
-"""
-
-import sys
-import logging
-from unittest.mock import MagicMock # Gardé pour _MockInternalJpypeModule si besoin, ou autres mocks directs
-
-# Importer les composants nécessaires depuis le package jpype_components
-from tests.mocks.jpype_components.jvm import ( # MODIFIÉ
-    isJVMStarted,
-    startJVM,
-    shutdownJVM,
-    getDefaultJVMPath,
-    getJVMPath,
-    getJVMVersion,
-    getClassPath,
-    _jvm_started, # Exposer pour _MockInternalJpypeModule
-    _jvm_path # Exposer si nécessaire globalement
-)
-from tests.mocks.jpype_components.config import config # MODIFIÉ
-from tests.mocks.jpype_components.imports import imports # MODIFIÉ
-from tests.mocks.jpype_components.types import ( # MODIFIÉ
-    JString,
-    JArray,
-    JObject,
-    JBoolean,
-    JInt,
-    JDouble,
-    JLong,
-    JFloat,
-    JShort,
-    JByte,
-    JChar
-)
-from tests.mocks.jpype_components.exceptions import JException, JVMNotFoundException # MODIFIÉ
-from tests.mocks.jpype_components.jclass_core import MockJClassCore # MODIFIÉ
-from tests.mocks.jpype_components import tweety_enums # MODIFIÉ
-
-# Configuration du logging pour le mock principal
-mock_logger = logging.getLogger(__name__)
-if not mock_logger.hasHandlers():
-    handler = logging.StreamHandler(sys.stdout)
-    formatter = logging.Formatter('[MOCK JPYPE MAIN LOG] %(asctime)s - %(levelname)s - %(message)s')
-    handler.setFormatter(formatter)
-    mock_logger.addHandler(handler)
-mock_logger.setLevel(logging.DEBUG)
-
-mock_logger.info("jpype_mock.py (principal) en cours de chargement.")
-
-# Version du mock
-__version__ = "1.4.0-mock-refactored"
-
-# --- _MockInternalJpypeModule ---
-# Cette classe simule le module `_jpype` interne que `jpype.imports` pourrait utiliser.
-# Elle doit rester ici ou être accessible globalement car `sys.modules['jpype._jpype']` est patché.
-class _MockJpypeCoreModule: # Renommée pour clarifier son rôle
-    def isStarted(self):
-        # Utilise la variable _jvm_started importée depuis .jpype_components.jvm
-        mock_logger.debug(f"[MOCK jpype._core.isStarted()] Appelée. Retourne: {_jvm_started}")
-        return _jvm_started
-    
-    isJVMStarted = isStarted # Alias pour correspondre à l'API publique si nécessaire
-
-    def getVersion(self):
-        return f"Mocked jpype._core module (simulated in jpype_mock.py, JVM version: {getJVMVersion()})"
-
-    def isPackage(self, name):
-        # Simule la vérification de l'existence d'un package.
-        mock_logger.debug(f"[MOCK jpype._core.isPackage('{name}')] Appelée. _jvm_started: {_jvm_started}. Retourne: {_jvm_started}")
-        return _jvm_started
-
-_jpype_core_mock_instance = _MockJpypeCoreModule()
-sys.modules['jpype._jpype'] = _jpype_core_mock_instance # Historique, peut-être encore utilisé par d'anciens imports
-sys.modules['_jpype'] = _jpype_core_mock_instance      # Pour jpype.imports qui fait "from . import _jpype"
-sys.modules['jpype._core'] = _jpype_core_mock_instance # Pour s'assurer que si jpype._core est accédé, c'est notre mock
-mock_logger.info("Instance de _MockJpypeCoreModule injectée dans sys.modules['jpype._jpype'], sys.modules['_jpype'] et sys.modules['jpype._core'].")
-
-
-# --- MockJavaNamespace ---
-# Simule jpype.java (pour accès comme jpype.java.lang.String)
-class MockJavaNamespace:
-    def __init__(self, path_prefix=""):
-        self._path_prefix = path_prefix
-        mock_logger.debug(f"MockJavaNamespace créé pour préfixe: '{path_prefix}'")
-
-    def __getattr__(self, name):
-        new_path = f"{self._path_prefix}.{name}" if self._path_prefix else name
-        final_segment = new_path.split('.')[-1]
-
-        # Heuristique: si le dernier segment commence par une majuscule, c'est une classe.
-        if final_segment and final_segment[0].isupper():
-            mock_logger.debug(f"Accès à jpype.java...{new_path}, interprété comme JClass('{new_path}')")
-            return JClass(new_path) # Utilise la fonction JClass définie ci-dessous
-        else:
-            mock_logger.debug(f"Accès à jpype.java...{new_path}, interprété comme sous-namespace")
-            return MockJavaNamespace(new_path)
-
-java = MockJavaNamespace("java") # Pour jpype.java.xxx
-# Potentiellement d'autres namespaces de haut niveau si nécessaire (org, net, etc.)
-# Exemple: org = MockJavaNamespace("org")
-
-# --- Fonction JClass principale ---
-# Cette fonction est le point d'entrée pour obtenir des classes Java mockées.
-# Elle utilisera MockJClassCore et appliquera des configurations spécifiques.
-_jclass_cache = {} # Cache simple pour les instances de MockJClassCore
-
-def JClass(name: str, *args, **kwargs): # Ajout de *args, **kwargs pour ignorer les arguments non supportés comme 'loader'
-    """
-    Simule jpype.JClass(). Retourne une instance de MockJClassCore configurée.
-    """
-    mock_logger.info(f"[MOCK JPYPE JClass] Appelée avec name='{name}', args={args}, kwargs={kwargs}") # Log ajouté
-    if name in _jclass_cache:
-        # mock_logger.debug(f"JClass('{name}') trouvé dans le cache.")
-        return _jclass_cache[name]
-
-    mock_logger.debug(f"JClass('{name}') demandé.")
-
-    # Vérifier si c'est une énumération Tweety connue
-    if name in tweety_enums.ENUM_MAPPINGS:
-        enum_class_mock = tweety_enums.ENUM_MAPPINGS[name]
-        # Assurer que les membres de l'enum sont initialisés si ce n'est pas déjà fait
-        # Normalement, la métaclasse s'en charge, mais une vérification ici peut être utile.
-        # hasattr(enum_class_mock, '_initialize_enum_members') and enum_class_mock._initialize_enum_members()
-        # La métaclasse devrait avoir déjà appelé _initialize_enum_members.
-        # On s'assure que la classe retournée a bien le jclass_provider si besoin (pas typique pour les enums statiques)
-        # et que son __name__ correspond bien au nom Java demandé.
-        # Les classes Enum mockées héritent de MockJClassCore, donc elles ont un __name__ et un class_name.
-        # Il faut s'assurer que le nom Java est bien celui attendu.
-        # Le MOCK_JAVA_CLASS_NAME est utilisé pour cela.
-        mock_logger.info(f"JClass('{name}') identifié comme une énumération Tweety. Retourne la classe mockée: {enum_class_mock}.")
-        _jclass_cache[name] = enum_class_mock
-        return enum_class_mock
-
-    mock_logger.debug(f"JClass('{name}'): Pas une énumération Tweety connue. Création d'une instance de MockJClassCore.")
-    # Passer la fonction JClass elle-même pour que MockJClassCore puisse la fournir aux configurateurs.
-    core_class_mock = MockJClassCore(name, jclass_provider_func=JClass)
-
-    # La configuration des reasoners et agents est maintenant gérée dans MockJClassCore.__call__
-    # via le module tweety_reasoners.
-    # Exemple:
-    # if name.startswith("org.tweetyproject.arg"):
-    #     from .jpype_components import tweety_syntax # ou autre module pertinent
-    #     tweety_syntax.configure_tweety_argument_class(core_class_mock)
-    # elif name.startswith("org.tweetyproject.logics"):
-    #     # ...
-    #     pass
-
-    _jclass_cache[name] = core_class_mock
-    return core_class_mock
-
-
-# --- Patchs finaux et exports ---
-
-# Le patch pour jpype.imports._jpype.isStarted est maintenant géré par
-# le fait que _MockInternalJpypeModule utilise _jvm_started de jvm.py.
-
-# Le module `imports` (qui est `jpype_components.imports.imports_module`)
-# est déjà dans sys.modules['jpype.imports'] grâce à son propre code d'initialisation.
-# On s'assure juste qu'il est exposé par ce module principal si on fait `import jpype_mock; jpype_mock.imports`.
-# La variable `imports` importée de `.jpype_components.imports` est déjà le module correct.
-
-# Les variables globales _jvm_started et _jvm_path sont gérées dans jvm.py.
-# Les fonctions comme isJVMStarted, startJVM, etc., sont importées directement.
-# L'instance `config` est importée directement.
-# Les types JString, JArray, etc., sont importés directement.
-# Les exceptions JException, JVMNotFoundException sont importées directement.
-
-mock_logger.info("Mock JPype1 (jpype_mock.py principal) initialisé et refactorisé.")
-mock_logger.info("Les composants sont maintenant dans le package 'jpype_components'.")
-
-# Optionnel: Exposer explicitement ce qui fait partie de l'API publique du mock
-# __all__ = [
-#     "__version__",
-#     "isJVMStarted", "startJVM", "shutdownJVM",
-#     "getDefaultJVMPath", "getJVMPath", "getJVMVersion", "getClassPath",
-#     "config",
-#     "imports",
-#     "java", # Pour jpype.java.xxx
-#     "JClass", "JString", "JArray", "JObject",
-#     "JBoolean", "JInt", "JDouble", "JLong", "JFloat", "JShort", "JByte", "JChar",
-#     "JException", "JVMNotFoundException",
-#     # Les classes MockJClassCore, MockJavaCollection ne sont pas typiquement accédées directement
-#     # par l'utilisateur du mock, mais via JClass() ou des méthodes retournant des collections.
-# ]
-
-# Exposer l'instance du mock interne _jpype pour que conftest.py puisse l'importer
-_jpype = _jpype_core_mock_instance # Exposer l'instance mockée
-# La deuxième assignation était redondante
+"""
+Mock de JPype1 pour la compatibilité avec Python 3.12+.
+Ce fichier sert de point d'entrée principal pour le mock JPype,
+important les composants spécifiques depuis le package jpype_components.
+"""
+
+import os
+import sys
+import logging
+from unittest.mock import MagicMock # Gardé pour _MockInternalJpypeModule si besoin, ou autres mocks directs
+
+# Importer les composants nécessaires depuis le package jpype_components
+from tests.mocks.jpype_components.jvm import ( # MODIFIÉ
+    isJVMStarted,
+    startJVM,
+    shutdownJVM,
+    getDefaultJVMPath,
+    getJVMPath,
+    getJVMVersion,
+    getClassPath,
+    _jvm_started, # Exposer pour _MockInternalJpypeModule
+    _jvm_path # Exposer si nécessaire globalement
+)
+from tests.mocks.jpype_components.config import config # MODIFIÉ
+from tests.mocks.jpype_components.imports import imports # MODIFIÉ
+from tests.mocks.jpype_components.types import ( # MODIFIÉ
+    JString,
+    JArray,
+    JObject,
+    JBoolean,
+    JInt,
+    JDouble,
+    JLong,
+    JFloat,
+    JShort,
+    JByte,
+    JChar
+)
+from tests.mocks.jpype_components.exceptions import JException, JVMNotFoundException # MODIFIÉ
+from tests.mocks.jpype_components.jclass_core import MockJClassCore # MODIFIÉ
+from tests.mocks.jpype_components import tweety_enums # MODIFIÉ
+
+# Configuration du logging pour le mock principal
+mock_logger = logging.getLogger(__name__)
+if not mock_logger.hasHandlers():
+    handler = logging.StreamHandler(sys.stdout)
+    formatter = logging.Formatter('[MOCK JPYPE MAIN LOG] %(asctime)s - %(levelname)s - %(message)s')
+    handler.setFormatter(formatter)
+    mock_logger.addHandler(handler)
+mock_logger.setLevel(logging.DEBUG)
+
+mock_logger.info("jpype_mock.py (principal) en cours de chargement.")
+
+# Version du mock
+__version__ = "1.4.0-mock-refactored"
+
+# --- _MockInternalJpypeModule ---
+# Cette classe simule le module `_jpype` interne que `jpype.imports` pourrait utiliser.
+# Elle doit rester ici ou être accessible globalement car `sys.modules['jpype._jpype']` est patché.
+class _MockJpypeCoreModule: # Renommée pour clarifier son rôle
+    def isStarted(self):
+        # Utilise la variable _jvm_started importée depuis .jpype_components.jvm
+        mock_logger.debug(f"[MOCK jpype._core.isStarted()] Appelée. Retourne: {_jvm_started}")
+        return _jvm_started
+    
+    isJVMStarted = isStarted # Alias pour correspondre à l'API publique si nécessaire
+
+    def getVersion(self):
+        return f"Mocked jpype._core module (simulated in jpype_mock.py, JVM version: {getJVMVersion()})"
+
+    def isPackage(self, name):
+        # Simule la vérification de l'existence d'un package.
+        mock_logger.debug(f"[MOCK jpype._core.isPackage('{name}')] Appelée. _jvm_started: {_jvm_started}. Retourne: {_jvm_started}")
+        return _jvm_started
+
+if os.environ.get('USE_REAL_JPYPE', 'false').lower() != 'true':
+    _jpype_core_mock_instance = _MockJpypeCoreModule()
+    sys.modules['jpype._jpype'] = _jpype_core_mock_instance
+    sys.modules['_jpype'] = _jpype_core_mock_instance
+    sys.modules['jpype._core'] = _jpype_core_mock_instance
+    mock_logger.info("MOCK JPYPE: Instance de _MockJpypeCoreModule injectée dans sys.modules.")
+else:
+    _jpype_core_mock_instance = None # Définir à None si on utilise le vrai JPype
+    mock_logger.info("MOCK JPYPE: USE_REAL_JPYPE est true. Injection du mock _MockJpypeCoreModule ANNULÉE. _jpype_core_mock_instance est None.")
+
+
+# --- MockJavaNamespace ---
+# Simule jpype.java (pour accès comme jpype.java.lang.String)
+class MockJavaNamespace:
+    def __init__(self, path_prefix=""):
+        self._path_prefix = path_prefix
+        mock_logger.debug(f"MockJavaNamespace créé pour préfixe: '{path_prefix}'")
+
+    def __getattr__(self, name):
+        new_path = f"{self._path_prefix}.{name}" if self._path_prefix else name
+        final_segment = new_path.split('.')[-1]
+
+        # Heuristique: si le dernier segment commence par une majuscule, c'est une classe.
+        if final_segment and final_segment[0].isupper():
+            mock_logger.debug(f"Accès à jpype.java...{new_path}, interprété comme JClass('{new_path}')")
+            return JClass(new_path) # Utilise la fonction JClass définie ci-dessous
+        else:
+            mock_logger.debug(f"Accès à jpype.java...{new_path}, interprété comme sous-namespace")
+            return MockJavaNamespace(new_path)
+
+java = MockJavaNamespace("java") # Pour jpype.java.xxx
+# Potentiellement d'autres namespaces de haut niveau si nécessaire (org, net, etc.)
+# Exemple: org = MockJavaNamespace("org")
+
+# --- Fonction JClass principale ---
+# Cette fonction est le point d'entrée pour obtenir des classes Java mockées.
+# Elle utilisera MockJClassCore et appliquera des configurations spécifiques.
+_jclass_cache = {} # Cache simple pour les instances de MockJClassCore
+
+def JClass(name: str, *args, **kwargs): # Ajout de *args, **kwargs pour ignorer les arguments non supportés comme 'loader'
+    """
+    Simule jpype.JClass(). Retourne une instance de MockJClassCore configurée.
+    """
+    mock_logger.info(f"[MOCK JPYPE JClass] Appelée avec name='{name}', args={args}, kwargs={kwargs}") # Log ajouté
+    if name in _jclass_cache:
+        # mock_logger.debug(f"JClass('{name}') trouvé dans le cache.")
+        return _jclass_cache[name]
+
+    mock_logger.debug(f"JClass('{name}') demandé.")
+
+    # Vérifier si c'est une énumération Tweety connue
+    if name in tweety_enums.ENUM_MAPPINGS:
+        enum_class_mock = tweety_enums.ENUM_MAPPINGS[name]
+        # Assurer que les membres de l'enum sont initialisés si ce n'est pas déjà fait
+        # Normalement, la métaclasse s'en charge, mais une vérification ici peut être utile.
+        # hasattr(enum_class_mock, '_initialize_enum_members') and enum_class_mock._initialize_enum_members()
+        # La métaclasse devrait avoir déjà appelé _initialize_enum_members.
+        # On s'assure que la classe retournée a bien le jclass_provider si besoin (pas typique pour les enums statiques)
+        # et que son __name__ correspond bien au nom Java demandé.
+        # Les classes Enum mockées héritent de MockJClassCore, donc elles ont un __name__ et un class_name.
+        # Il faut s'assurer que le nom Java est bien celui attendu.
+        # Le MOCK_JAVA_CLASS_NAME est utilisé pour cela.
+        mock_logger.info(f"JClass('{name}') identifié comme une énumération Tweety. Retourne la classe mockée: {enum_class_mock}.")
+        _jclass_cache[name] = enum_class_mock
+        return enum_class_mock
+
+    mock_logger.debug(f"JClass('{name}'): Pas une énumération Tweety connue. Création d'une instance de MockJClassCore.")
+    # Passer la fonction JClass elle-même pour que MockJClassCore puisse la fournir aux configurateurs.
+    core_class_mock = MockJClassCore(name, jclass_provider_func=JClass)
+
+    # La configuration des reasoners et agents est maintenant gérée dans MockJClassCore.__call__
+    # via le module tweety_reasoners.
+    # Exemple:
+    # if name.startswith("org.tweetyproject.arg"):
+    #     from .jpype_components import tweety_syntax # ou autre module pertinent
+    #     tweety_syntax.configure_tweety_argument_class(core_class_mock)
+    # elif name.startswith("org.tweetyproject.logics"):
+    #     # ...
+    #     pass
+
+    _jclass_cache[name] = core_class_mock
+    return core_class_mock
+
+
+# --- Patchs finaux et exports ---
+
+# Le patch pour jpype.imports._jpype.isStarted est maintenant géré par
+# le fait que _MockInternalJpypeModule utilise _jvm_started de jvm.py.
+
+# Le module `imports` (qui est `jpype_components.imports.imports_module`)
+# est déjà dans sys.modules['jpype.imports'] grâce à son propre code d'initialisation.
+# On s'assure juste qu'il est exposé par ce module principal si on fait `import jpype_mock; jpype_mock.imports`.
+# La variable `imports` importée de `.jpype_components.imports` est déjà le module correct.
+
+# Les variables globales _jvm_started et _jvm_path sont gérées dans jvm.py.
+# Les fonctions comme isJVMStarted, startJVM, etc., sont importées directement.
+# L'instance `config` est importée directement.
+# Les types JString, JArray, etc., sont importés directement.
+# Les exceptions JException, JVMNotFoundException sont importées directement.
+
+mock_logger.info("Mock JPype1 (jpype_mock.py principal) initialisé et refactorisé.")
+mock_logger.info("Les composants sont maintenant dans le package 'jpype_components'.")
+
+# Optionnel: Exposer explicitement ce qui fait partie de l'API publique du mock
+# __all__ = [
+#     "__version__",
+#     "isJVMStarted", "startJVM", "shutdownJVM",
+#     "getDefaultJVMPath", "getJVMPath", "getJVMVersion", "getClassPath",
+#     "config",
+#     "imports",
+#     "java", # Pour jpype.java.xxx
+#     "JClass", "JString", "JArray", "JObject",
+#     "JBoolean", "JInt", "JDouble", "JLong", "JFloat", "JShort", "JByte", "JChar",
+#     "JException", "JVMNotFoundException",
+#     # Les classes MockJClassCore, MockJavaCollection ne sont pas typiquement accédées directement
+#     # par l'utilisateur du mock, mais via JClass() ou des méthodes retournant des collections.
+# ]
+
+# Exposer l'instance du mock interne _jpype pour que conftest.py puisse l'importer
+_jpype = _jpype_core_mock_instance # Exposer l'instance mockée
+# La deuxième assignation était redondante
diff --git a/tests/mocks/jpype_setup.py b/tests/mocks/jpype_setup.py
deleted file mode 100644
index 5629bdb8..00000000
--- a/tests/mocks/jpype_setup.py
+++ /dev/null
@@ -1,328 +0,0 @@
-import sys
-import os
-import pytest
-from unittest.mock import MagicMock
-import importlib.util
-from argumentation_analysis.core.jvm_setup import shutdown_jvm_if_needed
-import logging
-
-# --- Configuration du Logger ---
-logger = logging.getLogger(__name__)
-# Configuration basique si le logger n'est pas déjà configuré par pytest ou autre
-if not logger.handlers:
-    handler = logging.StreamHandler(sys.stdout)
-    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
-    handler.setFormatter(formatter)
-    logger.addHandler(handler)
-    logger.setLevel(logging.INFO) # Ou logging.DEBUG pour plus de détails
-    logger.propagate = False
-
-# --- Détermination de la disponibilité du vrai JPype via variable d'environnement ---
-# Cette variable est utilisée par les décorateurs skipif dans les fichiers de test.
-logger.info(f"jpype_setup.py: Évaluation de _REAL_JPYPE_AVAILABLE...")
-logger.info(f"jpype_setup.py: Valeur brute de os.environ.get('USE_REAL_JPYPE', 'false'): '{os.environ.get('USE_REAL_JPYPE', 'false')}'")
-_REAL_JPYPE_AVAILABLE = os.environ.get('USE_REAL_JPYPE', 'false').lower() in ('true', '1')
-logger.info(f"jpype_setup.py: _REAL_JPYPE_AVAILABLE évalué à: {_REAL_JPYPE_AVAILABLE}")
-# Les prints de débogage précédents ont confirmé que _REAL_JPYPE_AVAILABLE est correctement évalué.
-# La cause du skip était une erreur dans la fixture integration_jvm (chemin des libs).
-
-
-# --- Sauvegarde du module JPype potentiellement pré-importé ou import frais ---
-_REAL_JPYPE_MODULE = None
-_PRE_EXISTING_JPYPE_IN_SYS_MODULES = sys.modules.get('jpype')
-
-if _PRE_EXISTING_JPYPE_IN_SYS_MODULES:
-    _REAL_JPYPE_MODULE = _PRE_EXISTING_JPYPE_IN_SYS_MODULES
-    logger.info(f"jpype_setup.py: _REAL_JPYPE_MODULE initialisé à partir de _PRE_EXISTING_JPYPE_IN_SYS_MODULES (ID: {id(_REAL_JPYPE_MODULE)}).")
-else:
-    logger.info("jpype_setup.py: JPype non préchargé, tentative d'import frais.")
-    try:
-        import jpype as r_jpype_fresh_import
-        _REAL_JPYPE_MODULE = r_jpype_fresh_import
-        logger.info(f"jpype_setup.py: Vrai module JPype importé fraîchement (ID: {id(_REAL_JPYPE_MODULE)}).")
-    except ImportError as e_fresh_import:
-        logger.warning(f"jpype_setup.py: Le vrai module JPype n'a pas pu être importé fraîchement: {e_fresh_import}")
-        _REAL_JPYPE_MODULE = None
-    except NameError as e_name_error_fresh_import:
-        logger.error(f"jpype_setup.py: NameError lors de l'import frais de JPype: {e_name_error_fresh_import}.")
-        _REAL_JPYPE_MODULE = None
-
-if _REAL_JPYPE_MODULE is None:
-    logger.error("jpype_setup.py: _REAL_JPYPE_MODULE EST NONE après la tentative d'initialisation.")
-else:
-    logger.info(f"jpype_setup.py: _REAL_JPYPE_MODULE est initialisé (ID: {id(_REAL_JPYPE_MODULE)}) avant la définition des fixtures.")
-
-# --- Mock JPype ---
-try:
-    from tests.mocks import jpype_mock # Importer le module via son chemin de package
-    # Importer le vrai module mock d'imports depuis le sous-package jpype_components
-    from tests.mocks.jpype_components.imports import imports_module as actual_mock_jpype_imports_module
-
-    jpype_module_mock_obj = MagicMock(name="jpype_module_mock")
-    jpype_module_mock_obj.__path__ = []
-    jpype_module_mock_obj.isJVMStarted = jpype_mock.isJVMStarted
-    jpype_module_mock_obj.startJVM = jpype_mock.startJVM
-    jpype_module_mock_obj.getJVMPath = jpype_mock.getJVMPath
-    jpype_module_mock_obj.getJVMVersion = jpype_mock.getJVMVersion
-    jpype_module_mock_obj.getDefaultJVMPath = jpype_mock.getDefaultJVMPath
-    jpype_module_mock_obj.JClass = jpype_mock.JClass
-    jpype_module_mock_obj.JException = jpype_mock.JException
-    jpype_module_mock_obj.JObject = jpype_mock.JObject
-    jpype_module_mock_obj.JVMNotFoundException = jpype_mock.JVMNotFoundException
-    jpype_module_mock_obj.__version__ = '1.4.1.mock' # ou jpype_mock.__version__ si défini
-    jpype_module_mock_obj.imports = actual_mock_jpype_imports_module
-    _JPYPE_MODULE_MOCK_OBJ_GLOBAL = jpype_module_mock_obj
-    _MOCK_DOT_JPYPE_MODULE_GLOBAL = jpype_mock._jpype # Accéder à _jpype depuis le module jpype_mock importé
-    logger.info("jpype_setup.py: Mock JPype préparé.")
-except ImportError as e_jpype:
-    logger.error(f"jpype_setup.py: ERREUR CRITIQUE lors de l'import de jpype_mock ou ses composants: {e_jpype}. Utilisation de mocks de fallback pour JPype.")
-    _fb_jpype_mock = MagicMock(name="jpype_fallback_mock")
-    _fb_jpype_mock.imports = MagicMock(name="jpype.imports_fallback_mock")
-    _fb_dot_jpype_mock = MagicMock(name="_jpype_fallback_mock")
-
-    _JPYPE_MODULE_MOCK_OBJ_GLOBAL = _fb_jpype_mock
-    _MOCK_DOT_JPYPE_MODULE_GLOBAL = _fb_dot_jpype_mock
-    logger.info("jpype_setup.py: Mock JPype de FALLBACK préparé et assigné aux variables globales de mock.")
-
-
-@pytest.fixture(scope="function", autouse=True)
-def activate_jpype_mock_if_needed(request):
-    """
-    Fixture à portée "function" et "autouse=True" pour gérer la sélection entre le mock JPype et le vrai JPype.
-
-    Logique de sélection :
-    1. Si un test est marqué avec `@pytest.mark.real_jpype`, le vrai module JPype (`_REAL_JPYPE_MODULE`)
-       est placé dans `sys.modules['jpype']`.
-    2. Si le chemin du fichier de test contient 'tests/integration/' ou 'tests/minimal_jpype_tweety_tests/',
-       le vrai JPype est également utilisé.
-    3. Dans tous les autres cas (tests unitaires par défaut), le mock JPype (`_JPYPE_MODULE_MOCK_OBJ_GLOBAL`)
-       est activé.
-
-    Gestion de l'état du mock :
-    - Avant chaque test utilisant le mock, l'état interne du mock JPype est réinitialisé :
-        - `tests.mocks.jpype_components.jvm._jvm_started` est mis à `False`.
-        - `tests.mocks.jpype_components.jvm._jvm_path` est mis à `None`.
-        - `_JPYPE_MODULE_MOCK_OBJ_GLOBAL.config.jvm_path` est mis à `None`.
-      Cela garantit que chaque test unitaire commence avec une JVM mockée "propre" et non démarrée.
-      `jpype.isJVMStarted()` (version mockée) retournera donc `False` au début de ces tests.
-      Un appel à `jpype.startJVM()` (version mockée) mettra `_jvm_started` à `True` pour la durée du test.
-
-    Restauration :
-    - Après chaque test, l'état original de `sys.modules['jpype']`, `sys.modules['_jpype']`,
-      et `sys.modules['jpype.imports']` est restauré.
-
-    Interaction avec `integration_jvm` :
-    - Pour les tests nécessitant la vraie JVM (marqués `real_jpype` ou dans les chemins d'intégration),
-      cette fixture s'assure que le vrai `jpype` est dans `sys.modules`. La fixture `integration_jvm`
-      (scope session), définie dans `integration_fixtures.py`, est alors responsable du démarrage
-      effectif de la vraie JVM une fois par session et de sa gestion.
-    """
-    global _JPYPE_MODULE_MOCK_OBJ_GLOBAL, _MOCK_DOT_JPYPE_MODULE_GLOBAL, _REAL_JPYPE_MODULE
-    
-    use_real_jpype = False
-    if request.node.get_closest_marker("real_jpype"):
-        use_real_jpype = True
-    path_str = str(request.node.fspath).replace(os.sep, '/')
-    if 'tests/integration/' in path_str or 'tests/minimal_jpype_tweety_tests/' in path_str:
-        use_real_jpype = True
-
-    if use_real_jpype:
-        logger.info(f"Test {request.node.name} demande REAL JPype. Configuration de sys.modules pour utiliser le vrai JPype.")
-        if _REAL_JPYPE_MODULE:
-            sys.modules['jpype'] = _REAL_JPYPE_MODULE
-            if hasattr(_REAL_JPYPE_MODULE, '_jpype'):
-                sys.modules['_jpype'] = _REAL_JPYPE_MODULE._jpype
-            elif '_jpype' in sys.modules and sys.modules.get('_jpype') is not getattr(_REAL_JPYPE_MODULE, '_jpype', None) :
-                del sys.modules['_jpype']
-            if hasattr(_REAL_JPYPE_MODULE, 'imports'):
-                sys.modules['jpype.imports'] = _REAL_JPYPE_MODULE.imports
-            elif 'jpype.imports' in sys.modules and sys.modules.get('jpype.imports') is not getattr(_REAL_JPYPE_MODULE, 'imports', None):
-                del sys.modules['jpype.imports']
-            logger.debug(f"REAL JPype (ID: {id(_REAL_JPYPE_MODULE)}) est maintenant sys.modules['jpype'].")
-        else:
-            logger.error(f"Test {request.node.name} demande REAL JPype, mais _REAL_JPYPE_MODULE n'est pas disponible. Test échouera probablement.")
-        yield
-    else:
-        logger.info(f"Test {request.node.name} utilise MOCK JPype.")
-        
-        # Réinitialiser l'état _jvm_started et _jvm_path du mock JPype avant chaque test l'utilisant.
-        try:
-            # L'import est fait ici pour éviter une dépendance circulaire si jvm.py importe depuis jpype_setup
-            jpype_components_jvm_module = importlib.import_module('tests.mocks.jpype_components.jvm')
-            if hasattr(jpype_components_jvm_module, '_jvm_started'):
-                jpype_components_jvm_module._jvm_started = False
-            if hasattr(jpype_components_jvm_module, '_jvm_path'):
-                jpype_components_jvm_module._jvm_path = None
-            if hasattr(_JPYPE_MODULE_MOCK_OBJ_GLOBAL, 'config') and hasattr(_JPYPE_MODULE_MOCK_OBJ_GLOBAL.config, 'jvm_path'):
-                _JPYPE_MODULE_MOCK_OBJ_GLOBAL.config.jvm_path = None
-
-            logger.info("État (_jvm_started, _jvm_path, config.jvm_path) du mock JPype réinitialisé pour le test.")
-        except Exception as e_reset_mock:
-            logger.error(f"Erreur lors de la réinitialisation de l'état du mock JPype: {e_reset_mock}")
-
-        original_sys_jpype = sys.modules.get('jpype')
-        original_sys_dot_jpype = sys.modules.get('_jpype')
-        original_sys_jpype_imports = sys.modules.get('jpype.imports')
-
-        sys.modules['jpype'] = _JPYPE_MODULE_MOCK_OBJ_GLOBAL
-        sys.modules['_jpype'] = _MOCK_DOT_JPYPE_MODULE_GLOBAL
-        assert sys.modules['jpype'] is _JPYPE_MODULE_MOCK_OBJ_GLOBAL, "Mock JPype global n'a pas été correctement appliqué!"
-        yield
-
-        if original_sys_jpype is not None:
-            sys.modules['jpype'] = original_sys_jpype
-        elif 'jpype' in sys.modules:
-             del sys.modules['jpype']
-        if original_sys_dot_jpype is not None:
-            sys.modules['_jpype'] = original_sys_dot_jpype
-        elif '_jpype' in sys.modules:
-            del sys.modules['_jpype']
-        if original_sys_jpype_imports is not None:
-            sys.modules['jpype.imports'] = original_sys_jpype_imports
-        elif 'jpype.imports' in sys.modules:
-            del sys.modules['jpype.imports']
-        logger.info(f"État de JPype restauré après test {request.node.name} (utilisation du mock).")
-
-def pytest_sessionstart(session):
-    global _REAL_JPYPE_MODULE, _JPYPE_MODULE_MOCK_OBJ_GLOBAL, logger
-    logger.info("jpype_setup.py: pytest_sessionstart hook triggered.")
-    if not hasattr(logger, 'info'):
-        import logging
-        logger = logging.getLogger(__name__)
-
-    if _REAL_JPYPE_MODULE and _REAL_JPYPE_MODULE is not _JPYPE_MODULE_MOCK_OBJ_GLOBAL:
-        logger.info("   pytest_sessionstart: Real JPype module is available.")
-        # try:
-            # La logique de configuration de destroy_jvm et l'import de jpype.config
-            # sont maintenant gérés de manière centralisée par initialize_jvm lors du premier démarrage réel.
-            # Commenter cette section pour éviter les conflits ou les configurations prématurées.
-            # original_sys_jpype_module = sys.modules.get('jpype')
-            # if sys.modules.get('jpype') is not _REAL_JPYPE_MODULE:
-            #     sys.modules['jpype'] = _REAL_JPYPE_MODULE
-            #     logger.info("   pytest_sessionstart: Temporarily set sys.modules['jpype'] to _REAL_JPYPE_MODULE for config import.")
-
-            # if not hasattr(_REAL_JPYPE_MODULE, 'config') or _REAL_JPYPE_MODULE.config is None:
-            #     logger.info("   pytest_sessionstart: Attempting to import jpype.config explicitly.")
-            #     import jpype.config # This might be problematic if called before JVM start or with wrong classpath context
-            
-            # if original_sys_jpype_module is not None and sys.modules.get('jpype') is not original_sys_jpype_module:
-            #     sys.modules['jpype'] = original_sys_jpype_module
-            #     logger.info("   pytest_sessionstart: Restored original sys.modules['jpype'].")
-            # elif original_sys_jpype_module is None and 'jpype' in sys.modules and sys.modules['jpype'] is _REAL_JPYPE_MODULE:
-            #     pass # It was correctly set
-            
-            # Tentative d'assurer que jpype.config est le vrai config, si possible.
-            # initialize_jvm s'occupera de mettre destroy_jvm à False.
-            # Bloc try/except correctement indenté :
-        try:
-            if not hasattr(_REAL_JPYPE_MODULE, 'config') or _REAL_JPYPE_MODULE.config is None:
-                logger.info("   pytest_sessionstart: _REAL_JPYPE_MODULE.config non trouvé, tentative d'import de jpype.config.")
-                _current_sys_jpype = sys.modules.get('jpype')
-                sys.modules['jpype'] = _REAL_JPYPE_MODULE
-                import jpype.config
-                sys.modules['jpype'] = _current_sys_jpype
-                logger.info(f"   pytest_sessionstart: Import de jpype.config tenté. hasattr(config): {hasattr(_REAL_JPYPE_MODULE, 'config')}")
-
-            if hasattr(_REAL_JPYPE_MODULE, 'config') and _REAL_JPYPE_MODULE.config is not None:
-                if 'jpype.config' not in sys.modules or sys.modules.get('jpype.config') is not _REAL_JPYPE_MODULE.config:
-                    sys.modules['jpype.config'] = _REAL_JPYPE_MODULE.config
-                    logger.info("   pytest_sessionstart: Assuré que sys.modules['jpype.config'] est _REAL_JPYPE_MODULE.config.")
-            else:
-                logger.warning("   pytest_sessionstart: _REAL_JPYPE_MODULE.config toujours non disponible après tentative d'import.")
-
-        except ImportError as e_cfg_imp_sess_start:
-            logger.error(f"   pytest_sessionstart: ImportError lors de la tentative d'import de jpype.config: {e_cfg_imp_sess_start}")
-        except Exception as e_sess_start_cfg:
-            logger.error(f"   pytest_sessionstart: Erreur inattendue lors de la manipulation de jpype.config: {e_sess_start_cfg}", exc_info=True)
-
-        logger.info("   pytest_sessionstart: La configuration de jpype.config.destroy_jvm est gérée par initialize_jvm.")
-    elif _JPYPE_MODULE_MOCK_OBJ_GLOBAL and _REAL_JPYPE_MODULE is _JPYPE_MODULE_MOCK_OBJ_GLOBAL:
-        logger.info("   pytest_sessionstart: JPype module is the MOCK. No changes to destroy_jvm needed for the mock.")
-    else:
-        logger.info("   pytest_sessionstart: Real JPype module not definitively available or identified as mock. La configuration de jpype.config est gérée par initialize_jvm.")
-
-def pytest_sessionfinish(session, exitstatus):
-    global _REAL_JPYPE_MODULE, _JPYPE_MODULE_MOCK_OBJ_GLOBAL, logger
-    logger.info(f"jpype_setup.py: pytest_sessionfinish hook triggered. Exit status: {exitstatus}")
-
-    # Déterminer si le vrai JPype a été utilisé pour la session ou le dernier test
-    # Cela est une heuristique. Idéalement, on saurait si la JVM a été démarrée par notre code.
-    real_jpype_was_potentially_used = False
-    if _REAL_JPYPE_MODULE and sys.modules.get('jpype') is _REAL_JPYPE_MODULE:
-        logger.info("   pytest_sessionfinish: sys.modules['jpype'] IS _REAL_JPYPE_MODULE. Le vrai JPype a potentiellement été utilisé.")
-        real_jpype_was_potentially_used = True
-    elif _REAL_JPYPE_MODULE and _REAL_JPYPE_MODULE is not _JPYPE_MODULE_MOCK_OBJ_GLOBAL:
-        logger.info("   pytest_sessionfinish: _REAL_JPYPE_MODULE est disponible et n'est pas le mock global. Le vrai JPype a potentiellement été utilisé.")
-        real_jpype_was_potentially_used = True
-    else:
-        logger.info("   pytest_sessionfinish: sys.modules['jpype'] n'est pas _REAL_JPYPE_MODULE ou _REAL_JPYPE_MODULE est le mock. Le mock JPype a probablement été utilisé.")
-
-    if real_jpype_was_potentially_used:
-        logger.info("   pytest_sessionfinish: Tentative d'arrêt de la JVM via shutdown_jvm_if_needed() car le vrai JPype a potentiellement été utilisé.")
-        try:
-            # S'assurer que le vrai jpype est dans sys.modules pour que shutdown_jvm_if_needed fonctionne correctement
-            original_jpype_in_sys = sys.modules.get('jpype')
-            if original_jpype_in_sys is not _REAL_JPYPE_MODULE and _REAL_JPYPE_MODULE is not None:
-                logger.info(f"   pytest_sessionfinish: Temporairement, sys.modules['jpype'] = _REAL_JPYPE_MODULE (ID: {id(_REAL_JPYPE_MODULE)}) pour shutdown.")
-                sys.modules['jpype'] = _REAL_JPYPE_MODULE
-            
-            shutdown_jvm_if_needed() # Appel de notre fonction centralisée
-            
-            # Restaurer l'état précédent de sys.modules['jpype'] si modifié
-            if original_jpype_in_sys is not None and sys.modules.get('jpype') is not original_jpype_in_sys:
-                logger.info(f"   pytest_sessionfinish: Restauration de sys.modules['jpype'] à son état original (ID: {id(original_jpype_in_sys)}).")
-                sys.modules['jpype'] = original_jpype_in_sys
-            elif original_jpype_in_sys is None and 'jpype' in sys.modules: # Si on l'a ajouté et qu'il n'y était pas
-                del sys.modules['jpype']
-                logger.info("   pytest_sessionfinish: sys.modules['jpype'] supprimé car il n'était pas là initialement.")
-
-        except Exception as e_shutdown:
-            logger.error(f"   pytest_sessionfinish: Erreur lors de l'appel à shutdown_jvm_if_needed(): {e_shutdown}", exc_info=True)
-        
-        # La logique ci-dessous pour restaurer sys.modules['jpype'] et sys.modules['jpype.config']
-        # est importante si la JVM n'est PAS arrêtée par JPype via atexit (destroy_jvm=False).
-        # Si shutdown_jvm_if_needed() a bien arrêté la JVM, cette partie est moins critique mais ne fait pas de mal.
-        logger.info("   pytest_sessionfinish: Vérification de l'état de la JVM après tentative d'arrêt.")
-        try:
-            jvm_still_started_after_shutdown_attempt = False
-            if _REAL_JPYPE_MODULE and hasattr(_REAL_JPYPE_MODULE, 'isJVMStarted'):
-                 # Assurer que _REAL_JPYPE_MODULE est utilisé pour la vérification
-                _current_jpype_for_check = sys.modules.get('jpype')
-                if _current_jpype_for_check is not _REAL_JPYPE_MODULE and _REAL_JPYPE_MODULE is not None:
-                    sys.modules['jpype'] = _REAL_JPYPE_MODULE
-                jvm_still_started_after_shutdown_attempt = _REAL_JPYPE_MODULE.isJVMStarted()
-                if _current_jpype_for_check is not None and _current_jpype_for_check is not _REAL_JPYPE_MODULE: # restaurer
-                    sys.modules['jpype'] = _current_jpype_for_check
-                elif _current_jpype_for_check is None and 'jpype' in sys.modules and sys.modules['jpype'] is _REAL_JPYPE_MODULE:
-                    del sys.modules['jpype']
-
-
-            logger.info(f"   pytest_sessionfinish: JVM encore démarrée après tentative d'arrêt: {jvm_still_started_after_shutdown_attempt}")
-
-            destroy_jvm_is_false = False # Valeur par défaut si config non accessible
-            if _REAL_JPYPE_MODULE and hasattr(_REAL_JPYPE_MODULE, 'config') and _REAL_JPYPE_MODULE.config is not None and hasattr(_REAL_JPYPE_MODULE.config, 'destroy_jvm'):
-                destroy_jvm_is_false = not _REAL_JPYPE_MODULE.config.destroy_jvm
-            logger.info(f"   pytest_sessionfinish: destroy_jvm est False (selon config): {destroy_jvm_is_false}")
-
-            if jvm_still_started_after_shutdown_attempt and destroy_jvm_is_false:
-                logger.info("   pytest_sessionfinish: JVM est toujours active et destroy_jvm est False. Assurer la présence des modules jpype pour atexit.")
-                current_sys_jpype = sys.modules.get('jpype')
-                if current_sys_jpype is not _REAL_JPYPE_MODULE and _REAL_JPYPE_MODULE is not None:
-                    logger.warning(f"   pytest_sessionfinish: sys.modules['jpype'] (ID: {id(current_sys_jpype)}) n'est pas _REAL_JPYPE_MODULE (ID: {id(_REAL_JPYPE_MODULE)}). Restauration de _REAL_JPYPE_MODULE.")
-                    sys.modules['jpype'] = _REAL_JPYPE_MODULE
-                
-                if _REAL_JPYPE_MODULE and hasattr(_REAL_JPYPE_MODULE, 'config') and _REAL_JPYPE_MODULE.config is not None:
-                    current_sys_jpype_config = sys.modules.get('jpype.config')
-                    if current_sys_jpype_config is not _REAL_JPYPE_MODULE.config:
-                        logger.warning(f"   pytest_sessionfinish: sys.modules['jpype.config'] (ID: {id(current_sys_jpype_config)}) n'est pas _REAL_JPYPE_MODULE.config (ID: {id(_REAL_JPYPE_MODULE.config)}). Restauration.")
-                        sys.modules['jpype.config'] = _REAL_JPYPE_MODULE.config
-                else:
-                    logger.warning("   pytest_sessionfinish: _REAL_JPYPE_MODULE.config non disponible, ne peut pas assurer sys.modules['jpype.config'].")
-            else:
-                logger.info("   pytest_sessionfinish: JVM non démarrée ou destroy_jvm est True. Pas de gestion spéciale de sys.modules pour atexit depuis ici.")
-        except AttributeError as ae:
-             logger.error(f"   pytest_sessionfinish: AttributeError (vérification post-arrêt): {ae}.", exc_info=True)
-        except Exception as e:
-            logger.error(f"   pytest_sessionfinish: Erreur inattendue (vérification post-arrêt): {type(e).__name__}: {e}", exc_info=True)
-    else:
-        logger.info("   pytest_sessionfinish: Le mock JPype a probablement été utilisé. Aucun arrêt de JVM nécessaire depuis ici.")
\ No newline at end of file
diff --git a/tests/mocks/numpy_setup.py b/tests/mocks/numpy_setup.py
index c98e2052..c83c2156 100644
--- a/tests/mocks/numpy_setup.py
+++ b/tests/mocks/numpy_setup.py
@@ -1,16 +1,295 @@
 import sys
-import os
 from unittest.mock import MagicMock
 import pytest
-import importlib
-import logging
+import importlib # Ajouté pour numpy_mock si besoin d'import dynamique
+import logging # Ajout pour la fonction helper
+from types import ModuleType # Ajouté pour créer des objets modules
 
+# Configuration du logger pour ce module si pas déjà fait globalement
+# Ceci est un exemple, adaptez selon la configuration de logging du projet.
+# Si un logger est déjà configuré au niveau racine et propagé, ceci n'est pas nécessaire.
 logger = logging.getLogger(__name__)
+# Pour s'assurer que les messages INFO de la fonction helper sont visibles pendant le test:
+# if not logger.handlers: # Décommentez et ajustez si les logs ne s'affichent pas comme attendu
+#     handler = logging.StreamHandler(sys.stdout)
+#     handler.setFormatter(logging.Formatter('%(levelname)s:%(name)s:%(message)s'))
+#     logger.addHandler(handler)
+#     logger.setLevel(logging.INFO)
 
-def is_module_available(module_name):
-    """Vérifie si un module est réellement installé, sans être un mock."""
-    if module_name in sys.modules and isinstance(sys.modules[module_name], MagicMock):
-        return False
+
+# DÉBUT : Fonction helper à ajouter
+def deep_delete_from_sys_modules(module_name_prefix, logger_instance):
+    keys_to_delete = [k for k in sys.modules if k == module_name_prefix or k.startswith(module_name_prefix + '.')]
+    if keys_to_delete:
+        logger_instance.info(f"Nettoyage des modules sys pour préfixe '{module_name_prefix}': {keys_to_delete}")
+    for key in keys_to_delete:
+        try:
+            del sys.modules[key]
+        except KeyError:
+            logger_instance.warning(f"Clé '{key}' non trouvée dans sys.modules lors de la tentative de suppression (deep_delete).")
+# FIN : Fonction helper
+
+
+# Tentative d'importation de numpy_mock. S'il est dans le même répertoire (tests/mocks), cela devrait fonctionner.
+try:
+    import legacy_numpy_array_mock # legacy_numpy_array_mock.py devrait définir .core, ._core, et dans ceux-ci, ._multiarray_umath
+except ImportError:
+    print("ERREUR: numpy_setup.py: Impossible d'importer numpy_mock directement.")
+    numpy_mock = MagicMock(name="numpy_mock_fallback_in_numpy_setup")
+    numpy_mock.typing = MagicMock()
+    numpy_mock._core = MagicMock() 
+    numpy_mock.core = MagicMock()  
+    numpy_mock.linalg = MagicMock()
+    numpy_mock.fft = MagicMock()
+    numpy_mock.lib = MagicMock()
+    numpy_mock.__version__ = '1.24.3.mock_fallback'
+    if hasattr(numpy_mock._core, 'multiarray'): # Pourrait être redondant si core est bien mocké
+        numpy_mock._core.multiarray = MagicMock()
+    if hasattr(numpy_mock.core, 'multiarray'):
+        numpy_mock.core.multiarray = MagicMock()
+    if hasattr(numpy_mock.core, 'numeric'):
+        numpy_mock.core.numeric = MagicMock()
+    if hasattr(numpy_mock._core, 'numeric'):
+        numpy_mock._core.numeric = MagicMock()
+
+
+class MockRecarray:
+    def __init__(self, *args, **kwargs):
+        self.args = args
+        self.kwargs = kwargs
+        shape_arg = kwargs.get('shape')
+        if shape_arg is not None:
+            self.shape = shape_arg
+        elif args and isinstance(args[0], tuple): 
+             self.shape = args[0]
+        elif args and args[0] is not None: 
+             self.shape = (args[0],)
+        else:
+             self.shape = (0,) 
+        self.dtype = MagicMock(name="recarray_dtype_mock")
+        names_arg = kwargs.get('names')
+        self.dtype.names = list(names_arg) if names_arg is not None else []
+        self._formats = kwargs.get('formats') 
+
+    @property
+    def names(self):
+        return self.dtype.names
+
+    @property
+    def formats(self):
+        return self._formats
+
+    def __getattr__(self, name):
+        if name == 'names': 
+            return self.dtype.names
+        if name == 'formats': 
+            return self._formats
+        if name in self.kwargs.get('names', []):
+            field_mock = MagicMock(name=f"MockRecarray.field.{name}")
+            return field_mock
+        if name in ['shape', 'dtype', 'args', 'kwargs']:
+            return object.__getattribute__(self, name)
+        return MagicMock(name=f"MockRecarray.unhandled.{name}")
+
+    def __getitem__(self, key):
+        if isinstance(key, str) and key in self.kwargs.get('names', []):
+            field_mock = MagicMock(name=f"MockRecarray.field_getitem.{key}")
+            field_mock.__getitem__ = lambda idx: MagicMock(name=f"MockRecarray.field_getitem.{key}.item_{idx}")
+            return field_mock
+        elif isinstance(key, int):
+            row_mock = MagicMock(name=f"MockRecarray.row_{key}")
+            def get_field_from_row(field_name):
+                if field_name in self.kwargs.get('names', []):
+                    return MagicMock(name=f"MockRecarray.row_{key}.field_{field_name}")
+                raise KeyError(field_name)
+            row_mock.__getitem__ = get_field_from_row
+            return row_mock
+        return MagicMock(name=f"MockRecarray.getitem.{key}")
+
+def _install_numpy_mock_immediately():
+    print("INFO: numpy_setup.py: _install_numpy_mock_immediately: Tentative d'installation/réinstallation du mock NumPy.")
+    try:
+        # Utiliser legacy_numpy_array_mock directement ici
+        mock_numpy_attrs = {attr: getattr(legacy_numpy_array_mock, attr) for attr in dir(legacy_numpy_array_mock) if not attr.startswith('__')}
+        mock_numpy_attrs['__version__'] = legacy_numpy_array_mock.__version__ if hasattr(legacy_numpy_array_mock, '__version__') else '1.24.3.mock'
+        
+        mock_numpy_module = type('numpy', (), mock_numpy_attrs)
+        mock_numpy_module.__path__ = []
+        sys.modules['numpy'] = mock_numpy_module
+        
+        if hasattr(legacy_numpy_array_mock, 'typing'):
+            sys.modules['numpy.typing'] = legacy_numpy_array_mock.typing
+
+        # Configuration de numpy.core comme un module
+        if hasattr(legacy_numpy_array_mock, 'core'):
+            numpy_core_obj = type('core', (object,), {})
+            numpy_core_obj.__name__ = 'numpy.core'
+            numpy_core_obj.__package__ = 'numpy'
+            numpy_core_obj.__path__ = [] 
+            
+            # Assigner les attributs de la classe numpy_mock.core à l'objet module
+            # (legacy_numpy_array_mock.core est la classe définie dans legacy_numpy_array_mock.py)
+            # (legacy_numpy_array_mock.core._multiarray_umath est _multiarray_umath_mock_instance)
+            if hasattr(legacy_numpy_array_mock.core, '_multiarray_umath'):
+                # Créer un véritable objet ModuleType pour _multiarray_umath
+                umath_module_name_core = 'numpy.core._multiarray_umath'
+                umath_mock_obj_core = ModuleType(umath_module_name_core)
+                
+                # Copier les attributs de l'instance de _NumPy_Core_Multiarray_Umath_Mock
+                # vers le nouvel objet module. legacy_numpy_array_mock.core._multiarray_umath est l'instance.
+                source_mock_instance_core = legacy_numpy_array_mock.core._multiarray_umath
+                for attr_name in dir(source_mock_instance_core):
+                    if not attr_name.startswith('__') or attr_name in ['__name__', '__package__', '__path__']: # Copier certains dunders
+                        setattr(umath_mock_obj_core, attr_name, getattr(source_mock_instance_core, attr_name))
+                
+                # S'assurer que les attributs essentiels de module sont là
+                if not hasattr(umath_mock_obj_core, '__name__'):
+                    umath_mock_obj_core.__name__ = umath_module_name_core
+                if not hasattr(umath_mock_obj_core, '__package__'):
+                    umath_mock_obj_core.__package__ = 'numpy.core'
+                if not hasattr(umath_mock_obj_core, '__path__'):
+                     umath_mock_obj_core.__path__ = [] # Les modules C n'ont pas de __path__ mais pour un mock c'est ok
+                # Forcer _ARRAY_API à None pour éviter les conflits
+                umath_mock_obj_core._ARRAY_API = None
+
+                numpy_core_obj._multiarray_umath = umath_mock_obj_core
+                sys.modules[umath_module_name_core] = umath_mock_obj_core
+                logger.info(f"NumpyMock: {umath_module_name_core} configuré comme ModuleType et défini dans sys.modules.")
+
+            if hasattr(legacy_numpy_array_mock.core, 'multiarray'): # legacy_numpy_array_mock.core.multiarray est une CLASSE vide
+                multiarray_module_name_core = 'numpy.core.multiarray'
+                multiarray_mock_obj_core = ModuleType(multiarray_module_name_core)
+                multiarray_mock_obj_core.__name__ = multiarray_module_name_core
+                multiarray_mock_obj_core.__package__ = 'numpy.core'
+                multiarray_mock_obj_core.__path__ = []
+                multiarray_mock_obj_core._ARRAY_API = None # Forcer à None
+
+                # Potentiellement copier d'autres attributs si _NumPy_Core_Multiarray_Mock était plus fournie
+                # source_multiarray_cls_core = legacy_numpy_array_mock.core.multiarray
+                # try:
+                #     # Si c'est une classe avec des attributs statiques ou un __init__ simple
+                #     # pour une instance temporaire afin de copier les attributs.
+                #     temp_instance = source_multiarray_cls_core()
+                #     for attr_name_ma in dir(temp_instance):
+                #         if not attr_name_ma.startswith('__') or attr_name_ma in ['__name__', '__package__', '__path__']:
+                #             setattr(multiarray_mock_obj_core, attr_name_ma, getattr(temp_instance, attr_name_ma))
+                # except TypeError: # Si la classe ne peut pas être instanciée simplement
+                #     logger.warning(f"NumpyMock: La classe {source_multiarray_cls_core} pour multiarray n'a pas pu être instanciée pour copier les attributs.")
+                #     pass
+
+
+                numpy_core_obj.multiarray = multiarray_mock_obj_core
+                sys.modules[multiarray_module_name_core] = multiarray_mock_obj_core
+                logger.info(f"NumpyMock: {multiarray_module_name_core} configuré comme ModuleType et défini dans sys.modules.")
+
+            if hasattr(legacy_numpy_array_mock.core, 'numeric'):
+                numpy_core_obj.numeric = legacy_numpy_array_mock.core.numeric
+            for attr_name in dir(legacy_numpy_array_mock.core):
+                if not attr_name.startswith('__') and not hasattr(numpy_core_obj, attr_name):
+                    setattr(numpy_core_obj, attr_name, getattr(legacy_numpy_array_mock.core, attr_name))
+            
+            sys.modules['numpy.core'] = numpy_core_obj
+            if hasattr(mock_numpy_module, '__dict__'):
+                mock_numpy_module.core = numpy_core_obj
+            logger.info(f"NumpyMock: numpy.core configuré comme module. _multiarray_umath présent: {hasattr(numpy_core_obj, '_multiarray_umath')}")
+
+        # Configuration de numpy._core comme un module
+        if hasattr(legacy_numpy_array_mock, '_core'):
+            numpy_underscore_core_obj = type('_core', (object,), {})
+            numpy_underscore_core_obj.__name__ = 'numpy._core'
+            numpy_underscore_core_obj.__package__ = 'numpy'
+            numpy_underscore_core_obj.__path__ = []
+
+            if hasattr(legacy_numpy_array_mock._core, '_multiarray_umath'):
+                # Créer un véritable objet ModuleType pour _multiarray_umath
+                umath_module_name_underscore_core = 'numpy._core._multiarray_umath'
+                umath_mock_obj_underscore_core = ModuleType(umath_module_name_underscore_core)
+
+                # Copier les attributs de l'instance de _NumPy_Core_Multiarray_Umath_Mock
+                source_mock_instance_underscore_core = legacy_numpy_array_mock._core._multiarray_umath
+                for attr_name in dir(source_mock_instance_underscore_core):
+                    if not attr_name.startswith('__') or attr_name in ['__name__', '__package__', '__path__']:
+                        setattr(umath_mock_obj_underscore_core, attr_name, getattr(source_mock_instance_underscore_core, attr_name))
+                
+                if not hasattr(umath_mock_obj_underscore_core, '__name__'):
+                    umath_mock_obj_underscore_core.__name__ = umath_module_name_underscore_core
+                if not hasattr(umath_mock_obj_underscore_core, '__package__'):
+                     umath_mock_obj_underscore_core.__package__ = 'numpy._core'
+                if not hasattr(umath_mock_obj_underscore_core, '__path__'):
+                     umath_mock_obj_underscore_core.__path__ = []
+                     # Forcer _ARRAY_API à None pour éviter les conflits
+                     umath_mock_obj_underscore_core._ARRAY_API = None
+
+                numpy_underscore_core_obj._multiarray_umath = umath_mock_obj_underscore_core
+                sys.modules[umath_module_name_underscore_core] = umath_mock_obj_underscore_core
+                logger.info(f"NumpyMock: {umath_module_name_underscore_core} configuré comme ModuleType et défini dans sys.modules.")
+            
+            if hasattr(legacy_numpy_array_mock._core, 'multiarray'): # legacy_numpy_array_mock._core.multiarray est une CLASSE vide
+                multiarray_module_name_underscore_core = 'numpy._core.multiarray'
+                multiarray_mock_obj_underscore_core = ModuleType(multiarray_module_name_underscore_core)
+                multiarray_mock_obj_underscore_core.__name__ = multiarray_module_name_underscore_core
+                multiarray_mock_obj_underscore_core.__package__ = 'numpy._core'
+                multiarray_mock_obj_underscore_core.__path__ = []
+                # Forcer _ARRAY_API à None pour éviter les conflits
+                multiarray_mock_obj_underscore_core._ARRAY_API = None
+
+                # Idem pour copier les attributs si _NumPy_Core_Multiarray_Mock était plus fournie
+                # source_multiarray_cls_underscore_core = legacy_numpy_array_mock._core.multiarray
+                # try:
+                #     temp_instance_uc = source_multiarray_cls_underscore_core()
+                #     for attr_name_ma_uc in dir(temp_instance_uc):
+                #         if not attr_name_ma_uc.startswith('__') or attr_name_ma_uc in ['__name__', '__package__', '__path__']:
+                #             setattr(multiarray_mock_obj_underscore_core, attr_name_ma_uc, getattr(temp_instance_uc, attr_name_ma_uc))
+                # except TypeError:
+                #     logger.warning(f"NumpyMock: La classe {source_multiarray_cls_underscore_core} pour _core.multiarray n'a pas pu être instanciée.")
+                #     pass
+
+                numpy_underscore_core_obj.multiarray = multiarray_mock_obj_underscore_core
+                sys.modules[multiarray_module_name_underscore_core] = multiarray_mock_obj_underscore_core
+                logger.info(f"NumpyMock: {multiarray_module_name_underscore_core} configuré comme ModuleType et défini dans sys.modules.")
+
+            if hasattr(legacy_numpy_array_mock._core, 'numeric'):
+                numpy_underscore_core_obj.numeric = legacy_numpy_array_mock._core.numeric
+            for attr_name in dir(legacy_numpy_array_mock._core):
+                if not attr_name.startswith('__') and not hasattr(numpy_underscore_core_obj, attr_name):
+                    setattr(numpy_underscore_core_obj, attr_name, getattr(legacy_numpy_array_mock._core, attr_name))
+            
+            sys.modules['numpy._core'] = numpy_underscore_core_obj
+            if hasattr(mock_numpy_module, '__dict__'):
+                mock_numpy_module._core = numpy_underscore_core_obj
+            logger.info(f"NumpyMock: numpy._core configuré comme module. _multiarray_umath présent: {hasattr(numpy_underscore_core_obj, '_multiarray_umath')}")
+        
+        _mock_rec_submodule = type('rec', (), {})
+        _mock_rec_submodule.recarray = MockRecarray
+        sys.modules['numpy.rec'] = _mock_rec_submodule
+
+        if 'numpy' in sys.modules and sys.modules['numpy'] is mock_numpy_module:
+            mock_numpy_module.rec = _mock_rec_submodule
+        else:
+            print("AVERTISSEMENT: numpy_setup.py: mock_numpy_module n'était pas sys.modules['numpy'] lors de l'attribution de .rec")
+            if 'numpy' in sys.modules and hasattr(sys.modules['numpy'], '__dict__'):
+                 setattr(sys.modules['numpy'], 'rec', _mock_rec_submodule)
+        
+        print(f"INFO: numpy_setup.py: Mock numpy.rec configuré. sys.modules['numpy.rec'] (ID: {id(sys.modules.get('numpy.rec'))}), mock_numpy_module.rec (ID: {id(getattr(mock_numpy_module, 'rec', None))})")
+        
+        if hasattr(legacy_numpy_array_mock, 'linalg'):
+             sys.modules['numpy.linalg'] = legacy_numpy_array_mock.linalg
+        if hasattr(legacy_numpy_array_mock, 'fft'):
+             sys.modules['numpy.fft'] = legacy_numpy_array_mock.fft
+        if hasattr(legacy_numpy_array_mock, 'lib'):
+             sys.modules['numpy.lib'] = legacy_numpy_array_mock.lib
+        
+        print("INFO: numpy_setup.py: Mock NumPy installé immédiatement (avec sous-modules).")
+    except ImportError as e:
+        print(f"ERREUR dans numpy_setup.py lors de l'installation immédiate du mock NumPy: {e}")
+    except Exception as e_global:
+        print(f"ERREUR GLOBALE dans numpy_setup.py/_install_numpy_mock_immediately: {type(e_global).__name__}: {e_global}")
+
+
+def is_module_available(module_name): 
+    if module_name in sys.modules:
+        if isinstance(sys.modules[module_name], MagicMock):
+            return True 
     try:
         spec = importlib.util.find_spec(module_name)
         return spec is not None
@@ -18,43 +297,246 @@ def is_module_available(module_name):
         return False
 
 def setup_numpy():
-    """
-    Décide d'utiliser le vrai NumPy ou le mock.
-    Le mock est utilisé si le vrai n'est pas installé ou si la version de Python est >= 3.12.
-    """
-    major, minor = sys.version_info.major, sys.version_info.minor
-    use_mock = (major == 3 and minor >= 12) or not is_module_available('numpy')
-
-    if use_mock:
-        logger.info("Utilisation du MOCK NumPy (depuis numpy_setup.py).")
-        try:
-            from . import numpy_mock
-            
-            # Le module numpy_mock lui-même est configuré pour être un mock de module.
-            # Il contient des sous-modules mockés comme `core`, `linalg`, etc.
-            sys.modules['numpy'] = numpy_mock
-            
-            # S'assurer que les sous-modules sont aussi dans sys.modules pour les imports directs
-            for sub_name in ['typing', 'core', '_core', 'linalg', 'fft', 'lib', 'random', 'rec']:
-                if hasattr(numpy_mock, sub_name):
-                    sys.modules[f'numpy.{sub_name}'] = getattr(numpy_mock, sub_name)
-            
-            return numpy_mock
-        except ImportError as e:
-            logger.error(f"Échec de l'import de tests.mocks.numpy_mock: {e}. Fallback sur MagicMock.")
-            mock_fallback = MagicMock(name="numpy_fallback_mock")
-            sys.modules['numpy'] = mock_fallback
-            return mock_fallback
+    if (sys.version_info.major == 3 and sys.version_info.minor >= 12) or not is_module_available('numpy'):
+        if not is_module_available('numpy'): print("NumPy non disponible, utilisation du mock (depuis numpy_setup.py).")
+        else: print("Python 3.12+ détecté, utilisation du mock NumPy (depuis numpy_setup.py).")
+        
+        _install_numpy_mock_immediately()
+        print("INFO: numpy_setup.py: Mock NumPy configuré dynamiquement via setup_numpy -> _install_numpy_mock_immediately.")
+        return sys.modules['numpy']
     else:
-        logger.info("Utilisation du VRAI NumPy (depuis numpy_setup.py).")
         import numpy
+        print(f"Utilisation de la vraie bibliothèque NumPy (version {getattr(numpy, '__version__', 'inconnue')}) (depuis numpy_setup.py).")
         return numpy
 
-@pytest.fixture(scope="session", autouse=True)
-def setup_numpy_for_tests_fixture():
-    """
-    Fixture de session qui configure NumPy (réel ou mock) pour toute la session de test.
-    `autouse=True` garantit qu'elle est exécutée au début de la session.
-    """
-    setup_numpy()
-    yield
\ No newline at end of file
+@pytest.fixture(scope="function", autouse=True)
+def setup_numpy_for_tests_fixture(request):
+    # Nettoyage FORCÉ au tout début de chaque exécution de la fixture
+    logger.info(f"Fixture numpy_setup pour {request.node.name}: Nettoyage FORCÉ initial systématique de numpy, pandas, scipy, sklearn.")
+    deep_delete_from_sys_modules("numpy", logger)
+    deep_delete_from_sys_modules("pandas", logger)
+    deep_delete_from_sys_modules("scipy", logger)
+    deep_delete_from_sys_modules("sklearn", logger)
+
+    use_real_numpy_marker = request.node.get_closest_marker("use_real_numpy")
+    real_jpype_marker = request.node.get_closest_marker("real_jpype")
+
+    print(f"DEBUG: numpy_setup.py: sys.path au début de la fixture pour {request.node.name}: {sys.path}")
+    
+    # L'état de numpy est capturé APRÈS le nettoyage forcé initial.
+    # Il devrait idéalement être None ici si le nettoyage a bien fonctionné.
+    numpy_state_before_this_fixture = sys.modules.get('numpy')
+    numpy_rec_state_before_this_fixture = sys.modules.get('numpy.rec')
+
+    _initial_numpy_after_forced_clean = sys.modules.get('numpy')
+    try:
+        if _initial_numpy_after_forced_clean:
+            print(f"DEBUG: numpy_setup.py: NumPy PRÉSENT dans sys.modules pour {request.node.name} APRÈS NETTOYAGE FORCÉ INITIAL: {getattr(_initial_numpy_after_forced_clean, '__version__', 'inconnue')} (ID: {id(_initial_numpy_after_forced_clean)}) from {getattr(_initial_numpy_after_forced_clean, '__file__', 'N/A')}")
+        else:
+            print(f"DEBUG: numpy_setup.py: NumPy NON PRÉSENT dans sys.modules pour {request.node.name} APRÈS NETTOYAGE FORCÉ INITIAL.")
+    except Exception as e_debug_initial:
+        print(f"DEBUG: numpy_setup.py: Erreur lors du log de NumPy APRÈS NETTOYAGE FORCÉ pour {request.node.name}: {e_debug_initial}")
+
+    if numpy_state_before_this_fixture: # Devrait être None si le nettoyage forcé a fonctionné
+        logger.info(f"Fixture pour {request.node.name}: État de sys.modules['numpy'] APRÈS NETTOYAGE FORCÉ (devrait être None): version {getattr(numpy_state_before_this_fixture, '__version__', 'N/A')}, ID {id(numpy_state_before_this_fixture)}")
+    else:
+        logger.info(f"Fixture pour {request.node.name}: sys.modules['numpy'] est absent APRÈS NETTOYAGE FORCÉ (comme attendu).")
+    
+    # La logique de nettoyage spécifique à la branche (use_real_numpy vs mock) suit.
+    # Le nettoyage ci-dessous est donc une DEUXIÈME passe de nettoyage pour la branche use_real_numpy.
+    if use_real_numpy_marker or real_jpype_marker:
+        marker_name = "use_real_numpy" if use_real_numpy_marker else "real_jpype"
+        logger.info(f"Test {request.node.name} marqué {marker_name}: Configuration pour VRAI NumPy.")
+
+        # Nettoyage agressif juste avant d'importer le vrai numpy
+        logger.info(f"Nettoyage agressif de numpy et pandas avant import réel pour {request.node.name}")
+        deep_delete_from_sys_modules("numpy", logger)
+        deep_delete_from_sys_modules("pandas", logger) # Assurons-nous que pandas est aussi nettoyé ici
+
+        # Vérification supplémentaire et suppression forcée si nécessaire
+        if 'numpy' in sys.modules:
+            logger.warning(f"NumPy (ID: {id(sys.modules['numpy'])}, Version: {getattr(sys.modules['numpy'], '__version__', 'N/A')}) encore dans sys.modules APRÈS deep_delete pour {request.node.name}. Suppression forcée.")
+            del sys.modules['numpy']
+            # Nettoyer aussi les sous-modules courants qui pourraient persister si la clé principale est supprimée mais pas les enfants
+            keys_to_delete_numpy_children = [k for k in sys.modules if k.startswith('numpy.')]
+            if keys_to_delete_numpy_children:
+                logger.warning(f"Suppression forcée des sous-modules NumPy enfants: {keys_to_delete_numpy_children}")
+                for k_child in keys_to_delete_numpy_children:
+                    del sys.modules[k_child]
+        
+        if 'pandas' in sys.modules:
+            logger.warning(f"Pandas (ID: {id(sys.modules['pandas'])}) encore dans sys.modules APRÈS deep_delete pour {request.node.name}. Suppression forcée.")
+            del sys.modules['pandas']
+            keys_to_delete_pandas_children = [k for k in sys.modules if k.startswith('pandas.')]
+            if keys_to_delete_pandas_children:
+                logger.warning(f"Suppression forcée des sous-modules Pandas enfants: {keys_to_delete_pandas_children}")
+                for k_child in keys_to_delete_pandas_children:
+                    del sys.modules[k_child]
+        
+        imported_numpy_for_test = None
+        original_sys_path = list(sys.path)
+        mocks_path_for_numpy_setup = os.path.abspath(os.path.dirname(__file__)) # Chemin vers tests/mocks
+
+        try:
+            logger.info(f"Tentative d'importation du vrai NumPy pour {request.node.name}.")
+            
+            # Logique de manipulation de sys.path
+            _original_sys_path_for_numpy_import = list(sys.path) # Sauvegarde pour une restauration précise si nécessaire, mais on va surtout gérer le mocks_path
+            _mocks_path_abspath_for_numpy_import = os.path.abspath(os.path.dirname(__file__))
+            _path_removed_for_numpy_import = False
+            if _mocks_path_abspath_for_numpy_import in sys.path:
+                try:
+                    sys.path.remove(_mocks_path_abspath_for_numpy_import)
+                    logger.info(f"NumpySetupFixture: Temporairement retiré '{_mocks_path_abspath_for_numpy_import}' de sys.path avant importlib.import_module('numpy').")
+                    _path_removed_for_numpy_import = True
+                except ValueError: # Devrait peu arriver si 'in sys.path' est vrai
+                    logger.warning(f"NumpySetupFixture: Échec de la suppression de '{_mocks_path_abspath_for_numpy_import}' de sys.path (non trouvé lors du remove).")
+            
+            imported_numpy_for_test = importlib.import_module('numpy')
+            
+            # Restaurer sys.path si on l'a modifié
+            if _path_removed_for_numpy_import:
+                # Il est crucial de restaurer sys.path à son état *exact* d'avant si possible,
+                # mais l'insertion en position 0 est une stratégie commune pour les mocks.
+                # Ici, on s'assure juste que le chemin des mocks est remis s'il a été enlevé.
+                # Si d'autres éléments du sys.path ont été modifiés par ailleurs, cette restauration simple
+                # pourrait ne pas être suffisante pour tous les cas, mais elle adresse le retrait spécifique.
+                if _mocks_path_abspath_for_numpy_import not in sys.path: # S'il n'a pas été remis par une autre logique
+                    sys.path.insert(0, _mocks_path_abspath_for_numpy_import)
+                    logger.info(f"NumpySetupFixture: Restauré '{_mocks_path_abspath_for_numpy_import}' dans sys.path (en position 0).")
+                else:
+                    logger.info(f"NumpySetupFixture: '{_mocks_path_abspath_for_numpy_import}' est déjà de retour dans sys.path après import de numpy.")
+            # Fin de la logique de manipulation de sys.path
+            sys.modules['numpy'] = imported_numpy_for_test # Mettre le vrai numpy dans sys.modules
+            
+            # Restaurer sys.path immédiatement après l'import de numpy et avant l'import de pandas/scipy
+            # pour que les autres imports de mocks (si nécessaires par d'autres fixtures) puissent fonctionner.
+            sys.path = original_sys_path
+            if mocks_path_for_numpy_setup not in sys.path: # Double vérification, devrait être là
+                 logger.warning(f"Restauré sys.path, mais {mocks_path_for_numpy_setup} n'y est pas comme attendu.")
+            else:
+                 logger.info(f"Restauré sys.path original après l'import du vrai NumPy. {mocks_path_for_numpy_setup} est de retour.")
+
+            logger.info(f"Vrai NumPy (version {getattr(imported_numpy_for_test, '__version__', 'inconnue')}, ID: {id(imported_numpy_for_test)}) dynamiquement importé et placé dans sys.modules pour {request.node.name}.")
+            
+            try:
+                # Tentative d'import explicite pour s'assurer que numpy.rec est bien un module chargé
+                import numpy.rec as actual_rec_module
+                sys.modules['numpy.rec'] = actual_rec_module
+                logger.info(f"Vrai numpy.rec importé explicitement et assigné à sys.modules['numpy.rec'] pour {request.node.name}. Type: {type(actual_rec_module)}")
+            except ImportError as e_rec:
+                logger.error(f"Échec de l'import explicite de numpy.rec pour {request.node.name}: {e_rec}")
+                # Logique de fallback si l'import direct échoue (moins probable si numpy lui-même est ok)
+                if hasattr(imported_numpy_for_test, 'rec'):
+                    if not ('numpy.rec' in sys.modules and sys.modules['numpy.rec'] is imported_numpy_for_test.rec):
+                        sys.modules['numpy.rec'] = imported_numpy_for_test.rec
+                        logger.info(f"Vrai numpy.rec (attribut de numpy importé) assigné en fallback pour {request.node.name}.")
+                else:
+                    logger.warning(f"L'attribut 'rec' n'existe pas sur le module numpy importé pour {request.node.name} et l'import explicite a échoué.")
+
+            logger.info(f"Forcing re-import of pandas for {request.node.name} after loading real NumPy.")
+            logger.info(f"Nettoyage agressif de pandas et ses sous-modules _libs avant réimportation pour {request.node.name}")
+            deep_delete_from_sys_modules("pandas._libs", logger) # Cibler _libs spécifiquement
+            deep_delete_from_sys_modules("pandas", logger)       # Ensuite, le reste de pandas
+            try:
+                import pandas # Réimporter pandas
+                logger.info(f"Pandas re-imported successfully for {request.node.name} using real NumPy (version {getattr(sys.modules.get('numpy'), '__version__', 'N/A')}, ID: {id(sys.modules.get('numpy'))}). Pandas ID: {id(pandas)}")
+            except ImportError as e_pandas_reimport:
+                logger.error(f"Failed to re-import pandas for {request.node.name} after loading real NumPy: {e_pandas_reimport}")
+                # Optionnel: skipper le test si pandas ne peut être réimporté
+                # pytest.skip(f"Pandas re-import failed: {e_pandas_reimport}")
+
+            logger.info(f"Forcing re-import of scipy for {request.node.name} after loading real NumPy.")
+            deep_delete_from_sys_modules("scipy", logger)
+            try:
+                import scipy # Réimporter scipy
+                logger.info(f"Scipy re-imported successfully for {request.node.name} using real NumPy. Scipy ID: {id(scipy)}")
+            except ImportError as e_scipy_reimport:
+                logger.error(f"Failed to re-import scipy for {request.node.name} after loading real NumPy: {e_scipy_reimport}")
+
+            yield imported_numpy_for_test
+
+        except ImportError:
+            logger.error(f"Impossible d'importer dynamiquement le vrai NumPy pour {request.node.name} après nettoyage.")
+            pytest.skip("Vrai NumPy non disponible après tentative d'import dynamique.")
+            yield None 
+        finally:
+            logger.info(f"Fin de la section '{marker_name}' pour {request.node.name}. Restauration de l'état PRÉ-FIXTURE (avant nettoyage par CETTE fixture).")
+            if imported_numpy_for_test and 'numpy' in sys.modules and sys.modules['numpy'] is imported_numpy_for_test:
+                logger.info(f"Suppression du NumPy (ID: {id(imported_numpy_for_test)}) spécifiquement importé pour {request.node.name} ({marker_name}).")
+                del sys.modules['numpy']
+                if hasattr(imported_numpy_for_test, 'rec') and 'numpy.rec' in sys.modules and sys.modules['numpy.rec'] is imported_numpy_for_test.rec:
+                    del sys.modules['numpy.rec']
+            
+            if numpy_state_before_this_fixture:
+                sys.modules['numpy'] = numpy_state_before_this_fixture
+                logger.info(f"Restauré sys.modules['numpy'] à l'état pré-fixture (ID: {id(numpy_state_before_this_fixture)}) pour {request.node.name} ({marker_name}).")
+            elif 'numpy' in sys.modules: 
+                logger.warning(f"Après suppression du NumPy de test ({marker_name}), 'numpy' (ID: {id(sys.modules['numpy'])}) est toujours dans sys.modules alors qu'il n'y avait rien à l'origine (avant cette fixture). Suppression.")
+                del sys.modules['numpy']
+
+            if numpy_rec_state_before_this_fixture:
+                 sys.modules['numpy.rec'] = numpy_rec_state_before_this_fixture
+                 logger.info(f"Restauré sys.modules['numpy.rec'] à l'état pré-fixture pour {request.node.name} ({marker_name}).")
+            elif 'numpy.rec' in sys.modules:
+                 if not ('numpy' in sys.modules and hasattr(sys.modules['numpy'], 'rec') and sys.modules['numpy'].rec is sys.modules['numpy.rec']):
+                    logger.warning(f"Après suppression du NumPy de test ({marker_name}), 'numpy.rec' est toujours dans sys.modules et n'appartient pas au numpy restauré/absent. Suppression.")
+                    del sys.modules['numpy.rec']
+            logger.info(f"Fin de la restauration pour {request.node.name} (branche {marker_name}).")
+        return
+
+    else: 
+        logger.info(f"Test {request.node.name} SANS marqueur: Configuration pour MOCK NumPy.")
+        _install_numpy_mock_immediately() 
+        yield 
+        logger.info(f"Fin de la section SANS marqueur pour {request.node.name}. Restauration de l'état PRÉ-FIXTURE.")
+        current_numpy_in_sys = sys.modules.get('numpy')
+        is_our_mock = False
+        if current_numpy_in_sys:
+            if type(current_numpy_in_sys).__name__ == 'numpy' and hasattr(current_numpy_in_sys, '__path__') and not current_numpy_in_sys.__path__:
+                is_our_mock = True
+            elif hasattr(current_numpy_in_sys, '__version__') and "mock" in current_numpy_in_sys.__version__: 
+                is_our_mock = True
+
+        if is_our_mock:
+            logger.info(f"Suppression du Mock NumPy (ID: {id(current_numpy_in_sys)}) installé par {request.node.name}.")
+            del sys.modules['numpy']
+            if 'numpy.rec' in sys.modules and hasattr(current_numpy_in_sys, 'rec') and sys.modules['numpy.rec'] is getattr(current_numpy_in_sys, 'rec', None):
+                 del sys.modules['numpy.rec']
+            # Nettoyer aussi les sous-modules de core qui auraient pu être mis directement dans sys.modules
+            if 'numpy.core._multiarray_umath' in sys.modules:
+                del sys.modules['numpy.core._multiarray_umath']
+            if 'numpy._core._multiarray_umath' in sys.modules:
+                del sys.modules['numpy._core._multiarray_umath']
+            if 'numpy.core.multiarray' in sys.modules: # Nettoyage supplémentaire
+                del sys.modules['numpy.core.multiarray']
+            if 'numpy._core.multiarray' in sys.modules: # Nettoyage supplémentaire
+                del sys.modules['numpy._core.multiarray']
+            if 'numpy.core' in sys.modules:
+                del sys.modules['numpy.core']
+                logger.info(f"Supprimé sys.modules['numpy.core'] pour {request.node.name} (mock cleanup).")
+            if 'numpy._core' in sys.modules:
+                del sys.modules['numpy._core']
+                logger.info(f"Supprimé sys.modules['numpy._core'] pour {request.node.name} (mock cleanup).")
+
+        elif current_numpy_in_sys:
+             logger.warning(f"Tentative de restauration pour {request.node.name} (mock), mais sys.modules['numpy'] (ID: {id(current_numpy_in_sys)}) n'est pas le mock attendu.")
+
+        if numpy_state_before_this_fixture:
+            sys.modules['numpy'] = numpy_state_before_this_fixture
+            logger.info(f"Restauré sys.modules['numpy'] à l'état pré-fixture (ID: {id(numpy_state_before_this_fixture)}) pour {request.node.name} (mock).")
+        elif 'numpy' in sys.modules: 
+            logger.warning(f"Après suppression du Mock NumPy, 'numpy' (ID: {id(sys.modules['numpy'])}) est toujours dans sys.modules alors qu'il n'y avait rien à l'origine (avant cette fixture). Suppression.")
+            del sys.modules['numpy']
+
+        if numpy_rec_state_before_this_fixture:
+            sys.modules['numpy.rec'] = numpy_rec_state_before_this_fixture
+            logger.info(f"Restauré sys.modules['numpy.rec'] à l'état pré-fixture pour {request.node.name} (mock).")
+        elif 'numpy.rec' in sys.modules:
+            if not ('numpy' in sys.modules and hasattr(sys.modules['numpy'], 'rec') and sys.modules['numpy'].rec is sys.modules['numpy.rec']):
+                logger.warning(f"Après suppression du Mock NumPy, 'numpy.rec' est toujours dans sys.modules et n'appartient pas au numpy restauré/absent. Suppression.")
+                del sys.modules['numpy.rec']
+        logger.info(f"Fin de la restauration pour {request.node.name} (branche mock).")
+
+# if (sys.version_info.major == 3 and sys.version_info.minor >= 10):
+# _install_numpy_mock_immediately()
\ No newline at end of file
diff --git a/tests/test_numpy_rec_mock.py b/tests/test_numpy_rec_mock.py
deleted file mode 100644
index e69de29b..00000000

==================== COMMIT: 91425e4ddc1c358cfafb944ddfe45954782bfa8d ====================
commit 91425e4ddc1c358cfafb944ddfe45954782bfa8d
Author: jsboigeEpita <jean-sylvain.boige@epita.fr>
Date:   Mon Jun 16 02:02:31 2025 +0200

    feat(test): Intègre les corrections de tests du stash@{0}
    
    Ce commit résout les conflits du stash 'WIP on main: 3d38d46' en fusionnant plusieurs logiques :

diff --git a/argumentation_analysis/core/jvm_setup.py b/argumentation_analysis/core/jvm_setup.py
index 9900c367..77ec0dff 100644
--- a/argumentation_analysis/core/jvm_setup.py
+++ b/argumentation_analysis/core/jvm_setup.py
@@ -6,9 +6,234 @@ import os
 from pathlib import Path
 from typing import Optional, List
 
+import platform
+from typing import Optional, List
+import requests
+from tqdm.auto import tqdm
+import stat
+import shutil
+import zipfile
+
 # Configuration du logger pour ce module
 logger = logging.getLogger("Orchestration.JPype")
 
+# --- Fonctions de téléchargement et de provisioning (issues du stash) ---
+
+class TqdmUpTo(tqdm):
+    """Provides `update_to(block_num, block_size, total_size)`."""
+    def update_to(self, b=1, bsize=1, tsize=None):
+         if tsize is not None: self.total = tsize
+         self.update(b * bsize - self.n)
+
+def _download_file_with_progress(file_url: str, target_path: Path, description: str):
+    """Télécharge un fichier depuis une URL vers un chemin cible avec une barre de progression."""
+    try:
+        if target_path.exists() and target_path.stat().st_size > 0:
+            logger.debug(f"Fichier '{target_path.name}' déjà présent et non vide. Skip.")
+            return True, False
+        logger.info(f"Tentative de téléchargement: {file_url} vers {target_path}")
+        headers = {'User-Agent': 'Mozilla/5.0'}
+        response = requests.get(file_url, stream=True, timeout=15, headers=headers, allow_redirects=True)
+        if response.status_code == 404:
+             logger.error(f"❌ Fichier non trouvé (404) à l'URL: {file_url}")
+             return False, False
+        response.raise_for_status()
+        total_size = int(response.headers.get('content-length', 0))
+        with TqdmUpTo(unit='B', unit_scale=True, unit_divisor=1024, total=total_size, miniters=1, desc=description[:40]) as t:
+            with open(target_path, 'wb') as f:
+                for chunk in response.iter_content(chunk_size=8192):
+                    if chunk:
+                        f.write(chunk)
+                        t.update(len(chunk))
+        if target_path.exists() and target_path.stat().st_size > 0:
+            logger.info(f" -> Téléchargement de '{target_path.name}' réussi.")
+            return True, True
+        else:
+            logger.error(f"❓ Téléchargement de '{target_path.name}' semblait terminé mais fichier vide ou absent.")
+            if target_path.exists(): target_path.unlink(missing_ok=True)
+            return False, False
+    except requests.exceptions.RequestException as e:
+        logger.error(f"❌ Échec connexion/téléchargement pour '{target_path.name}': {e}")
+        if target_path.exists(): target_path.unlink(missing_ok=True)
+        return False, False
+    except Exception as e_other:
+        logger.error(f"❌ Erreur inattendue pour '{target_path.name}': {e_other}", exc_info=True)
+        if target_path.exists(): target_path.unlink(missing_ok=True)
+        return False, False
+def download_tweety_jars(
+    version: str = "1.28",
+    target_dir: str = None,
+    native_subdir: str = "native"
+    ) -> bool:
+    """
+    Vérifie et télécharge les JARs Tweety (Core + Modules) et les binaires natifs nécessaires.
+    """
+    if target_dir is None:
+        target_dir_path = find_libs_dir()
+        if not target_dir_path:
+            logger.critical("Impossible de trouver le répertoire des bibliothèques pour y télécharger les JARs.")
+            return False
+    else:
+        target_dir_path = Path(target_dir)
+
+    logger.info(f"\n--- Vérification/Téléchargement des JARs Tweety v{version} ---")
+    BASE_URL = f"https://tweetyproject.org/builds/{version}/"
+    LIB_DIR = target_dir_path
+    NATIVE_LIBS_DIR = LIB_DIR / native_subdir
+    LIB_DIR.mkdir(exist_ok=True)
+    NATIVE_LIBS_DIR.mkdir(exist_ok=True) 
+
+    CORE_JAR_NAME = f"org.tweetyproject.tweety-full-{version}-with-dependencies.jar"
+    REQUIRED_MODULES = sorted([
+        "arg.adf", "arg.aba", "arg.bipolar", "arg.aspic", "arg.dung", "arg.weighted",
+        "arg.social", "arg.setaf", "arg.rankings", "arg.prob", "arg.extended",
+        "arg.delp", "arg.deductive", "arg.caf",
+        "beliefdynamics", "agents.dialogues", "action",
+        "logics.pl", "logics.fol", "logics.ml", "logics.dl", "logics.cl",
+        "logics.qbf", "logics.pcl", "logics.rcl", "logics.rpcl", "logics.mln", "logics.bpm",
+        "lp.asp",
+        "math", "commons", "agents"
+    ])
+    system = platform.system()
+    native_binaries_repo_path = "https://raw.githubusercontent.com/TweetyProjectTeam/TweetyProject/main/org-tweetyproject-arg-adf/src/main/resources/"
+    native_binaries = {
+        "Windows": ["picosat.dll", "lingeling.dll", "minisat.dll"],
+        "Linux":   ["picosat.so", "lingeling.so", "minisat.so"],
+        "Darwin":  ["picosat.dylib", "lingeling.dylib", "minisat.dylib"]
+    }.get(system, [])
+
+    logger.info(f"Vérification de l'accès à {BASE_URL}...")
+    url_accessible = False
+    try:
+        response = requests.head(BASE_URL, timeout=10)
+        response.raise_for_status()
+        logger.info(f"✔️ URL de base Tweety v{version} accessible.")
+        url_accessible = True
+    except requests.exceptions.RequestException as e:
+        logger.error(f"❌ Impossible d'accéder à l'URL de base {BASE_URL}. Erreur : {e}")
+        logger.warning("   Le téléchargement des JARs/binaires manquants échouera. Seuls les fichiers locaux seront utilisables.")
+
+    logger.info(f"\n--- Vérification/Téléchargement JAR Core ---")
+    core_present, core_new = _download_file_with_progress(BASE_URL + CORE_JAR_NAME, LIB_DIR / CORE_JAR_NAME, CORE_JAR_NAME)
+    status_core = "téléchargé" if core_new else ("déjà présent" if core_present else "MANQUANT")
+    logger.info(f"✔️ JAR Core '{CORE_JAR_NAME}': {status_core}.")
+    if not core_present:
+        logger.critical(f"❌ ERREUR CRITIQUE : Le JAR core est manquant et n'a pas pu être téléchargé.")
+        return False 
+
+    logger.info(f"\n--- Vérification/Téléchargement des {len(REQUIRED_MODULES)} JARs de modules ---")
+    modules_present_count = 0
+    modules_downloaded_count = 0
+    modules_missing = []
+    for module in tqdm(REQUIRED_MODULES, desc="Modules JARs"):
+        module_jar_name = f"org.tweetyproject.{module}-{version}-with-dependencies.jar"
+        present, new_dl = _download_file_with_progress(BASE_URL + module_jar_name, LIB_DIR / module_jar_name, module_jar_name)
+        if present:
+            modules_present_count += 1
+            if new_dl: modules_downloaded_count += 1
+        elif url_accessible: 
+             modules_missing.append(module)
+    logger.info(f"-> Modules: {modules_downloaded_count} téléchargés, {modules_present_count}/{len(REQUIRED_MODULES)} présents.")
+    if modules_missing:
+        logger.warning(f"   Modules potentiellement manquants (non trouvés ou erreur DL): {', '.join(modules_missing)}")
+
+    logger.info(f"\n--- Vérification/Téléchargement des {len(native_binaries)} binaires natifs ({system}) ---")
+    native_present_count = 0
+    native_downloaded_count = 0
+    native_missing = []
+    if not native_binaries:
+         logger.info(f"   (Aucun binaire natif connu pour {system})")
+    else:
+        for name in tqdm(native_binaries, desc="Binaires Natifs"):
+             present, new_dl = _download_file_with_progress(native_binaries_repo_path + name, NATIVE_LIBS_DIR / name, name)
+             if present:
+                 native_present_count += 1
+                 if new_dl: native_downloaded_count += 1
+                 if new_dl and system != "Windows":
+                     try:
+                         target_path = NATIVE_LIBS_DIR / name
+                         current_permissions = target_path.stat().st_mode
+                         target_path.chmod(current_permissions | stat.S_IXUSR | stat.S_IXGRP | stat.S_IXOTH) 
+                         logger.debug(f"      Permissions d'exécution ajoutées à {name}")
+                     except Exception as e_chmod:
+                         logger.warning(f"      Impossible d'ajouter les permissions d'exécution à {name}: {e_chmod}")
+             elif url_accessible: 
+                  native_missing.append(name)
+        logger.info(f"-> Binaires natifs: {native_downloaded_count} téléchargés, {native_present_count}/{len(native_binaries)} présents.")
+        if native_missing:
+            logger.warning(f"   Binaires natifs potentiellement manquants: {', '.join(native_missing)}")
+        if native_present_count > 0:
+             logger.info(f"   Note: S'assurer que le chemin '{NATIVE_LIBS_DIR.resolve()}' est inclus dans java.library.path lors du démarrage JVM.")
+    logger.info("--- Fin Vérification/Téléchargement Tweety ---")
+    return core_present and modules_present_count > 0
+
+
+PORTABLE_JDK_DIR_NAME = "portable_jdk"
+PORTABLE_JDK_ZIP_NAME = "OpenJDK17U-jdk_x64_windows_hotspot_17.0.15_6_new.zip"
+TEMP_DIR_NAME = "_temp"
+
+def _extract_portable_jdk(project_root: Path, portable_jdk_parent_dir: Path, portable_jdk_zip_path: Path) -> Optional[Path]:
+    logger.info(f"Tentative d'extraction du JDK portable depuis '{portable_jdk_zip_path}' vers '{portable_jdk_parent_dir}'...")
+    try:
+        with zipfile.ZipFile(portable_jdk_zip_path, 'r') as zip_ref:
+            zip_ref.extractall(portable_jdk_parent_dir)
+        logger.info(f"JDK portable extrait avec succès dans '{portable_jdk_parent_dir}'.")
+        for item in portable_jdk_parent_dir.iterdir():
+            if item.is_dir() and item.name.startswith("jdk-"):
+                logger.info(f"Dossier racine du JDK portable détecté : '{item}'")
+                return item
+        logger.warning(f"Impossible de déterminer le dossier racine du JDK dans '{portable_jdk_parent_dir}' après extraction.")
+        extracted_items = [d for d in portable_jdk_parent_dir.iterdir() if d.is_dir()]
+        if len(extracted_items) == 1:
+            logger.info(f"Un seul dossier trouvé après extraction: '{extracted_items[0]}', en supposant que c'est le JDK.")
+            return extracted_items[0]
+        return None
+    except Exception as e:
+        logger.error(f"Erreur lors de l'extraction du JDK portable: {e}", exc_info=True)
+        return None
+
+def find_valid_java_home() -> Optional[str]:
+    logger.debug("Début recherche répertoire Java Home valide...")
+    
+    project_root = get_project_root()
+    portable_jdk_parent_dir = project_root / PORTABLE_JDK_DIR_NAME
+    portable_jdk_zip_path = project_root / TEMP_DIR_NAME / PORTABLE_JDK_ZIP_NAME
+    PORTABLE_JDK_DOWNLOAD_URL = "https://github.com/adoptium/temurin17-binaries/releases/download/jdk-17.0.15%2B6/OpenJDK17U-jdk_x64_windows_hotspot_17.0.15_6.zip" # Du Stash
+    
+    potential_jdk_root_dir = None
+    if portable_jdk_parent_dir.is_dir():
+        for item in portable_jdk_parent_dir.iterdir():
+            if item.is_dir() and item.name.startswith("jdk-"):
+                java_exe_portable = item / "bin" / f"java{'.exe' if os.name == 'nt' else ''}"
+                if java_exe_portable.is_file():
+                    logger.info(f"JDK portable trouvé et valide dans: '{item}'")
+                    potential_jdk_root_dir = item
+                    break
+    
+    if potential_jdk_root_dir:
+        logger.info(f"🎉 Utilisation du JDK portable intégré: '{potential_jdk_root_dir}'")
+        return str(potential_jdk_root_dir.resolve())
+
+    if portable_jdk_zip_path.is_file():
+        extracted_jdk_root = _extract_portable_jdk(project_root, portable_jdk_parent_dir, portable_jdk_zip_path)
+        if extracted_jdk_root and (extracted_jdk_root / "bin" / f"java{'.exe' if os.name == 'nt' else ''}").is_file():
+            return str(extracted_jdk_root.resolve())
+    else:
+        logger.info(f"Archive ZIP du JDK portable non trouvée. Tentative de téléchargement...")
+        temp_dir = project_root / TEMP_DIR_NAME
+        temp_dir.mkdir(parents=True, exist_ok=True)
+        jdk_downloaded, _ = _download_file_with_progress(PORTABLE_JDK_DOWNLOAD_URL, portable_jdk_zip_path, "JDK Portable")
+        if jdk_downloaded:
+            extracted_jdk_root = _extract_portable_jdk(project_root, portable_jdk_parent_dir, portable_jdk_zip_path)
+            if extracted_jdk_root and (extracted_jdk_root / "bin" / f"java{'.exe' if os.name == 'nt' else ''}").is_file():
+                return str(extracted_jdk_root.resolve())
+
+    # Si le portable échoue, on revient à la logique de HEAD.
+    # Je vais simplement appeler `find_jdk_path` de HEAD comme fallback.
+    logger.info("JDK portable non trouvé/installé. Retour à la détection standard (JAVA_HOME / chemin par défaut).")
+    jdk_path_from_head = find_jdk_path()
+    return str(jdk_path_from_head) if jdk_path_from_head else None
+
 # --- Fonctions pour une initialisation paresseuse (Lazy Initialization) ---
 
 _PROJECT_ROOT_DIR = None
@@ -141,6 +366,14 @@ def initialize_jvm(lib_dir_path: Optional[str] = None, specific_jar_path: Option
     logger.info(f"JVM_SETUP: _JVM_INITIALIZED_THIS_SESSION: {_JVM_INITIALIZED_THIS_SESSION}")
     logger.info(f"JVM_SETUP: _SESSION_FIXTURE_OWNS_JVM: {_SESSION_FIXTURE_OWNS_JVM}")
     
+    # --- Étape de Provisioning (issue du stash) ---
+    logger.info("JVM_SETUP: Lancement de l'étape de vérification/téléchargement des JARs.")
+    libs_ok = download_tweety_jars()
+    if not libs_ok:
+        logger.error("JVM_SETUP: Échec du provisioning des bibliothèques Tweety. Démarrage de la JVM annulé.")
+        return False
+    logger.info("JVM_SETUP: Provisioning des bibliothèques terminé.")
+
     # PROTECTION 1: Vérifier si une tentative de redémarrage est en cours
     if _JVM_WAS_SHUTDOWN and not jpype.isJVMStarted():
         logger.error("JVM_SETUP: ERREUR - Tentative de redémarrage de la JVM détectée. JPype ne supporte qu'un cycle de vie JVM par processus.")
@@ -201,7 +434,8 @@ def initialize_jvm(lib_dir_path: Optional[str] = None, specific_jar_path: Option
             return False
         
         jvm_options = get_jvm_options()
-        jdk_path = find_jdk_path()
+        jdk_path_str = find_valid_java_home()
+        jdk_path = Path(jdk_path_str) if jdk_path_str else None
         jvm_path = None
 
         # Stratégie de recherche de la JVM
diff --git a/tests/mocks/__init__.py b/tests/mocks/__init__.py
index e69de29b..ddfb6e03 100644
--- a/tests/mocks/__init__.py
+++ b/tests/mocks/__init__.py
@@ -0,0 +1,132 @@
+#!/usr/bin/env python
+# -*- coding: utf-8 -*-
+
+"""
+Package de mocks pour les tests.
+"""
+
+import logging
+import importlib.util
+import os
+import sys
+from pathlib import Path
+
+# Configuration du logging
+logging.basicConfig(
+    level=logging.INFO,
+    format='%(asctime)s [%(levelname)s] [%(name)s] %(message)s',
+    datefmt='%H:%M:%S'
+)
+logger = logging.getLogger("Mocks")
+
+# Log de chargement
+logger.debug("Package mocks chargé")
+# Mock pour semantic_kernel
+try:
+    import semantic_kernel
+except ImportError:
+    mock_path = Path(__file__).parent / "semantic_kernel_mock.py"
+    spec = importlib.util.spec_from_file_location("semantic_kernel", mock_path)
+    semantic_kernel = importlib.util.module_from_spec(spec)
+    spec.loader.exec_module(semantic_kernel)
+    sys.modules['semantic_kernel'] = semantic_kernel
+    sys.modules['semantic_kernel.contents'] = semantic_kernel.contents
+    sys.modules['semantic_kernel.functions'] = semantic_kernel.functions
+    sys.modules['semantic_kernel.connectors'] = semantic_kernel.connectors
+    sys.modules['semantic_kernel.connectors.ai'] = semantic_kernel.connectors.ai
+    sys.modules['semantic_kernel.connectors.ai.open_ai'] = semantic_kernel.connectors.ai.open_ai
+
+# Mock pour numpy_mock
+try:
+    from . import numpy_mock
+    sys.modules['numpy_mock'] = numpy_mock
+except ImportError:
+    logger.warning("Impossible d'importer numpy_mock")
+
+# Mock pour pandas_mock
+try:
+    from . import pandas_mock
+    sys.modules['pandas_mock'] = pandas_mock
+except ImportError:
+    logger.warning("Impossible d'importer pandas_mock")
+
+# Mock pour NetworkX
+try:
+    import networkx
+    logger.info("NetworkX déjà installé")
+except ImportError:
+    try:
+        from . import networkx_mock
+        sys.modules['networkx'] = networkx_mock
+        logger.info("Mock NetworkX activé")
+    except ImportError:
+        logger.warning("Impossible d'importer networkx_mock")
+
+# Mock pour JPype
+# Vérifier si la vraie JVM a été initialisée par le conftest racine
+# ou si une variable d'environnement force l'utilisation du vrai JPype.
+# JPYPE_REAL_JVM_INITIALIZED est défini par le conftest racine.
+# USE_REAL_JPYPE_FORCE peut être défini manuellement pour débogage.
+real_jvm_was_initialized = os.environ.get("JPYPE_REAL_JVM_INITIALIZED") == "1"
+force_real_jpype = os.environ.get("USE_REAL_JPYPE_FORCE") == "1"
+
+if real_jvm_was_initialized or force_real_jpype:
+    if real_jvm_was_initialized:
+        logger.info("Mock JPype NON activé car la VRAIE JVM a été initialisée par le conftest racine.")
+    if force_real_jpype:
+        logger.info("Mock JPype NON activé car USE_REAL_JPYPE_FORCE=1.")
+    # S'assurer que le vrai jpype est utilisé si déjà importé, sinon l'importer.
+    if 'jpype' not in sys.modules:
+        try:
+            import jpype
+            logger.info("Vrai module JPype importé car mock désactivé.")
+        except ImportError:
+            logger.error("ERREUR CRITIQUE: Vrai JPype non trouvé et mock désactivé.")
+    else:
+        # Vérifier si le jpype dans sys.modules est le vrai ou le mock
+        current_jpype = sys.modules['jpype']
+        module_path = getattr(current_jpype, '__file__', '')
+        if 'jpype_mock.py' in module_path:
+            logger.warning("ATTENTION: Le mock JPype était déjà dans sys.modules mais on force le vrai JPype. Tentative de restauration (peut être instable).")
+            # Tentative de suppression du mock pour forcer la réimportation du vrai.
+            # Cela est risqué si d'autres modules détiennent des références au mock.
+            del sys.modules['jpype']
+            if 'jpype1' in sys.modules and getattr(sys.modules['jpype1'], '__file__', '') == module_path:
+                del sys.modules['jpype1']
+            try:
+                import jpype # Réimporter le vrai
+                logger.info("Vrai module JPype réimporté après suppression du mock.")
+            except ImportError:
+                 logger.error("ERREUR CRITIQUE: Impossible de réimporter le vrai JPype après suppression du mock.")
+        else:
+            logger.info("Vrai module JPype déjà présent dans sys.modules.")
+
+else:
+    logger.info("Tentative d'activation du mock JPype (la vraie JVM n'a pas été signalée comme initialisée par le conftest racine ET USE_REAL_JPYPE_FORCE n'est pas à 1).")
+    try:
+        # Essayer d'importer le vrai jpype d'abord pour voir s'il est installé
+        import jpype as real_jpype_module
+        jpype_real_path = getattr(real_jpype_module, '__file__', 'N/A')
+        logger.info(f"Vrai JPype trouvé ({jpype_real_path}). Le mock ne sera PAS activé par défaut pour 'jpype' à moins que 'jpype1' ne soit pas trouvé.")
+        # Le comportement original était de mocker 'jpype' seulement si 'jpype1' n'était pas trouvé.
+        # On garde cette logique si on n'est pas dans un cas de forçage du vrai JPype.
+        try:
+            import jpype1 # Tenter d'importer jpype1
+            logger.info("Module 'jpype1' trouvé. Le mock JPype ne sera pas activé pour 'jpype' ou 'jpype1'.")
+        except ImportError:
+            logger.info("Module 'jpype1' non trouvé. Activation du mock JPype pour 'jpype' et 'jpype1'.")
+            from . import jpype_mock
+            sys.modules['jpype1'] = jpype_mock
+            sys.modules['jpype'] = jpype_mock
+            logger.info("Mock JPype activé pour 'jpype' et 'jpype1' car 'jpype1' est manquant.")
+
+    except ImportError:
+        # Le vrai jpype n'est pas installé, donc on peut mocker sans risque.
+        logger.info("Vrai module JPype non trouvé (ImportError). Activation du mock JPype.")
+        try:
+            from . import jpype_mock
+            # sys.modules['jpype1'] = jpype_mock # jpype1 n'est pas le nom standard
+            sys.modules['jpype'] = jpype_mock
+            logger.info("Mock JPype activé pour 'jpype'.")
+        except ImportError:
+            logger.warning("Impossible d'importer jpype_mock.")

==================== COMMIT: eeb909ab33bb169b2d1518e7d9ec0a0a677af5bd ====================
commit eeb909ab33bb169b2d1518e7d9ec0a0a677af5bd
Author: jsboigeEpita <jean-sylvain.boige@epita.fr>
Date:   Mon Jun 16 01:55:45 2025 +0200

    fix(agent): Résout le bug de double déclaration du parser

diff --git a/argumentation_analysis/agents/core/logic/belief_set.py b/argumentation_analysis/agents/core/logic/belief_set.py
index 144131c2..42178650 100644
--- a/argumentation_analysis/agents/core/logic/belief_set.py
+++ b/argumentation_analysis/agents/core/logic/belief_set.py
@@ -77,7 +77,8 @@ class BeliefSet(ABC):
         content = data.get("content", "")
         
         if logic_type == "propositional":
-            return PropositionalBeliefSet(content)
+            propositions = data.get("propositions")
+            return PropositionalBeliefSet(content, propositions=propositions)
         elif logic_type == "first_order":
             return FirstOrderBeliefSet(content)
         elif logic_type == "modal":
@@ -91,6 +92,10 @@ class PropositionalBeliefSet(BeliefSet):
     Classe pour représenter un ensemble de croyances en logique propositionnelle.
     """
     
+    def __init__(self, content: str, propositions: Optional[list[str]] = None):
+        super().__init__(content)
+        self.propositions = propositions if propositions is not None else []
+
     @property
     def logic_type(self) -> str:
         """
diff --git a/argumentation_analysis/agents/core/logic/propositional_logic_agent.py b/argumentation_analysis/agents/core/logic/propositional_logic_agent.py
index 83f335dc..61bdaf1c 100644
--- a/argumentation_analysis/agents/core/logic/propositional_logic_agent.py
+++ b/argumentation_analysis/agents/core/logic/propositional_logic_agent.py
@@ -11,139 +11,243 @@ texte-vers-PL, la génération de requêtes et l'interprétation des résultats.
 """
 
 import logging
-from typing import Dict, List, Optional, Any, Tuple
+import re
+import json
+from typing import Dict, List, Optional, Any, Tuple, AsyncGenerator
 
 from semantic_kernel import Kernel
 from semantic_kernel.functions.kernel_arguments import KernelArguments
 from semantic_kernel.connectors.ai.chat_completion_client_base import ChatCompletionClientBase
-from semantic_kernel.contents import ChatMessageContent, AuthorRole
+from semantic_kernel.contents import ChatMessageContent
 from semantic_kernel.contents.chat_history import ChatHistory
 from pydantic import Field
-from typing import AsyncGenerator
 
 from ..abc.agent_bases import BaseLogicAgent
 from .belief_set import BeliefSet, PropositionalBeliefSet
 from .tweety_bridge import TweetyBridge
 
-# Importer les prompts depuis le module pl existant
-from ..pl.prompts import (
-    prompt_text_to_pl_v8 as PROMPT_TEXT_TO_PL,
-    prompt_gen_pl_queries_v8 as PROMPT_GEN_PL_QUERIES,
-    prompt_interpret_pl_v8 as PROMPT_INTERPRET_PL
-)
-# Importer les instructions système
-from ..pl.pl_definitions import PL_AGENT_INSTRUCTIONS 
-
 # Configuration du logger
-logger = logging.getLogger(__name__) 
+logger = logging.getLogger(__name__)
 
-class PropositionalLogicAgent(BaseLogicAgent): 
-    """
-    Agent spécialisé pour la logique propositionnelle (PL).
+# --- Prompts pour la Logique Propositionnelle (PL) ---
+
+SYSTEM_PROMPT_PL = """Vous êtes un agent spécialisé dans l'analyse et le raisonnement en logique propositionnelle (PL).
+Vous utilisez la syntaxe de TweetyProject pour représenter les formules PL.
+Vos tâches principales incluent la traduction de texte en formules PL, la génération de requêtes PL pertinentes,
+l'exécution de ces requêtes sur un ensemble de croyances PL, et l'interprétation des résultats obtenus.
+"""
+
+PROMPT_TEXT_TO_PL_DEFS = """
+Vous êtes un expert en logique propositionnelle (PL). Votre tâche est d'identifier les propositions atomiques (faits de base) dans un texte donné.
+
+**Format de Sortie (JSON Strict):**
+Votre sortie DOIT être un objet JSON unique contenant une seule clé : `propositions`.
+La valeur de `propositions` doit être une liste de chaînes de caractères, où chaque chaîne est une proposition atomique.
+Les noms des propositions doivent être concis, en minuscules et en `snake_case` (ex: "is_mortal", "is_man").
+
+**Exemple:**
+Texte: "Socrate est un homme. Si un être est un homme, alors il est mortel."
+
+**Sortie JSON attendue:**
+```json
+{
+  "propositions": [
+    "socrates_is_a_man",
+    "socrates_is_mortal"
+  ]
+}
+```
+
+Analysez le texte suivant et extrayez uniquement les `propositions`.
+
+{{$input}}
+"""
+
+PROMPT_TEXT_TO_PL_FORMULAS = """
+Vous êtes un expert en logique propositionnelle (PL). Votre tâche est de traduire un texte en formules logiques, en utilisant un ensemble prédéfini de propositions atomiques.
+
+**Contexte Fourni:**
+1.  **Texte Original**: Le texte à traduire.
+2.  **Propositions Autorisées**: Une liste JSON des propositions atomiques que vous DEVEZ utiliser.
+
+**Votre Tâche:**
+Générez un objet JSON contenant UNIQUEMENT la clé `formulas`.
+
+**Règles Strictes:**
+*   **Utilisation Exclusive**: N'utilisez QUE les `propositions` fournies. N'en inventez pas de nouvelles.
+*   **Connecteurs**: Utilisez `!`, `&&`, `||`, `=>`, `<=>`.
+*   **Format**: Les formules sont une liste de chaînes de caractères. N'ajoutez PAS de point-virgule à la fin.
+
+**Exemple:**
+Texte Original: "Socrate est un homme. Si un être est un homme, alors il est mortel."
+Propositions Autorisées:
+```json
+{
+  "propositions": [
+    "socrates_is_a_man",
+    "socrates_is_mortal"
+  ]
+}
+```
+
+**Sortie JSON attendue:**
+```json
+{
+  "formulas": [
+    "socrates_is_a_man",
+    "socrates_is_a_man => socrates_is_mortal"
+  ]
+}
+```
+
+Maintenant, traduisez le texte suivant en utilisant les propositions fournies.
+
+**Texte Original:**
+{{$input}}
+
+**Propositions Autorisées:**
+{{$definitions}}
+"""
 
-    Cet agent étend `BaseLogicAgent` pour fournir des capacités de traitement
-    spécifiques à la logique propositionnelle. Il intègre des fonctions sémantiques
-    pour traduire le langage naturel en ensembles de croyances PL, générer des
-    requêtes PL pertinentes, exécuter ces requêtes via `TweetyBridge`, et
-    interpréter les résultats en langage naturel.
+PROMPT_GEN_PL_QUERIES_IDEAS = """
+Vous êtes un expert en logique propositionnelle. Votre tâche est de générer des "idées" de requêtes pertinentes pour interroger un ensemble de croyances (belief set) donné.
+
+**Contexte Fourni:**
+1.  **Texte Original**: Le texte qui motive l'analyse.
+2.  **Ensemble de Croyances (Knowledge Base)**: Une liste de propositions atomiques valides.
+
+**Votre Tâche:**
+Générez un objet JSON contenant UNIQUEMENT la clé `query_ideas`.
+La valeur de `query_ideas` doit être une liste de chaînes de caractères, où chaque chaîne est une proposition que vous jugez pertinent de vérifier.
+
+**Règles Strictes:**
+*   **Utilisation Exclusive**: N'utilisez QUE les propositions qui existent dans l'ensemble de croyances fourni. N'en inventez pas.
+*   **Pertinence**: Les idées de requêtes doivent être pertinentes par rapport au texte original et chercher à vérifier des conclusions ou des faits intéressants.
+*   **Format de Sortie**: Votre sortie DOIT être un objet JSON valide, sans aucun texte ou explication supplémentaire.
+
+**Exemple:**
+Texte Original: "Socrate est un homme. Si un être est un homme, alors il est mortel."
+Ensemble de Croyances:
+```json
+{
+  "propositions": [
+    "socrates_is_a_man",
+    "socrates_is_mortal"
+  ],
+  "formulas": [
+    "socrates_is_a_man",
+    "socrates_is_a_man => socrates_is_mortal"
+  ]
+}
+```
+
+**Sortie JSON attendue:**
+```json
+{
+  "query_ideas": [
+    "socrates_is_mortal",
+    "socrates_is_a_man"
+  ]
+}
+```
+
+Maintenant, analysez le contexte suivant et générez les idées de requêtes.
+
+**Texte Original:**
+{{$input}}
+
+**Ensemble de Croyances:**
+{{$belief_set}}
+"""
+
+PROMPT_INTERPRET_PL = """
+Vous êtes un expert en logique propositionnelle. Votre tâche est d'interpréter les résultats de requêtes et d'expliquer leur signification dans le contexte du texte source.
+
+Voici le texte source:
+{{$input}}
+
+Voici l'ensemble de croyances en logique propositionnelle:
+{{$belief_set}}
+
+Voici les requêtes qui ont été exécutées:
+{{$queries}}
+
+Voici les résultats de ces requêtes:
+{{$tweety_result}}
+
+Interprétez ces résultats et expliquez leur signification. Pour chaque requête:
+1. Expliquez ce que la requête cherchait à vérifier.
+2. Indiquez si la requête a été prouvée (True) ou non (False).
+3. Expliquez ce que cela signifie dans le contexte du texte source.
 
-    Attributes:
-        _tweety_bridge (TweetyBridge): Instance de `TweetyBridge` configurée pour la PL.
+Fournissez ensuite une conclusion générale. Votre réponse doit être claire et accessible.
+"""
+
+
+class PropositionalLogicAgent(BaseLogicAgent):
+    """
+    Agent spécialisé pour la logique propositionnelle (PL).
+    Refactorisé pour une robustesse et une transparence accrues, inspiré par FirstOrderLogicAgent.
     """
-    
     service: Optional[ChatCompletionClientBase] = Field(default=None, exclude=True)
     settings: Optional[Any] = Field(default=None, exclude=True)
 
     def __init__(self, kernel: Kernel, agent_name: str = "PropositionalLogicAgent", system_prompt: Optional[str] = None, service_id: Optional[str] = None):
-        """
-        Initialise une instance de `PropositionalLogicAgent`.
-
-        :param kernel: Le kernel Semantic Kernel à utiliser pour les fonctions sémantiques.
-        :type kernel: Kernel
-        :param agent_name: Le nom de l'agent, par défaut "PropositionalLogicAgent".
-        :type agent_name: str
-        :param system_prompt: Le prompt système optionnel. S'il n'est pas fourni,
-                              PL_AGENT_INSTRUCTIONS sera utilisé.
-        :type system_prompt: Optional[str]
-        """
-        actual_system_prompt = system_prompt if system_prompt is not None else PL_AGENT_INSTRUCTIONS
-        logger.info(f"DEBUG: Initializing PropositionalLogicAgent with name: {agent_name}")
+        actual_system_prompt = system_prompt if system_prompt is not None else SYSTEM_PROMPT_PL
         super().__init__(kernel,
                          agent_name=agent_name,
                          logic_type_name="PL",
                          system_prompt=actual_system_prompt)
         self._llm_service_id = service_id
-        self._tweety_bridge = None  # Initialiser à None
+        self._tweety_bridge = TweetyBridge()
+        self.logger.info(f"TweetyBridge initialisé pour {self.name}. JVM prête: {self._tweety_bridge.is_jvm_ready()}")
+        if not self._tweety_bridge.is_jvm_ready():
+            self.logger.error("La JVM n'est pas prête. Les fonctionnalités de TweetyBridge pourraient ne pas fonctionner.")
 
     def get_agent_capabilities(self) -> Dict[str, Any]:
-        """
-        Retourne un dictionnaire décrivant les capacités spécifiques de cet agent PL.
-
-        :return: Un dictionnaire mappant les noms des capacités à leurs descriptions.
-        :rtype: Dict[str, Any]
-        """
         return {
-            "text_to_belief_set": "Translates natural language text to a Propositional Logic belief set.",
-            "generate_queries": "Generates relevant PL queries based on text and a belief set.",
-            "execute_query": "Executes a PL query against a belief set using Tweety.",
-            "interpret_results": "Interprets the results of PL queries in natural language.",
-            "validate_formula": "Validates the syntax of a PL formula."
+            "name": self.name,
+            "logic_type": self.logic_type,
+            "description": "Agent capable d'analyser du texte en utilisant la logique propositionnelle (PL).",
+            "methods": {
+                "text_to_belief_set": "Convertit un texte en un ensemble de croyances PL.",
+                "generate_queries": "Génère des requêtes PL pertinentes à partir d'un texte et d'un ensemble de croyances.",
+                "execute_query": "Exécute une requête PL sur un ensemble de croyances.",
+                "interpret_results": "Interprète les résultats d'une ou plusieurs requêtes PL.",
+                "validate_formula": "Valide la syntaxe d'une formule PL."
+            }
         }
 
-    def setup_agent_components(self, llm_service_id: str) -> None: 
-        """
-        Configure les composants spécifiques de l'agent de logique propositionnelle.
-
-        Initialise `TweetyBridge` pour la logique propositionnelle et enregistre
-        les fonctions sémantiques nécessaires (TextToPLBeliefSet, GeneratePLQueries,
-        InterpretPLResults) dans le kernel Semantic Kernel.
-
-        :param llm_service_id: L'ID du service LLM à utiliser pour les fonctions sémantiques.
-        :type llm_service_id: str
-        """
+    def setup_agent_components(self, llm_service_id: str) -> None:
         super().setup_agent_components(llm_service_id)
         self.logger.info(f"Configuration des composants sémantiques pour {self.name}...")
-        
-        # Initialiser TweetyBridge ici
-        if not self._tweety_bridge:
-            self._tweety_bridge = TweetyBridge()
-            self.logger.info(f"TweetyBridge initialisé dans setup_agent_components pour {self.name}. JVM prête: {self._tweety_bridge.is_jvm_ready()}")
-            if not self._tweety_bridge.is_jvm_ready():
-                self.logger.error("La JVM n'est pas prête. Les fonctionnalités de TweetyBridge pourraient ne pas fonctionner.")
+
+        if not self._tweety_bridge.is_jvm_ready():
+            self.logger.error(f"La JVM pour TweetyBridge de {self.name} n'est pas prête.")
+            return
 
         prompt_execution_settings = None
         if self._llm_service_id:
             try:
-                prompt_execution_settings = self.kernel.get_prompt_execution_settings_from_service_id(
-                    self._llm_service_id
-                )
+                prompt_execution_settings = self.sk_kernel.get_prompt_execution_settings_from_service_id(self._llm_service_id)
                 self.logger.debug(f"Settings LLM récupérés pour {self.name}.")
             except Exception as e:
                 self.logger.warning(f"Impossible de récupérer les settings LLM pour {self.name}: {e}")
-        
-        semantic_functions_map = {
-            "TextToPLBeliefSet": PROMPT_TEXT_TO_PL,
-            "GeneratePLQueries": PROMPT_GEN_PL_QUERIES,
-            "InterpretPLResults": PROMPT_INTERPRET_PL
-        }
-        
-        descriptions_map = {
-            "TextToPLBeliefSet": "Translates natural language text to a Propositional Logic belief set (Tweety syntax).",
-            "GeneratePLQueries": "Generates relevant Propositional Logic queries based on text and a belief set (Tweety syntax).",
-            "InterpretPLResults": "Interprets the results of Propositional Logic queries in natural language."
-        }
 
-        for func_name, prompt_template in semantic_functions_map.items():
+        semantic_functions = [
+            ("TextToPLDefs", PROMPT_TEXT_TO_PL_DEFS, "Extrait les propositions atomiques d'un texte."),
+            ("TextToPLFormulas", PROMPT_TEXT_TO_PL_FORMULAS, "Génère les formules PL à partir d'un texte et de propositions."),
+            ("GeneratePLQueryIdeas", PROMPT_GEN_PL_QUERIES_IDEAS, "Génère des idées de requêtes PL au format JSON."),
+            ("InterpretPLResults", PROMPT_INTERPRET_PL, "Interprète les résultats de requêtes PL.")
+        ]
+
+        for func_name, prompt, description in semantic_functions:
             try:
-                if not prompt_template or not isinstance(prompt_template, str):
-                    self.logger.error(f"Prompt invalide pour {self.name}.{func_name}. Skipping.")
-                    continue
-                
-                self.kernel.add_function(
-                    prompt=prompt_template,
+                self.sk_kernel.add_function(
+                    prompt=prompt,
                     plugin_name=self.name,
                     function_name=func_name,
-                    description=descriptions_map.get(func_name, f"{func_name} function"),
+                    description=description,
                     prompt_execution_settings=prompt_execution_settings
                 )
                 self.logger.info(f"Fonction sémantique {self.name}.{func_name} ajoutée.")
@@ -152,114 +256,173 @@ class PropositionalLogicAgent(BaseLogicAgent):
         
         self.logger.info(f"Composants pour {self.name} configurés.")
 
-    async def text_to_belief_set(self, text: str, context: Optional[Dict[str, Any]] = None) -> Tuple[Optional[BeliefSet], str]: 
-        """
-        Convertit un texte en langage naturel en un ensemble de croyances propositionnelles.
-
-        Utilise la fonction sémantique "TextToPLBeliefSet" pour la conversion,
-        puis valide l'ensemble de croyances généré avec `TweetyBridge`.
-
-        :param text: Le texte en langage naturel à convertir.
-        :type text: str
-        :param context: Un dictionnaire optionnel de contexte (non utilisé actuellement).
-        :type context: Optional[Dict[str, Any]]
-        :return: Un tuple contenant l'objet `PropositionalBeliefSet` si la conversion
-                 et la validation réussissent, et un message de statut.
-                 Retourne (None, message_erreur) en cas d'échec.
-        :rtype: Tuple[Optional[BeliefSet], str]
-        """
-        self.logger.info(f"Conversion de texte en ensemble de croyances propositionnelles pour le texte : '{text[:100]}...'")
+    def _extract_json_block(self, text: str) -> str:
+        """Extrait le premier bloc JSON valide de la réponse du LLM."""
+        match = re.search(r'```json\s*(\{.*?\})\s*```', text, re.DOTALL)
+        if match:
+            return match.group(1)
         
-        try:
-            arguments = KernelArguments(input=text)
-            result = await self.kernel.invoke(
-                plugin_name=self.name,
-                function_name="TextToPLBeliefSet",
-                arguments=arguments
-            )
-            belief_set_content = str(result) 
-            
-            if not belief_set_content or len(belief_set_content.strip()) == 0:
-                self.logger.error("La conversion a produit un ensemble de croyances vide.") 
-                return None, "La conversion a produit un ensemble de croyances vide."
-            
-            is_valid, validation_msg = self._tweety_bridge.validate_belief_set(belief_set_string=belief_set_content)
-            if not is_valid:
-                self.logger.error(f"Ensemble de croyances invalide: {validation_msg}")
-                return None, f"Ensemble de croyances invalide: {validation_msg}"
+        start_index = text.find('{')
+        end_index = text.rfind('}')
+        if start_index != -1 and end_index != -1 and end_index > start_index:
+            return text[start_index:end_index + 1]
+        
+        self.logger.warning("Impossible d'isoler un bloc JSON. Tentative de parsing de la chaîne complète.")
+        return text
+
+    async def text_to_belief_set(self, text: str, context: Optional[Dict[str, Any]] = None) -> Tuple[Optional[BeliefSet], str]:
+        self.logger.info(f"Conversion de texte en ensemble de croyances PL pour '{text[:100]}...'")
+        max_retries = 3
+        
+        # --- Étape 1: Génération des Propositions ---
+        self.logger.info("Étape 1: Génération des propositions atomiques...")
+        defs_json = None
+        for attempt in range(max_retries):
+            try:
+                defs_result = await self.sk_kernel.plugins[self.name]["TextToPLDefs"].invoke(self.sk_kernel, input=text)
+                defs_json_str = self._extract_json_block(str(defs_result))
+                defs_json = json.loads(defs_json_str)
+                if "propositions" in defs_json and isinstance(defs_json["propositions"], list):
+                    self.logger.info(f"Propositions générées avec succès à la tentative {attempt + 1}.")
+                    break
+                else:
+                    raise ValueError("Le JSON ne contient pas la clé 'propositions' ou ce n'est pas une liste.")
+            except (json.JSONDecodeError, ValueError, Exception) as e:
+                self.logger.warning(f"Tentative {attempt + 1}/{max_retries} échouée pour générer les propositions: {e}")
+                if attempt + 1 == max_retries:
+                    error_msg = f"Échec final de la génération des propositions: {e}"
+                    self.logger.error(error_msg)
+                    return None, error_msg
+        
+        if defs_json is None:
+            return None, "Impossible de générer les propositions après plusieurs tentatives."
+
+        # --- Étape 2: Génération des Formules ---
+        self.logger.info("Étape 2: Génération des formules...")
+        formulas_json = None
+        for attempt in range(max_retries):
+            try:
+                definitions_for_prompt = json.dumps(defs_json, indent=2)
+                formulas_result = await self.sk_kernel.plugins[self.name]["TextToPLFormulas"].invoke(
+                    self.sk_kernel, input=text, definitions=definitions_for_prompt
+                )
+                formulas_json_str = self._extract_json_block(str(formulas_result))
+                formulas_json = json.loads(formulas_json_str)
+                if "formulas" in formulas_json and isinstance(formulas_json["formulas"], list):
+                    self.logger.info(f"Formules générées avec succès à la tentative {attempt + 1}.")
+                    break
+                else:
+                    raise ValueError("Le JSON ne contient pas la clé 'formulas' ou ce n'est pas une liste.")
+            except (json.JSONDecodeError, ValueError, Exception) as e:
+                self.logger.warning(f"Tentative {attempt + 1}/{max_retries} échouée pour générer les formules: {e}")
+                if attempt + 1 == max_retries:
+                    error_msg = f"Échec final de la génération des formules: {e}"
+                    self.logger.error(error_msg)
+                    return None, error_msg
+
+        if formulas_json is None:
+            return None, "Impossible de générer les formules après plusieurs tentatives."
+
+        # --- Étape 3: Filtrage programmatique des formules ---
+        self.logger.info("Étape 3: Filtrage des formules...")
+        declared_propositions = set(defs_json.get("propositions", []))
+        valid_formulas = []
+        all_formulas = formulas_json.get("formulas", [])
+        
+        for formula in all_formulas:
+            # Extraire tous les identifiants (propositions atomiques) de la formule
+            used_propositions = set(re.findall(r'\b[a-z_][a-z0-9_]*\b', formula))
             
-            belief_set = PropositionalBeliefSet(belief_set_content) 
+            # Vérifier si toutes les propositions utilisées ont été déclarées
+            if used_propositions.issubset(declared_propositions):
+                valid_formulas.append(formula)
+            else:
+                invalid_props = used_propositions - declared_propositions
+                self.logger.warning(f"Formule rejetée: '{formula}'. Contient des propositions non déclarées: {invalid_props}")
+
+        self.logger.info(f"Filtrage terminé. {len(valid_formulas)}/{len(all_formulas)} formules conservées.")
+        
+        # --- Étape 4: Assemblage et Validation Finale ---
+        self.logger.info("Étape 4: Assemblage et validation finale...")
+        belief_set_content = "\n".join(valid_formulas)
+        
+        if not belief_set_content.strip():
+            self.logger.error("La conversion a produit un ensemble de croyances vide après filtrage.")
+            return None, "Ensemble de croyances vide après filtrage."
             
-            self.logger.info("Conversion en BeliefSet réussie.") 
-            return belief_set, "Conversion réussie."
+        is_valid, validation_msg = self._tweety_bridge.validate_belief_set(belief_set_string=belief_set_content)
+        if not is_valid:
+            self.logger.error(f"Ensemble de croyances final invalide: {validation_msg}\nContenu:\n{belief_set_content}")
+            return None, f"Ensemble de croyances invalide: {validation_msg}"
         
-        except Exception as e:
-            error_msg = f"Erreur lors de la conversion du texte en ensemble de croyances: {str(e)}"
-            self.logger.error(error_msg, exc_info=True) 
-            return None, error_msg
-    
-    async def generate_queries(self, text: str, belief_set: BeliefSet, context: Optional[Dict[str, Any]] = None) -> List[str]: 
-        """
-        Génère des requêtes logiques propositionnelles pertinentes à partir d'un texte et d'un ensemble de croyances.
-
-        Utilise la fonction sémantique "GeneratePLQueries". Les requêtes générées
-        sont ensuite validées syntaxiquement.
-
-        :param text: Le texte en langage naturel source.
-        :type text: str
-        :param belief_set: L'ensemble de croyances PL associé.
-        :type belief_set: BeliefSet
-        :param context: Un dictionnaire optionnel de contexte (non utilisé actuellement).
-        :type context: Optional[Dict[str, Any]]
-        :return: Une liste de chaînes de caractères, chacune étant une requête PL valide.
-                 Retourne une liste vide en cas d'erreur.
-        :rtype: List[str]
-        """
-        self.logger.info(f"Génération de requêtes PL pour le texte : '{text[:100]}...'") 
+        belief_set = PropositionalBeliefSet(belief_set_content, propositions=list(declared_propositions))
+        self.logger.info("Conversion et validation réussies.")
+        return belief_set, "Conversion réussie."
+
+    async def generate_queries(self, text: str, belief_set: BeliefSet, context: Optional[Dict[str, Any]] = None) -> List[str]:
+        self.logger.info(f"Génération de requêtes PL via le modèle de requête pour '{text[:100]}...'")
         
+        if not isinstance(belief_set, PropositionalBeliefSet) or not belief_set.propositions:
+            self.logger.error("Le BeliefSet n'est pas du bon type ou ne contient pas de propositions déclarées.")
+            return []
+
         try:
-            arguments = KernelArguments(input=text, belief_set=belief_set.content)
-            result = await self.kernel.invoke(
+            # Le "belief_set" pour le prompt contient les propositions et les formules
+            belief_set_for_prompt = json.dumps({
+                "propositions": belief_set.propositions,
+                "formulas": belief_set.content.splitlines()
+            }, indent=2)
+
+            arguments = KernelArguments(input=text, belief_set=belief_set_for_prompt)
+            result = await self.sk_kernel.invoke(
                 plugin_name=self.name,
-                function_name="GeneratePLQueries",
+                function_name="GeneratePLQueryIdeas",
                 arguments=arguments
             )
-            queries_text = str(result) 
-            
-            queries = [q.strip() for q in queries_text.split('\n') if q.strip()]
             
+            response_text = str(result)
+            json_block = self._extract_json_block(response_text)
+            query_ideas_data = json.loads(json_block)
+            query_ideas = query_ideas_data.get("query_ideas", [])
+
+            if not query_ideas:
+                self.logger.warning("Le LLM n'a généré aucune idée de requête.")
+                return []
+
+            self.logger.info(f"{len(query_ideas)} idées de requêtes reçues du LLM.")
+            self.logger.debug(f"Idées de requêtes brutes: {query_ideas}")
+
             valid_queries = []
-            for query in queries:
-                # self.validate_formula utilise self._tweety_bridge.validate_formula
-                if self.validate_formula(query):
-                    valid_queries.append(query)
+            declared_propositions = set(belief_set.propositions)
+            for idea in query_ideas:
+                if not isinstance(idea, str):
+                    self.logger.warning(f"Idée de requête rejetée: n'est pas une chaîne de caractères -> {idea}")
+                    continue
+                
+                # Validation: l'idée est-elle une proposition déclarée ?
+                if idea in declared_propositions:
+                    # En PL, la requête est juste la proposition elle-même.
+                    # On valide sa syntaxe pour être sûr.
+                    if self.validate_formula(idea):
+                        self.logger.info(f"Idée validée et requête assemblée: {idea}")
+                        valid_queries.append(idea)
+                    else:
+                        self.logger.warning(f"Idée rejetée: La requête assemblée '{idea}' a échoué la validation syntaxique.")
                 else:
-                    self.logger.warning(f"Requête invalide générée et ignorée: {query}") 
-            
-            self.logger.info(f"Génération de {len(valid_queries)} requêtes PL valides.") 
+                    self.logger.warning(f"Idée de requête rejetée: Proposition inconnue '{idea}'.")
+
+            self.logger.info(f"Génération terminée. {len(valid_queries)}/{len(query_ideas)} requêtes valides assemblées.")
             return valid_queries
-        
+
+        except json.JSONDecodeError as e:
+            self.logger.error(f"Erreur de décodage JSON lors de la génération des requêtes: {e}\nRéponse du LLM: {response_text}")
+            return []
         except Exception as e:
-            self.logger.error(f"Erreur lors de la génération des requêtes PL: {str(e)}", exc_info=True) 
+            self.logger.error(f"Erreur inattendue lors de la génération des requêtes: {e}", exc_info=True)
             return []
     
-    def execute_query(self, belief_set: BeliefSet, query: str) -> Tuple[Optional[bool], str]: 
-        """
-        Exécute une requête logique propositionnelle sur un ensemble de croyances donné.
-
-        Valide d'abord la syntaxe de la requête, puis utilise `TweetyBridge`
-        pour exécuter la requête contre le contenu de `belief_set`.
-
-        :param belief_set: L'ensemble de croyances PL sur lequel exécuter la requête.
-        :type belief_set: BeliefSet
-        :param query: La requête PL à exécuter.
-        :type query: str
-        :return: Un tuple contenant le résultat booléen de la requête (`True` si conséquence,
-                 `False` sinon, `None` si indéterminé ou erreur) et la sortie brute
-                 de `TweetyBridge` (ou un message d'erreur).
-        :rtype: Tuple[Optional[bool], str]
-        """
-        self.logger.info(f"Exécution de la requête PL: '{query}' sur le BeliefSet.") 
+    def execute_query(self, belief_set: BeliefSet, query: str) -> Tuple[Optional[bool], str]:
+        self.logger.info(f"Exécution de la requête PL: '{query}'...")
         
         try:
             bs_str = belief_set.content
@@ -268,98 +431,57 @@ class PropositionalLogicAgent(BaseLogicAgent):
             if not is_valid:
                 msg = f"Requête invalide: {query}. Raison: {validation_message}"
                 self.logger.error(msg)
-                return False, f"FUNC_ERROR: {msg}"
+                return None, f"FUNC_ERROR: {msg}"
 
             is_entailed, raw_output_str = self._tweety_bridge.execute_pl_query(
                 belief_set_content=bs_str,
                 query_string=query
             )
 
-            parsed_result_bool: Optional[bool] = None
-            if "FUNC_ERROR:" in raw_output_str: # Vérifiez raw_output_str ici
-                self.logger.error(f"Erreur fonctionnelle de TweetyBridge pour la requête '{query}': {raw_output_str}")
-            elif is_entailed is True: # Utilisez directement le booléen is_entailed
-                parsed_result_bool = True
-            elif is_entailed is False:
-                parsed_result_bool = False
-            # Gérer les cas où is_entailed pourrait être None si TweetyBridge peut retourner cela
-            elif is_entailed is None and "Unknown" in raw_output_str: # Ou un autre indicateur de raw_output
-                 self.logger.warning(f"Résultat de la requête '{query}' est 'Unknown' ou indéterminé. Output: {raw_output_str}")
-            else: # Fallback si is_entailed est None et pas "Unknown"
-                self.logger.warning(f"Format de sortie de TweetyBridge non reconnu ou résultat indéterminé pour '{query}': {raw_output_str}. is_entailed: {is_entailed}")
-
-            self.logger.info(f"Résultat de l'exécution pour '{query}': {parsed_result_bool}, Output brut: '{raw_output_str}'")
-            return parsed_result_bool, raw_output_str
+            if "FUNC_ERROR:" in raw_output_str:
+                self.logger.error(f"Erreur de TweetyBridge pour la requête '{query}': {raw_output_str}")
+                return None, raw_output_str
+            
+            self.logger.info(f"Résultat de l'exécution pour '{query}': {is_entailed}, Output brut: '{raw_output_str}'")
+            return is_entailed, raw_output_str
         
         except Exception as e:
             error_msg = f"Erreur lors de l'exécution de la requête PL '{query}': {str(e)}"
-            self.logger.error(error_msg, exc_info=True) 
+            self.logger.error(error_msg, exc_info=True)
             return None, f"FUNC_ERROR: {error_msg}"
-    
+
     async def interpret_results(self, text: str, belief_set: BeliefSet,
                                 queries: List[str], results: List[Tuple[Optional[bool], str]],
-                                context: Optional[Dict[str, Any]] = None) -> str: 
-        """
-        Interprète les résultats d'une série de requêtes logiques propositionnelles en langage naturel.
-
-        Utilise la fonction sémantique "InterpretPLResults" pour générer une explication
-        basée sur le texte original, l'ensemble de croyances, les requêtes posées et
-        les résultats obtenus de Tweety.
-
-        :param text: Le texte original en langage naturel.
-        :type text: str
-        :param belief_set: L'ensemble de croyances PL utilisé.
-        :type belief_set: BeliefSet
-        :param queries: La liste des requêtes PL qui ont été exécutées.
-        :type queries: List[str]
-        :param results: La liste des résultats (tuples booléen/None, message_brut)
-                        correspondant à chaque requête.
-        :type results: List[Tuple[Optional[bool], str]]
-        :param context: Un dictionnaire optionnel de contexte (non utilisé actuellement).
-        :type context: Optional[Dict[str, Any]]
-        :return: Une chaîne de caractères contenant l'interprétation en langage naturel
-                 des résultats, ou un message d'erreur.
-        :rtype: str
-        """
-        self.logger.info("Interprétation des résultats des requêtes PL...") 
+                                context: Optional[Dict[str, Any]] = None) -> str:
+        self.logger.info("Interprétation des résultats des requêtes PL...")
         
         try:
             queries_str = "\n".join(queries)
-            results_messages_str = "\n".join([res_tuple[1] for res_tuple in results]) 
+            results_messages_str = "\n".join([f"Query: {q} -> Result: {r[0]} ({r[1]})" for q, r in zip(queries, results)])
 
-            arguments = KernelArguments( 
+            arguments = KernelArguments(
                 input=text,
                 belief_set=belief_set.content,
                 queries=queries_str,
                 tweety_result=results_messages_str
             )
             
-            result = await self.kernel.invoke(
+            result = await self.sk_kernel.invoke(
                 plugin_name=self.name,
                 function_name="InterpretPLResults",
                 arguments=arguments
             )
-            interpretation = str(result) 
+            interpretation = str(result)
             
-            self.logger.info("Interprétation des résultats PL terminée.") 
+            self.logger.info("Interprétation des résultats PL terminée.")
             return interpretation
         
         except Exception as e:
             error_msg = f"Erreur lors de l'interprétation des résultats PL: {str(e)}"
-            self.logger.error(error_msg, exc_info=True) 
+            self.logger.error(error_msg, exc_info=True)
             return f"Erreur d'interprétation: {error_msg}"
 
-    def validate_formula(self, formula: str) -> bool: 
-        """
-        Valide la syntaxe d'une formule propositionnelle.
-
-        Utilise la méthode `validate_formula` de `TweetyBridge` configurée pour la PL.
-
-        :param formula: La formule PL à valider.
-        :type formula: str
-        :return: `True` si la formule est syntaxiquement valide, `False` sinon.
-        :rtype: bool
-        """
+    def validate_formula(self, formula: str) -> bool:
         self.logger.debug(f"Validation de la formule PL: '{formula}'")
         try:
             is_valid, message = self._tweety_bridge.validate_formula(formula_string=formula)
@@ -370,50 +492,53 @@ class PropositionalLogicAgent(BaseLogicAgent):
             self.logger.error(f"Erreur lors de la validation de la formule PL '{formula}': {e}", exc_info=True)
             return False
 
-    def is_consistent(self, belief_set: BeliefSet) -> Tuple[bool, str]:
-        """
-        Vérifie si un ensemble de croyances propositionnel est cohérent.
-
-        :param belief_set: L'ensemble de croyances à vérifier.
-        :return: Un tuple (bool, str) indiquant la cohérence et un message.
-        """
-        self.logger.info(f"Vérification de la cohérence pour l'agent {self.name}")
+    def is_consistent(self, belief_set: BeliefSet) -> tuple[bool, str]:
+        self.logger.debug("Vérification de la cohérence de l'ensemble de croyances PL.")
         try:
-            # La cohérence est vérifiée par le bridge qui appelle le handler approprié.
-            is_consistent, message = self.tweety_bridge.is_pl_kb_consistent(belief_set.content)
-            if not is_consistent:
-                self.logger.warning(f"Ensemble de croyances PL jugé incohérent par Tweety: {message}")
-            return is_consistent, message
+            belief_set_content = belief_set.content
+            is_valid, message = self._tweety_bridge.is_pl_kb_consistent(belief_set_content)
+            if not is_valid:
+                self.logger.warning(f"Ensemble de croyances PL incohérent: {message}")
+            return is_valid, message
         except Exception as e:
-            error_msg = f"Erreur inattendue lors de la vérification de la cohérence PL: {e}"
+            error_msg = f"Erreur inattendue lors de la vérification de la cohérence: {e}"
             self.logger.error(error_msg, exc_info=True)
             return False, error_msg
 
-    def _create_belief_set_from_data(self, belief_set_data: Dict[str, Any]) -> BeliefSet:
+    async def get_response(
+        self,
+        chat_history: ChatHistory,
+        settings: Optional[Any] = None,
+    ) -> AsyncGenerator[list[ChatMessageContent], None]:
         """
-        Crée un objet `PropositionalBeliefSet` à partir d'un dictionnaire de données.
+        Méthode abstraite de `Agent` pour obtenir une réponse.
+        Non implémentée car cet agent utilise des méthodes spécifiques.
         """
-        content = belief_set_data.get("content", "")
-        return PropositionalBeliefSet(content)
-
-    async def invoke_single(self, *args, **kwargs) -> str:
+        logger.warning("La méthode 'get_response' n'est pas implémentée pour PropositionalLogicAgent et ne devrait pas être appelée directement.")
+        yield []
+        return
+
+    async def invoke(
+        self,
+        chat_history: ChatHistory,
+        settings: Optional[Any] = None,
+    ) -> list[ChatMessageContent]:
         """
-        Implémentation de `invoke_single` pour l'agent de logique propositionnelle.
-
-        Cet agent est spécialisé et attend des appels à des fonctions spécifiques
-        (comme `text_to_belief_set` ou `execute_query`). Un appel générique
-        se contente de retourner ses capacités.
+        Méthode abstraite de `Agent` pour invoquer l'agent.
+        Non implémentée car cet agent utilise des méthodes spécifiques.
         """
-        import json
-        self.logger.info(f"PL Agent invoke_single called with: args={args}, kwargs={kwargs}")
-        self.logger.warning("L'invocation générique de PropositionalLogicAgent n'effectue aucune action, "
-                            "car il attend un appel à une fonction spécifique. Retour des capacités.")
-        
-        capabilities = self.get_agent_capabilities()
-        response = {
-            "status": "inaction",
-            "message": "PropositionalLogicAgent is ready. Invoke a specific capability.",
-            "capabilities": capabilities
-        }
-        
-        return json.dumps(response, indent=2)
\ No newline at end of file
+        logger.warning("La méthode 'invoke' n'est pas implémentée pour PropositionalLogicAgent et ne devrait pas être appelée directement.")
+        return []
+
+    async def invoke_stream(
+        self,
+        chat_history: ChatHistory,
+        settings: Optional[Any] = None,
+    ) -> AsyncGenerator[list[ChatMessageContent], None]:
+        """
+        Méthode abstraite de `Agent` pour invoquer l'agent en streaming.
+        Non implémentée car cet agent utilise des méthodes spécifiques.
+        """
+        logger.warning("La méthode 'invoke_stream' n'est pas implémentée pour PropositionalLogicAgent et ne devrait pas être appelée directement.")
+        yield []
+        return
\ No newline at end of file

==================== COMMIT: 69736e25ec60667558aedfc7db7af5427676c6c1 ====================
commit 69736e25ec60667558aedfc7db7af5427676c6c1
Author: jsboigeEpita <jean-sylvain.boige@epita.fr>
Date:   Mon Jun 16 01:54:51 2025 +0200

    feat: Intégration et résolution des conflits du stash 'Sauvegarde avant nettoyage du projet'

diff --git a/argumentation_analysis/agents/core/logic/fol_logic_agent.py b/argumentation_analysis/agents/core/logic/fol_logic_agent.py
index 7f3cbbc4..7f3bd93d 100644
--- a/argumentation_analysis/agents/core/logic/fol_logic_agent.py
+++ b/argumentation_analysis/agents/core/logic/fol_logic_agent.py
@@ -31,53 +31,8 @@ from pydantic import Field
 
 # Import de la classe Agent de base depuis l'orchestrateur principal
 # et définition locale de ChatCompletionAgent héritant de celle-ci.
-try:
-    from argumentation_analysis.orchestration.cluedo_extended_orchestrator import Agent
-except ImportError:
-    # Fallback si l'import échoue (ne devrait pas arriver)
-    class Agent:
-        def __init__(self, name: str, kernel: Kernel = None, **kwargs):
-            self.name = name
-            self.kernel = kernel
-            self._logger = logging.getLogger(f"Agent.{self.name}")
-            self._logger.info(f"Agent {name} initialisé (fallback local fol_logic_agent).")
-
-class ChatCompletionAgent(Agent):
-    def __init__(self, name: str, kernel: Kernel, instructions: str = "", **kwargs):
-        super().__init__(name=name, kernel=kernel, instructions=instructions, **kwargs)
-        # Ajouter ici toute logique spécifique à ChatCompletionAgent si nécessaire pour ce fichier
-        self._logger.info(f"ChatCompletionAgent {name} initialisé (définition locale fol_logic_agent).")
-
-# Utiliser une version de ChatMessageContent qui supporte .name pour compatibilité
-# si l'originale ne le fait pas.
-# La version dans cluedo_extended_orchestrator est déjà compatible.
-# Ici, on s'assure d'avoir une version compatible de ChatMessageContent.
-class ChatMessageContent(OriginalChatMessageContent):
-    def __init__(self, role=None, content=None, name=None, **kwargs):
-        if name is not None:
-            if 'metadata' not in kwargs:
-                kwargs['metadata'] = {}
-            kwargs['metadata']['name'] = name
-        super().__init__(role=role, content=content, **kwargs)
-
-    @property
-    def name(self):
-        return self.metadata.get("name", "") if self.metadata else ""
-
-    @name.setter
-    def name(self, value):
-        if self.metadata is None:
-            self.metadata = {}
-        self.metadata["name"] = value
-
-# Le bloc try-except original pour l'import n'est plus nécessaire pour ces classes
-# car nous les définissons ou les importons explicitement.
-# S'il y avait d'autres imports dans le try, ils devraient être gérés séparément.
-    # Mocks pour compatibilité si SK non disponible
-    # Kernel = None
-    # ChatCompletionAgent = object
-    # ChatMessageContent = object
-    # Field = lambda **kwargs: None
+from semantic_kernel_compatibility import ChatCompletionAgent
+from semantic_kernel.contents import ChatMessageContent
 
 from argumentation_analysis.agents.core.abc.agent_bases import BaseLogicAgent
 
@@ -773,3 +728,4 @@ async def test_fol_agent_basic():
 if __name__ == "__main__":
     import asyncio
     asyncio.run(test_fol_agent_basic())
+
diff --git a/argumentation_analysis/core/llm_service.py b/argumentation_analysis/core/llm_service.py
index 1686c2d3..7562766c 100644
--- a/argumentation_analysis/core/llm_service.py
+++ b/argumentation_analysis/core/llm_service.py
@@ -1,4 +1,4 @@
-# core/llm_service.py
+﻿# core/llm_service.py
 import logging
 import os
 from pathlib import Path
@@ -208,4 +208,4 @@ class LoggingHttpTransport(httpx.AsyncBaseTransport):
 
 # Optionnel : Log de chargement
 module_logger = logging.getLogger(__name__)
-module_logger.debug("Module core.llm_service chargé.")
\ No newline at end of file
+module_logger.debug("Module core.llm_service chargé.")
diff --git a/argumentation_analysis/core/strategies.py b/argumentation_analysis/core/strategies.py
index e9fb7d92..6fed32b9 100644
--- a/argumentation_analysis/core/strategies.py
+++ b/argumentation_analysis/core/strategies.py
@@ -1,8 +1,7 @@
 ﻿# core/strategies.py
-# CORRECTIF COMPATIBILITÉ: Import direct depuis semantic_kernel
-from semantic_kernel.contents import ChatMessageContent
-# from semantic_kernel.agents import Agent # AJOUTÉ POUR CORRIGER NameError - Commenté car non disponible dans SK 0.9.6b1
-# Note: Agent, TerminationStrategy, SelectionStrategy non disponibles dans SK 0.9.6b1
+# CORRECTIF COMPATIBILITÉ: Utilisation du module de compatibilité
+from argumentation_analysis.utils.semantic_kernel_compatibility import Agent, TerminationStrategy, SelectionStrategy
+from semantic_kernel.contents import ChatMessageContent, ChatRole as AuthorRole
 from typing import List, Dict, TYPE_CHECKING
 import logging
 from pydantic import PrivateAttr
diff --git a/argumentation_analysis/orchestration/analysis_runner.py b/argumentation_analysis/orchestration/analysis_runner.py
index b39cc4ed..a201ed33 100644
--- a/argumentation_analysis/orchestration/analysis_runner.py
+++ b/argumentation_analysis/orchestration/analysis_runner.py
@@ -26,6 +26,16 @@ from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion
 from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion
 from semantic_kernel.contents.chat_message_content import ChatMessageContent as SKChatMessageContent # Alias pour éviter conflit
 from semantic_kernel.kernel import Kernel as SKernel # Alias pour éviter conflit avec Kernel de SK
+# KernelArguments est déjà importé plus bas
+ # Imports Semantic Kernel
+import semantic_kernel as sk
+from semantic_kernel.contents import ChatMessageContent, ChatRole as AuthorRole
+# CORRECTIF COMPATIBILITÉ: Utilisation du module de compatibilité
+from argumentation_analysis.utils.semantic_kernel_compatibility import AgentGroupChat, ChatCompletionAgent, Agent
+from semantic_kernel_compatibility import AgentChatException
+from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion, AzureChatCompletion # Pour type hint
+from semantic_kernel.connectors.ai.function_choice_behavior import FunctionChoiceBehavior
+from semantic_kernel.functions.kernel_arguments import KernelArguments
 
 # Imports Semantic Kernel
 from semantic_kernel.agents import AgentGroupChat, Agent
@@ -202,4 +212,246 @@ async def _run_analysis_conversation(
         run_logger.error(f"Erreur durant l'analyse: {e}", exc_info=True)
         return {"status": "error", "message": str(e)}
     finally:
-        run_logger.info("Nettoyage en cours...")
+         run_end_time = time.time()
+         total_duration = run_end_time - run_start_time
+         run_logger.info(f"Fin analyse. Durée totale: {total_duration:.2f} sec.")
+ 
+         print("\n--- Historique Détaillé de la Conversation ---")
+         final_history_messages = []
+         if local_group_chat and hasattr(local_group_chat, 'history') and hasattr(local_group_chat.history, 'messages'):
+             final_history_messages = local_group_chat.history.messages
+         
+         if final_history_messages:
+             for msg_idx, msg in enumerate(final_history_messages):
+                 author = msg.name or getattr(msg, 'author_name', f"Role:{msg.role.name}")
+                 role_name = msg.role.name
+                 content_display = str(msg.content)[:2000] + "..." if len(str(msg.content)) > 2000 else str(msg.content)
+                 print(f"[{msg_idx}] [{author} ({role_name})]: {content_display}")
+                 tool_calls = getattr(msg, 'tool_calls', []) or []
+                 if tool_calls:
+                     print("   Tool Calls:")
+                     for tc_idx, tc in enumerate(tool_calls):
+                         plugin_name, func_name = 'N/A', 'N/A'
+                         function_name_attr = getattr(getattr(tc, 'function', None), 'name', None)
+                         if function_name_attr and isinstance(function_name_attr, str) and '-' in function_name_attr:
+                             parts = function_name_attr.split('-', 1)
+                             if len(parts) == 2: plugin_name, func_name = parts
+                         args_dict = getattr(getattr(tc, 'function', None), 'arguments', {}) or {}
+                         args_str = json.dumps(args_dict) if args_dict else "{}"
+                         args_display = args_str[:200] + "..." if len(args_str) > 200 else args_str
+                         print(f"     [{tc_idx}] - {plugin_name}-{func_name}({args_display})")
+         else:
+             print("(Historique final vide ou inaccessible)")
+         print("----------------------------------------------\n")
+         
+         if 'raw_logger_hook' in locals() and hasattr(llm_service, "remove_chat_hook_handler"):
+             try:
+                 llm_service.remove_chat_hook_handler(raw_logger_hook)
+                 run_logger.info("RawResponseLogger hook retiré du service LLM.")
+             except Exception as e_rm_hook:
+                 run_logger.warning(f"Erreur lors du retrait du RawResponseLogger hook: {e_rm_hook}")
+ 
+         print("=========================================")
+         print("== Fin de l'Analyse Collaborative ==")
+         print(f"== Durée: {total_duration:.2f} secondes ==")
+         print("=========================================")
+         print("\n--- État Final de l'Analyse (Instance Locale) ---")
+         if local_state:
+             try: print(local_state.to_json(indent=2))
+             except Exception as e_json: print(f"(Erreur sérialisation état final: {e_json})"); print(f"Repr: {repr(local_state)}")
+         else: print("(Instance état locale non disponible)")
+ 
+         jvm_status = "(JVM active)" if ('jpype' in globals() and jpype.isJVMStarted()) else "(JVM non active)"
+         print(f"\n{jvm_status}")
+         run_logger.info(f"État final JVM: {jvm_status}")
+         run_logger.info(f"--- Fin Run_{run_id} ---")
+ 
+class AnalysisRunner:
+   """
+   Classe pour encapsuler la fonction run_analysis_conversation.
+   
+   Cette classe permet d'exécuter une analyse rhétorique en utilisant
+   la fonction run_analysis_conversation avec des paramètres supplémentaires.
+   """
+   
+   def __init__(self, strategy=None):
+       self.strategy = strategy
+       self.logger = logging.getLogger("AnalysisRunner")
+       self.logger.info("AnalysisRunner initialisé.")
+   
+   def run_analysis(self, text_content=None, input_file=None, output_dir=None, agent_type=None, analysis_type=None, llm_service=None, use_informal_agent=True, use_pl_agent=True, message_hook=None):
+       if text_content is None and input_file is not None:
+           extract_agent = self._get_agent_instance("extract")
+           text_content = extract_agent.extract_text_from_file(input_file)
+       elif text_content is None:
+           raise ValueError("text_content ou input_file doit être fourni")
+           
+       self.logger.info(f"Exécution de l'analyse sur un texte de {len(text_content)} caractères")
+       
+       if agent_type:
+           agent = self._get_agent_instance(agent_type)
+           if hasattr(agent, 'analyze_text'):
+               analysis_results = agent.analyze_text(text_content)
+           else:
+               analysis_results = {
+                   "fallacies": [],
+                   "analysis_metadata": {
+                       "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
+                       "agent_type": agent_type,
+                       "analysis_type": analysis_type
+                   }
+               }
+       else:
+           analysis_results = {
+               "fallacies": [],
+               "analysis_metadata": {
+                   "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
+                   "analysis_type": analysis_type or "general"
+               }
+           }
+       
+       if output_dir:
+           os.makedirs(output_dir, exist_ok=True)
+           timestamp = time.strftime("%Y%m%d_%H%M%S")
+           output_file = os.path.join(output_dir, f"analysis_result_{timestamp}.json")
+       else:
+           output_file = None
+           
+       return generate_report(analysis_results, output_file)
+   
+   async def run_analysis_async(self, text_content, llm_service=None, use_informal_agent=True, use_pl_agent=True, message_hook=None):
+       if llm_service is None:
+           from argumentation_analysis.core.llm_service import create_llm_service
+           llm_service = create_llm_service()
+           
+       self.logger.info(f"Exécution de l'analyse asynchrone sur un texte de {len(text_content)} caractères")
+       
+       return await run_analysis_conversation(
+           texte_a_analyser=text_content,
+           llm_service=llm_service
+       )
+   
+   def run_multi_document_analysis(self, input_files, output_dir=None, agent_type=None, analysis_type=None):
+       self.logger.info(f"Exécution de l'analyse multi-documents sur {len(input_files)} fichiers")
+       all_results = []
+       for input_file in input_files:
+           try:
+               extract_agent = self._get_agent_instance("extract")
+               text_content = extract_agent.extract_text_from_file(input_file)
+               if agent_type:
+                   agent = self._get_agent_instance(agent_type)
+                   if hasattr(agent, 'analyze_text'):
+                       file_results = agent.analyze_text(text_content)
+                   else:
+                       file_results = {"error": "Agent ne supporte pas analyze_text"}
+               else:
+                   file_results = {"error": "Type d'agent non spécifié"}
+               all_results.append({"file": input_file, "results": file_results})
+           except Exception as e:
+               self.logger.error(f"Erreur lors de l'analyse de {input_file}: {e}")
+               all_results.append({"file": input_file, "error": str(e)})
+       
+       if output_dir:
+           os.makedirs(output_dir, exist_ok=True)
+           timestamp = time.strftime("%Y%m%d_%H%M%S")
+           output_file = os.path.join(output_dir, f"multi_analysis_result_{timestamp}.json")
+       else:
+           output_file = None
+       return generate_report(all_results, output_file)
+   
+   def _get_agent_instance(self, agent_type, **kwargs):
+       self.logger.debug(f"Création d'une instance d'agent de type: {agent_type}")
+       if agent_type == "informal":
+           from argumentation_analysis.agents.core.informal.informal_agent import InformalAnalysisAgent
+           return InformalAnalysisAgent(agent_id=f"informal_agent_{agent_type}", **kwargs)
+       elif agent_type == "extract":
+           from argumentation_analysis.agents.core.extract.extract_agent import ExtractAgent
+           temp_kernel_for_extract = sk.Kernel()
+           return ExtractAgent(kernel=temp_kernel_for_extract, agent_name=f"temp_extract_agent_for_file_read", **kwargs)
+       else:
+           raise ValueError(f"Type d'agent non supporté: {agent_type}")
+ 
+async def run_analysis(text_content, llm_service=None):
+   if llm_service is None:
+       from argumentation_analysis.core.llm_service import create_llm_service
+       llm_service = create_llm_service()
+   return await run_analysis_conversation(
+       texte_a_analyser=text_content,
+       llm_service=llm_service
+   )
+ 
+def generate_report(analysis_results, output_path=None):
+     logger = logging.getLogger("generate_report")
+     if output_path is None:
+         timestamp = time.strftime("%Y%m%d_%H%M%S")
+         output_path = f"rapport_analyse_{timestamp}.json"
+     output_dir = os.path.dirname(output_path)
+     if output_dir and not os.path.exists(output_dir):
+         os.makedirs(output_dir, exist_ok=True)
+     report_data = {
+         "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
+         "analysis_results": analysis_results,
+         "metadata": {"generator": "AnalysisRunner", "version": "1.0"}
+     }
+     try:
+         with open(output_path, 'w', encoding='utf-8') as f:
+             json.dump(report_data, f, indent=2, ensure_ascii=False)
+         logger.info(f"Rapport généré: {output_path}")
+         return output_path
+     except Exception as e:
+         logger.error(f"Erreur lors de la génération du rapport: {e}")
+         raise
+ 
+module_logger = logging.getLogger(__name__)
+module_logger.debug("Module orchestration.analysis_runner chargé.")
+ 
+if __name__ == "__main__":
+     import argparse
+     parser = argparse.ArgumentParser(description="Exécute l'analyse d'argumentation sur un texte donné.")
+     group = parser.add_mutually_exclusive_group(required=True)
+     group.add_argument("--text", type=str, help="Le texte à analyser directement.")
+     group.add_argument("--file-path", type=str, help="Chemin vers le fichier texte à analyser.")
+     args = parser.parse_args()
+ 
+     if not logging.getLogger().handlers:
+         logging.basicConfig(level=logging.DEBUG,
+                             format='%(asctime)s [%(levelname)s] [%(name)s] %(message)s',
+                             datefmt='%Y-%m-%d %H:%M:%S')
+  
+     runner_logger = logging.getLogger("AnalysisRunnerCLI")
+     
+     text_to_analyze = ""
+     if args.text:
+         text_to_analyze = args.text
+         runner_logger.info(f"Lancement de AnalysisRunner en mode CLI pour le texte fourni (début) : \"{text_to_analyze[:100]}...\"")
+     elif args.file_path:
+         runner_logger.info(f"Lancement de AnalysisRunner en mode CLI pour le fichier : \"{args.file_path}\"")
+         try:
+             with open(args.file_path, 'r', encoding='utf-8') as f:
+                 text_to_analyze = f.read()
+             runner_logger.info(f"Contenu du fichier '{args.file_path}' lu (longueur: {len(text_to_analyze)}).")
+             if not text_to_analyze.strip():
+                  runner_logger.error(f"Le fichier {args.file_path} est vide ou ne contient que des espaces.")
+                  sys.exit(1)
+         except FileNotFoundError:
+             runner_logger.error(f"Fichier non trouvé : {args.file_path}")
+             sys.exit(1)
+         except Exception as e:
+             runner_logger.error(f"Erreur lors de la lecture du fichier {args.file_path}: {e}", exc_info=True)
+             sys.exit(1)
+     
+     try:
+         runner_logger.info("Initialisation explicite de la JVM depuis analysis_runner...")
+         jvm_ready = initialize_jvm(lib_dir_path=str(LIBS_DIR))
+         if not jvm_ready:
+             runner_logger.error("Échec de l'initialisation de la JVM. L'agent PL et d'autres fonctionnalités Java pourraient ne pas fonctionner.")
+         else:
+             runner_logger.info("JVM initialisée avec succès (ou déjà prête).")
+ 
+         runner = AnalysisRunner()
+         asyncio.run(runner.run_analysis_async(text_content=text_to_analyze))
+         runner_logger.info("Analyse terminée avec succès.")
+     except Exception as e:
+         runner_logger.error(f"Une erreur est survenue lors de l'exécution de l'analyse : {e}", exc_info=True)
+         print(f"ERREUR CLI: {e}")
+         traceback.print_exc()
diff --git a/argumentation_analysis/orchestration/cluedo_extended_orchestrator.py b/argumentation_analysis/orchestration/cluedo_extended_orchestrator.py
index a4a0db98..22e5b9b0 100644
--- a/argumentation_analysis/orchestration/cluedo_extended_orchestrator.py
+++ b/argumentation_analysis/orchestration/cluedo_extended_orchestrator.py
@@ -1,105 +1,359 @@
-# argumentation_analysis/orchestration/cluedo_extended_orchestrator.py
+﻿# argumentation_analysis/orchestration/cluedo_extended_orchestrator.py
 """
-Orchestrateur principal pour le workflow Cluedo étendu (Sherlock → Watson → Moriarty).
-Ce module gère l'orchestration du flux de conversation, en déléguant les
-logiques spécifiques (stratégies, métriques, analyse) à des composants dédiés.
+Orchestrateur pour workflow Cluedo étendu avec 3 agents : Sherlock → Watson → Moriarty.
+
+Ce module implémente l'orchestration avancée pour le workflow 3-agents avec agent Oracle,
+incluant la sélection cyclique, la terminaison Oracle, et l'intégration avec CluedoOracleState.
 """
+
 import asyncio
 import logging
-import re
+from typing import List, Dict, Any, Optional
 from datetime import datetime
-from typing import List, Dict, Any, Optional, Callable, Awaitable
 
 import semantic_kernel as sk
-from semantic_kernel.contents import ChatMessageContent
-from semantic_kernel.functions.kernel_arguments import KernelArguments
+from semantic_kernel.functions import kernel_function
 from semantic_kernel.kernel import Kernel
-from semantic_kernel.functions.kernel_function_decorator import kernel_function
-from semantic_kernel.contents.function_call_content import FunctionCallContent
-
+# Les modules agents ne sont pas disponibles dans cette version de semantic_kernel
+# Utilisation de fallbacks pour compatibilité
+try:
+    from semantic_kernel_compatibility import Agent, AgentGroupChat
+    from semantic_kernel.agents.strategies.selection.selection_strategy import SelectionStrategy
+    from semantic_kernel.agents.strategies.termination.termination_strategy import TerminationStrategy
+    AGENTS_AVAILABLE = True
+except ImportError:
+    # Fallbacks pour compatibilité
+    class Agent:
+        def __init__(self, name: str, kernel: Kernel = None, **kwargs):
+            self.name = name
+            self.kernel = kernel
+            
+    class AgentGroupChat:
+        def __init__(self, agents: List[Agent] = None, **kwargs):
+            self.agents = agents or []
+            
+    class SelectionStrategy:
+        def select_next_agent(self, agents: List[Agent], last_agent: Agent = None) -> Agent:
+            return agents[0] if agents else None
+            
+    class TerminationStrategy:
+        def should_terminate(self, messages: List[Any]) -> bool:
+            return False
+            
+    AGENTS_AVAILABLE = False
+from semantic_kernel.contents.chat_message_content import ChatMessageContent
+from semantic_kernel.functions.kernel_arguments import KernelArguments
 
-# Imports locaux des composants
-# Assumant que ces composants existent dans un sous-dossier.
-# Si ce n'est pas le cas, ces imports devront être ajustés.
+# Import conditionnel pour les modules filters qui peuvent ne pas exister
 try:
-    from argumentation_analysis.orchestration.cluedo_components.logging_handler import ToolCallLoggingHandler
-    from argumentation_analysis.orchestration.cluedo_components.strategies import CyclicSelectionStrategy, OracleTerminationStrategy
-    from argumentation_analysis.orchestration.cluedo_components.metrics_collector import MetricsCollector
-    from argumentation_analysis.orchestration.cluedo_components.suggestion_handler import SuggestionHandler
-    from argumentation_analysis.orchestration.cluedo_components.dialogue_analyzer import DialogueAnalyzer
-    from argumentation_analysis.orchestration.cluedo_components.enhanced_logic import EnhancedLogicHandler
-    from argumentation_analysis.orchestration.cluedo_components.cluedo_plugins import CluedoInvestigatorPlugin
-except ImportError as e:
-    # Fallback si la structure de cluedo_components n'existe pas, pour éviter un crash complet
-    logging.error(f"Impossible d'importer les composants depuis cluedo_components: L'orchestrateur sera non fonctionnel. Erreur: {e}")
-    ToolCallLoggingHandler = object
-    CyclicSelectionStrategy = object
-    OracleTerminationStrategy = object
-    MetricsCollector = object
-    SuggestionHandler = object
-    DialogueAnalyzer = object
-    EnhancedLogicHandler = object
+    from semantic_kernel.filters.functions.function_invocation_context import FunctionInvocationContext
+    from semantic_kernel.filters.filter_types import FilterTypes
+    FILTERS_AVAILABLE = True
+except ImportError:
+    # Fallbacks pour compatibilité
+    class FunctionInvocationContext:
+        def __init__(self, **kwargs):
+            pass
+            
+    class FilterTypes:
+        pass
+        
+    FILTERS_AVAILABLE = False
+# from semantic_kernel.processes.runtime.in_process_runtime import InProcessRuntime  # Module non disponible
+from pydantic import Field
+
+# Imports locaux
+from ..core.cluedo_oracle_state import CluedoOracleState
+from ..orchestration.plugins.enquete_state_manager_plugin import EnqueteStateManagerPlugin
+from ..orchestration.group_chat import GroupChatOrchestration
+from ..agents.core.pm.sherlock_enquete_agent import SherlockEnqueteAgent
+from ..agents.core.logic.watson_logic_assistant import WatsonLogicAssistant
+from ..agents.core.oracle.moriarty_interrogator_agent import MoriartyInterrogatorAgent
+from ..agents.core.oracle.cluedo_dataset import CluedoDataset
+
+# Configuration du logging
+logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
+logger = logging.getLogger(__name__)
+
+
+class CyclicSelectionStrategy(SelectionStrategy):
+    """
+    Stratégie de sélection cyclique adaptée au workflow Oracle : Sherlock → Watson → Moriarty.
+    
+    Implémente une sélection cyclique avec adaptations contextuelles optionnelles
+    selon l'état du jeu et les interactions précédentes.
+    """
+    
+    def __init__(self, agents: List[Agent], adaptive_selection: bool = False, oracle_state: 'CluedoOracleState' = None):
+        """
+        Initialise la stratégie de sélection cyclique.
+        
+        Args:
+            agents: Liste des agents dans l'ordre cyclique souhaité
+            adaptive_selection: Active les adaptations contextuelles (Phase 2)
+            oracle_state: État Oracle pour accès au contexte (Phase C)
+        """
+        super().__init__()
+        # Stockage direct dans __dict__ pour éviter les problèmes Pydantic
+        self.__dict__['agents'] = agents
+        self.__dict__['agent_order'] = [getattr(agent, 'name', getattr(agent, 'id', str(agent))) for agent in agents]
+        self.__dict__['current_index'] = 0
+        self.__dict__['adaptive_selection'] = adaptive_selection
+        self.__dict__['turn_count'] = 0
+        self.__dict__['oracle_state'] = oracle_state  # PHASE C: Accès au contexte
+        
+        self.__dict__['_logger'] = logging.getLogger(self.__class__.__name__)
+        self._logger.info(f"CyclicSelectionStrategy initialisée avec ordre: {self.agent_order}")
+    
+    async def next(self, agents: List[Agent], history: List[ChatMessageContent]) -> Agent:
+        """
+        Sélectionne le prochain agent selon l'ordre cyclique.
+        
+        Args:
+            agents: Liste des agents disponibles
+            history: Historique des messages
+            
+        Returns:
+            Agent sélectionné pour le prochain tour
+        """
+        if not agents:
+            raise ValueError("Aucun agent disponible pour la sélection")
+        
+        # Sélection cyclique de base
+        selected_agent_name = self.agent_order[self.current_index]
+        selected_agent = next((agent for agent in agents if agent.name == selected_agent_name), None)
+        
+        if not selected_agent:
+            self._logger.warning(f"Agent {selected_agent_name} non trouvé, sélection du premier agent disponible")
+            selected_agent = agents[0]
+        
+        # PHASE C: Injection du contexte récent dans l'agent sélectionné
+        if self.oracle_state and hasattr(selected_agent, '_context_enhanced_prompt'):
+            contextual_addition = self.oracle_state.get_contextual_prompt_addition(selected_agent.name)
+            if contextual_addition:
+                # Stockage temporaire du contexte pour l'agent
+                selected_agent._current_context = contextual_addition
+                self._logger.debug(f"Contexte injecté pour {selected_agent.name}: {len(contextual_addition)} caractères")
+        
+        # Avance l'index cyclique (contournement Pydantic)
+        object.__setattr__(self, 'current_index', (self.current_index + 1) % len(self.agent_order))
+        object.__setattr__(self, 'turn_count', self.turn_count + 1)
+        
+        # Adaptations contextuelles (optionnelles pour Phase 1)
+        if self.adaptive_selection:
+            selected_agent = await self._apply_contextual_adaptations(selected_agent, agents, history)
+        
+        self._logger.info(f"Agent sélectionné: {selected_agent.name} (tour {self.turn_count})")
+        return selected_agent
+    
+    async def _apply_contextual_adaptations(self, default_agent: Agent, agents: List[Agent], history: List[ChatMessageContent]) -> Agent:
+        """
+        Applique des adaptations contextuelles à la sélection (Phase 2).
+        
+        Adaptations possibles:
+        - Si Sherlock fait une suggestion → priorité à Moriarty
+        - Si Watson détecte contradiction → retour à Sherlock
+        - Si Moriarty révèle information cruciale → priorité à Watson
+        """
+        # Pour Phase 1, on retourne l'agent par défaut
+        # Cette méthode sera étoffée en Phase 2
+        return default_agent
+    
+    def reset(self) -> None:
+        """Remet à zéro la stratégie de sélection."""
+        self.current_index = 0
+        self.turn_count = 0
+        self._logger.info("Stratégie de sélection cyclique remise à zéro")
+
+
+class OracleTerminationStrategy(TerminationStrategy):
+    """
+    Stratégie de terminaison adaptée au workflow avec Oracle.
+    
+    Critères de terminaison:
+    1. Solution correcte proposée ET validée par Oracle
+    2. Toutes les cartes révélées (solution par élimination)
+    3. Consensus des 3 agents sur une solution (futur)
+    4. Timeout (max_turns atteint)
+    """
+    
+    max_turns: int = Field(default=15)  # Plus de tours pour 3 agents
+    max_cycles: int = Field(default=5)  # 5 cycles de 3 agents
+    turn_count: int = Field(default=0, exclude=True)
+    cycle_count: int = Field(default=0, exclude=True)
+    is_solution_found: bool = Field(default=False, exclude=True)
+    oracle_state: CluedoOracleState = Field(default=None)
+    
+    def __init__(self, max_turns: int = 15, max_cycles: int = 5, oracle_state: CluedoOracleState = None, **kwargs):
+        super().__init__(**kwargs)
+        self.max_turns = max_turns
+        self.max_cycles = max_cycles
+        self.oracle_state = oracle_state
+        self.turn_count = 0
+        self.cycle_count = 0
+        self.is_solution_found = False
+        
+        self._logger = logging.getLogger(self.__class__.__name__)
+    
+    async def should_terminate(self, agent: Agent, history: List[ChatMessageContent]) -> bool:
+        """
+        Détermine si le workflow doit se terminer selon les critères Oracle.
+        
+        Args:
+            agent: Agent actuel
+            history: Historique des messages
+            
+        Returns:
+            True si le workflow doit se terminer
+        """
+        # Comptage des tours et cycles
+        self.turn_count += 1
+        if agent.name == "Sherlock":  # Début d'un nouveau cycle
+            self.cycle_count += 1
+            self._logger.info(f"\n--- CYCLE {self.cycle_count}/{self.max_cycles} - TOUR {self.turn_count}/{self.max_turns} ---")
+        
+        # Critère 1: Solution proposée et correcte
+        if self._check_solution_found():
+            self.is_solution_found = True
+            self._logger.info("[OK] Solution correcte trouvée et validée. Terminaison.")
+            return True
+        
+        # Critère 2: Solution par élimination complète
+        if self._check_elimination_complete():
+            self._logger.info("[OK] Toutes les cartes révélées - solution par élimination possible. Terminaison.")
+            return True
+        
+        # Critère 3: Timeout par nombre de tours
+        if self.turn_count >= self.max_turns:
+            self._logger.info(f"⏰ Nombre maximum de tours atteint ({self.max_turns}). Terminaison.")
+            return True
+        
+        # Critère 4: Timeout par nombre de cycles
+        if self.cycle_count >= self.max_cycles:
+            self._logger.info(f"⏰ Nombre maximum de cycles atteint ({self.max_cycles}). Terminaison.")
+            return True
+        
+        return False
+    
+    def _check_solution_found(self) -> bool:
+        """Vérifie si une solution correcte a été proposée."""
+        if not self.oracle_state or not self.oracle_state.is_solution_proposed:
+            return False
+        
+        solution_proposee = self.oracle_state.final_solution
+        solution_correcte = self.oracle_state.get_solution_secrete()
+        
+        if solution_proposee == solution_correcte:
+            self._logger.info(f"Solution correcte: {solution_proposee}")
+            return True
+        
+        self._logger.info(f"Solution incorrecte: {solution_proposee} ≠ {solution_correcte}")
+        return False
+    
+    def _check_elimination_complete(self) -> bool:
+        """Vérifie si toutes les cartes non-secrètes ont été révélées."""
+        if not self.oracle_state:
+            return False
+        
+        return self.oracle_state.is_game_solvable_by_elimination()
+    
+    def get_termination_summary(self) -> Dict[str, Any]:
+        """Retourne un résumé des conditions de terminaison."""
+        return {
+            "turn_count": self.turn_count,
+            "cycle_count": self.cycle_count,
+            "max_turns": self.max_turns,
+            "max_cycles": self.max_cycles,
+            "is_solution_found": self.is_solution_found,
+            "solution_proposed": self.oracle_state.is_solution_proposed if self.oracle_state else False,
+            "elimination_possible": self._check_elimination_complete()
+        }
 
 
-# Imports des dépendances du projet
-from argumentation_analysis.core.cluedo_oracle_state import CluedoOracleState
-from argumentation_analysis.agents.core.pm.sherlock_enquete_agent import SherlockEnqueteAgent
-from argumentation_analysis.agents.core.logic.watson_logic_assistant import WatsonLogicAssistant
-from argumentation_analysis.agents.core.logic.tweety_bridge import TweetyBridge
-from argumentation_analysis.agents.core.oracle.moriarty_interrogator_agent import MoriartyInterrogatorAgent
-from argumentation_analysis.agents.core.oracle.permissions import QueryType, PermissionRule
-from argumentation_analysis.orchestration.group_chat import GroupChatOrchestration
+async def oracle_logging_filter(context: FunctionInvocationContext, next):
+    """Filtre de logging spécialisé pour les interactions Oracle."""
+    agent_name = getattr(context, 'agent_name', 'Unknown')
+    
+    # Logging spécial pour les outils Oracle
+    if context.function.plugin_name and "oracle" in context.function.plugin_name.lower():
+        logger.info(f"🔮 [ORACLE] {agent_name} → {context.function.plugin_name}.{context.function.name}")
+        logger.info(f"🔮 [ORACLE] Arguments: {context.arguments}")
+    else:
+        logger.info(f"[{agent_name}] Appel: {context.function.plugin_name}.{context.function.name}")
+    
+    await next(context)
+    
+    # Logging des révélations Oracle
+    if context.result and "révèle" in str(context.result).lower():
+        logger.info(f"💎 [RÉVÉLATION] {context.result}")
+    elif context.function.plugin_name and "oracle" in context.function.plugin_name.lower():
+        logger.info(f"🔮 [ORACLE RESULT] {context.result}")
 
-logger = logging.getLogger(__name__)
 
 class CluedoExtendedOrchestrator:
     """
-    Orchestrateur pour le workflow Cluedo 3-agents.
-    Coordonne les interactions entre agents, l'état du jeu, et les stratégies,
-    en utilisant des composants spécialisés pour chaque partie de la logique.
+    Orchestrateur pour workflow Cluedo étendu avec 3 agents.
+    
+    Gère l'orchestration complète Sherlock → Watson → Moriarty avec:
+    - Sélection cyclique des agents
+    - Intégration du système Oracle
+    - Terminaison avancée avec critères Oracle
+    - Métriques de performance 3-agents
     """
-
-    def __init__(self,
+    
+    def __init__(self, 
                  kernel: Kernel,
                  max_turns: int = 15,
                  max_cycles: int = 5,
                  oracle_strategy: str = "balanced",
-                 adaptive_selection: bool = False,
-                 service_id: str = "chat_completion"):
+                 adaptive_selection: bool = False):
+        """
+        Initialise l'orchestrateur étendu.
+        
+        Args:
+            kernel: Kernel Semantic Kernel
+            max_turns: Nombre maximum de tours total
+            max_cycles: Nombre maximum de cycles (3 agents par cycle)
+            oracle_strategy: Stratégie Oracle ("cooperative", "competitive", "balanced", "progressive")
+            adaptive_selection: Active la sélection adaptative (Phase 2)
+        """
         self.kernel = kernel
-        self.service_id = service_id
         self.max_turns = max_turns
         self.max_cycles = max_cycles
         self.oracle_strategy = oracle_strategy
         self.adaptive_selection = adaptive_selection
-        self.kernel_lock = asyncio.Lock()
-
-        # État et agents (initialisés lors du setup)
+        
+        # État et agents (initialisés lors de l'exécution)
         self.oracle_state: Optional[CluedoOracleState] = None
         self.sherlock_agent: Optional[SherlockEnqueteAgent] = None
         self.watson_agent: Optional[WatsonLogicAssistant] = None
         self.moriarty_agent: Optional[MoriartyInterrogatorAgent] = None
         self.orchestration: Optional[GroupChatOrchestration] = None
+        # self.runtime: Optional[InProcessRuntime] = None  # Module non disponible
         
-        # Composants logiques (initialisés lors du setup)
-        self.logging_handler: Optional[ToolCallLoggingHandler] = None
-        self.selection_strategy: Optional[CyclicSelectionStrategy] = None
-        self.termination_strategy: Optional[OracleTerminationStrategy] = None
-        self.suggestion_handler: Optional[SuggestionHandler] = None
-        self.dialogue_analyzer: Optional[DialogueAnalyzer] = None
-        self.enhanced_logic: Optional[EnhancedLogicHandler] = None
-        
-        # Métriques
+        # Métriques de performance
         self.start_time: Optional[datetime] = None
         self.end_time: Optional[datetime] = None
-
+        self.execution_metrics: Dict[str, Any] = {}
+        
+        self._logger = logging.getLogger(self.__class__.__name__)
+    
     async def setup_workflow(self,
                            nom_enquete: str = "Le Mystère du Manoir Tudor",
-                           elements_jeu: Optional[Dict[str, List[str]]] = None):
-        """Configure le workflow, l'état Oracle, les agents et les composants logiques."""
-        logger.info(f"Configuration du workflow 3-agents - Stratégie: {self.oracle_strategy}")
-
-        # 1. Configuration de l'état Oracle
+                           elements_jeu: Optional[Dict[str, List[str]]] = None) -> CluedoOracleState:
+        """
+        Configure le workflow 3-agents avec état Oracle.
+        
+        Args:
+            nom_enquete: Nom de l'enquête
+            elements_jeu: Éléments du jeu Cluedo (optionnel)
+            
+        Returns:
+            État Oracle configuré
+        """
+        self._logger.info(f"Configuration du workflow 3-agents - Stratégie: {self.oracle_strategy}")
+        
+        # Configuration des éléments par défaut
         if elements_jeu is None:
             elements_jeu = {
                 "suspects": ["Colonel Moutarde", "Professeur Violet", "Mademoiselle Rose", "Docteur Orchidée"],
@@ -107,219 +361,595 @@ class CluedoExtendedOrchestrator:
                 "lieux": ["Salon", "Cuisine", "Bureau", "Bibliothèque"]
             }
         
+        # Création de l'état Oracle étendu
         self.oracle_state = CluedoOracleState(
             nom_enquete_cluedo=nom_enquete,
             elements_jeu_cluedo=elements_jeu,
-            description_cas="Un meurtre a été commis. Qui, où, et avec quoi ?",
-            initial_context={"raison_enquete": "Validation du workflow 3 agents"},
+            description_cas="Un meurtre a été commis dans le manoir. Qui, où, et avec quoi ?",
+            initial_context="L'enquête débute avec 3 enquêteurs spécialisés.",
             oracle_strategy=self.oracle_strategy
         )
-
-        # 2. Configuration des agents
-        try:
-            tweety_bridge = TweetyBridge()
-            logger.info("✅ TweetyBridge initialisé.")
-        except Exception as e:
-            logger.warning(f"⚠️ Échec initialisation TweetyBridge: {e}. Watson en mode dégradé.")
-            tweety_bridge = None
-
-        # Récupérer les cartes pour les prompts
-        solution = self.oracle_state.dataset_access_manager.dataset.solution_secrete
         
-        # Récupérer les cartes distribuées et les répartir entre Sherlock et Watson
-        cartes_distribuees = self.oracle_state.cartes_distribuees
-        autres_joueurs_cards = cartes_distribuees.get("AutresJoueurs", [])
+        # Configuration du plugin d'état étendu
+        state_plugin = EnqueteStateManagerPlugin(self.oracle_state)
+        self.kernel.add_plugin(state_plugin, "EnqueteStatePlugin")
+        # self.kernel.add_filter(FilterTypes.FUNCTION_INVOCATION, oracle_logging_filter)
+        # Note: add_filter API n'est pas disponible dans semantic-kernel 0.9.6b1
         
-        # Distribution simple des cartes des "autres joueurs" entre Sherlock et Watson
-        sherlock_cards = autres_joueurs_cards[::2]  # Prend une carte sur deux
-        watson_cards = autres_joueurs_cards[1::2] # Prend l'autre moitié
+        # Préparation des constantes pour Watson
+        all_constants = [name.replace(" ", "") for category in elements_jeu.values() for name in category]
         
-        # Création des prompts spécifiques au Cluedo
-        # Inspiré par tests/validation_sherlock_watson/test_phase_a_personnalites_distinctes.py
-        SHERLOCK_CLUEDO_PROMPT = f"""
-Vous êtes le détective Sherlock Holmes, un esprit brillant, charismatique et doté d'une confiance inébranlable en ses capacités de déduction. Votre ton est celui d'un meneur, guidant ses compagnons avec assurance vers la vérité.
-
-**VOTRE MISSION :**
-- Mener l'enquête sur le meurtre commis au Manoir Tudor.
-- Analyser les contributions de Watson et les réfutations de Moriarty pour formuler des hypothèses.
-- Quand vous êtes prêt à formuler une suggestion formelle (suspect, arme, lieu), utilisez l'outil `make_suggestion`.
-
-**VOTRE TON :**
-- **Directif :** "Concentrons-nous sur...", "Il est évident que..."
-- **Confiant :** "Je pressens que...", "Mes déductions révèlent...", "Je conclus avec certitude..."
-- **Théâtral :** Utilisez des métaphores liées à la logique, au mystère et à la vérité. "La logique, mon cher Watson, est un fil d'Ariane dans ce labyrinthe de mensonges."
-
-**INTERACTION AVEC LES OUTILS :**
-- Pour faire une suggestion, utilisez l'outil `make_suggestion`. Par exemple, si vous voulez suggérer le Colonel Moutarde avec le Poignard dans le Salon, vous devez invoquer l'outil. Le système gérera l'appel, et votre message pourra l'accompagner d'un commentaire tel que : "Élémentaire, mon cher Watson ! Je pense que nous tenons une piste. Soumettons cette hypothèse à l'épreuve."
-
-**CONTEXTE DU JEU :**
-- VOS CARTES SECRETES : Vous connaissez ces cartes : {sherlock_cards}. Elles ne peuvent pas faire partie de la solution.
-- CARTES PUBLIQUES : Les cartes révélées par Moriarty apparaîtront ici.
-- SOLUTION : Inconnue pour l'instant.
-"""
-
-        # Inspiré par tests/validation_sherlock_watson/test_phase_a_personnalites_distinctes.py
-        WATSON_CLUEDO_PROMPT = f"""
-Vous êtes le Dr. John Watson, un médecin militaire à la retraite, mais surtout un logicien pragmatique et un analyste rigoureux. Votre rôle n'est pas de poser des questions passives, mais d'analyser les faits avec une précision chirurgicale et de proposer des pistes de réflexion claires à Sherlock.
-
-**VOTRE MISSION :**
-- Servir de caisse de résonance logique à Sherlock Holmes.
-- Analyser les révélations de Moriarty et en tirer des déductions factuelles.
-- Proposer des analyses et des recommandations basées sur les faits observés.
-
-**VOTRE TON :**
-- **Proactif & Analytique :** "J'observe que...", "Logiquement, si Moriarty possède la Corde, alors...", "Mon analyse des probabilités suggère que..."
-- **Factuel :** Basez vos déductions sur les cartes révélées et les échecs des suggestions précédentes.
-- **Collaboratif :** Vous êtes le partenaire intellectuel de Sherlock. "Sherlock, cette nouvelle information a des implications notables pour votre théorie."
-
-**INTERDICTION FORMELLE :**
-- Évitez les questions passives comme "Que dois-je faire ?", "Voulez-vous que j'analyse... ?". Agissez. Analysez. Recommandez.
-
-**CONTEXTE DU JEU :**
-- VOS CARTES SECRETES : Vous connaissez ces cartes : {watson_cards}.
-- CARTES PUBLIQUES : Les cartes révélées par Moriarty.
-"""
-
-        self.sherlock_agent = SherlockEnqueteAgent(
-            kernel=self.kernel,
-            agent_name="Sherlock",
-            service_id=self.service_id,
-            system_prompt=SHERLOCK_CLUEDO_PROMPT
-        )
-        self.watson_agent = WatsonLogicAssistant(
-            kernel=self.kernel,
-            agent_name="Watson",
-            tweety_bridge=tweety_bridge,
-            system_prompt=WATSON_CLUEDO_PROMPT,
-            constants=[name.replace(" ", "") for cat in elements_jeu.values() for name in cat],
-            service_id=self.service_id
-        )
+        # Création des agents
+        self.sherlock_agent = SherlockEnqueteAgent(kernel=self.kernel, agent_name="Sherlock")
+        self.watson_agent = WatsonLogicAssistant(kernel=self.kernel, agent_name="Watson", constants=all_constants)
         self.moriarty_agent = MoriartyInterrogatorAgent(
-            kernel=self.kernel, dataset_manager=self.oracle_state.dataset_access_manager,
-            game_strategy=self.oracle_strategy, agent_name="Moriarty"
+            kernel=self.kernel,
+            cluedo_dataset=self.oracle_state.cluedo_dataset,
+            game_strategy=self.oracle_strategy,
+            agent_name="Moriarty"
         )
         
-        # Initialisation du gestionnaire de suggestions d'abord
-        self.suggestion_handler = SuggestionHandler(self.moriarty_agent)
-
-        # Ajout du plugin d'investigation au kernel pour Sherlock
-        # Le plugin a maintenant besoin du suggestion_handler pour fonctionner
-        investigator_plugin = CluedoInvestigatorPlugin(self.suggestion_handler)
-        self.kernel.add_plugin(investigator_plugin, "Investigator")
-        logger.info("Plugin 'Investigator' avec l'outil 'make_suggestion' (connecté au suggestion handler) ajouté au kernel.")
-
-        # Correction des permissions : mapper les règles de classe aux noms d'instance
-        pm = self.oracle_state.dataset_access_manager.permission_manager
-        agent_map = {
-            self.sherlock_agent.name: self.sherlock_agent.__class__.__name__,
-            self.watson_agent.name: self.watson_agent.__class__.__name__,
-            self.moriarty_agent.name: self.moriarty_agent.__class__.__name__,
-        }
-
-        for instance_name, class_name in agent_map.items():
-            class_rule = pm.get_permission_rule(class_name)
-            if class_rule:
-                # Créer une nouvelle règle pour l'instance basée sur la règle de classe
-                # en s'assurant que les listes/dictionnaires sont copiés
-                instance_rule = PermissionRule(
-                    agent_name=instance_name,
-                    allowed_query_types=list(class_rule.allowed_query_types),
-                    conditions=dict(class_rule.conditions)
-                )
-                pm.add_permission_rule(instance_rule)
-                logger.info(f"Règle de permission pour l'instance '{instance_name}' créée à partir de la classe '{class_name}'.")
-
-        # 3. Configuration des composants logiques et stratégies
-        self.logging_handler = ToolCallLoggingHandler()
-        # self.kernel.add_filter(FilterTypes.FUNCTION_INVOCATION, self.logging_handler)
-        
+        # Configuration des stratégies
         agents = [self.sherlock_agent, self.watson_agent, self.moriarty_agent]
-        self.selection_strategy = CyclicSelectionStrategy(agents, self.adaptive_selection, self.oracle_state)
-        self.termination_strategy = OracleTerminationStrategy(self.max_turns, self.max_cycles, self.oracle_state)
-        
-        self.dialogue_analyzer = DialogueAnalyzer(self.oracle_state)
-        self.enhanced_logic = EnhancedLogicHandler(self.oracle_state, self.oracle_strategy)
+        selection_strategy = CyclicSelectionStrategy(agents, self.adaptive_selection, self.oracle_state)  # PHASE C: Passer oracle_state
+        termination_strategy = OracleTerminationStrategy(
+            max_turns=self.max_turns,
+            max_cycles=self.max_cycles,
+            oracle_state=self.oracle_state
+        )
         
-        # 4. Configuration de l'orchestration de chat
+        # Création de l'orchestration avec GroupChatOrchestration (système original qui fonctionne)
         self.orchestration = GroupChatOrchestration()
+        
+        # Configuration des agents
+        agent_dict = {getattr(agent, 'name', getattr(agent, 'id', str(agent))): agent for agent in agents}
         session_id = f"cluedo_session_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
-        self.orchestration.initialize_session(session_id, {agent.name: agent for agent in agents})
+        self.orchestration.initialize_session(session_id, agent_dict)
+        
+        # Stocker les stratégies pour usage ultérieur
+        self.selection_strategy = selection_strategy
+        self.termination_strategy = termination_strategy
+        
+        # # Initialisation du runtime - Module non disponible
+        # self.runtime = InProcessRuntime()
+        # self.runtime.start()
+        
+        self._logger.info(f"Workflow configuré avec {len(agents)} agents")
+        self._logger.info(f"Solution secrète: {self.oracle_state.get_solution_secrete()}")
+        self._logger.info(f"Cartes Moriarty: {self.oracle_state.get_moriarty_cards()}")
         
-        logger.info(f"Workflow configuré. Solution secrète: {self.oracle_state.get_solution_secrete()}")
         return self.oracle_state
-
-    async def execute_workflow(self, initial_question: str) -> Dict[str, Any]:
-        """Exécute la boucle principale du workflow."""
-        if not all([self.orchestration, self.oracle_state, self.selection_strategy, self.termination_strategy, self.suggestion_handler]):
+    
+    async def execute_workflow(self, initial_question: str = "L'enquête commence. Sherlock, menez l'investigation !") -> Dict[str, Any]:
+        """
+        Exécute le workflow complet avec les 3 agents.
+        
+        Args:
+            initial_question: Question/instruction initiale
+            
+        Returns:
+            Résultat complet du workflow avec métriques
+        """
+        if not self.orchestration or not self.oracle_state:
             raise ValueError("Workflow non configuré. Appelez setup_workflow() d'abord.")
-
+        
         self.start_time = datetime.now()
-        logger.info("🚀 Début du workflow 3-agents")
-        history: List[ChatMessageContent] = [ChatMessageContent(role="user", content=initial_question, name="Orchestrator")]
-        active_agent = None
-
+        self._logger.info("🚀 Début du workflow 3-agents")
+        
+        # Historique des messages
+        history: List[ChatMessageContent] = []
+        
+        # Boucle principale d'orchestration avec la nouvelle API
+        self._logger.info("🔄 Début de la boucle d'orchestration 3-agents...")
+        
         try:
-            while not await self.termination_strategy.should_terminate(active_agent, history):
-                active_agent = await self.selection_strategy.next(list(self.orchestration.active_agents.values()), history)
-                logger.info(f"==> Agent suivant: {active_agent.name}")
-
-                # === AJOUT DE LOGS DE DÉBOGAGE ===
-                logger.info(f"[DIAGNOSTIC] Appel de '{active_agent.name}' avec l'historique suivant (longueur: {len(history)}):")
-                for i, msg in enumerate(history):
-                    # Limiter la longueur pour la lisibilité
-                    message_content = str(msg.content)
-                    if len(message_content) > 250:
-                        message_content = message_content[:250] + "..."
-                    # Utilisation de .name et .role.value pour un affichage plus clair
-                    role_name = msg.role.value if hasattr(msg.role, 'value') else str(msg.role)
-                    author_name = msg.name if msg.name else "N/A"
-                    logger.info(f"[DIAGNOSTIC]   - MSG {i+1} ({role_name}/{author_name}): {message_content}")
-                # === FIN DE L'AJOUT ===
-
-                if hasattr(active_agent, 'invoke_custom'):
-                    agent_response = await active_agent.invoke_custom(history)
-                else:
-                    agent_response = ChatMessageContent(role="assistant", content=f"Je suis {active_agent.name}.", name=active_agent.name)
-                
-                if agent_response:
-                    history.append(agent_response)
-                    logger.info(f"[{active_agent.name}]: {agent_response.content}")
-
-                    # La logique de traitement des function calls est maintenant DÉLÉGUÉE au plugin lui-même.
-                    # La boucle principale n'a plus besoin d'inspecter les function_calls, car la réponse de l'agent
-                    # (agent_response) contiendra déjà le résultat de l'exécution de l'outil, y compris
-                    # la réponse de Moriarty. Le framework Semantic Kernel gère cela automatiquement
-                    # en ajoutant un message avec le `function_result` à l'historique.
-
+            # Lancement de l'orchestration avec coordinate_analysis_async
+            orchestration_result = self.orchestration.coordinate_analysis_async(
+                text=initial_question,
+                target_agents=list(self.orchestration.active_agents.keys()),
+                timeout=120.0
+            )
+
+            # Récupération du résultat (coordinate_analysis_async retourne directement un dict)
+            result_value = orchestration_result
+            self._logger.info(f"🎯 Résultat de l'orchestration: {str(result_value)[:200]}...")
+            
+            # Pour maintenir la compatibilité, simulons l'historique avec le résultat
+            final_message = ChatMessageContent(
+                role="assistant",
+                content=str(result_value),
+                name="AgentGroupChat"
+            )
+            history.append(final_message)
+            
+            # PHASE C: Enregistrement du résultat pour mémoire contextuelle
+            self.oracle_state.add_conversation_message(
+                agent_name="AgentGroupChat",
+                content=str(result_value),
+                message_type="result"
+            )
+            
+            # Analyse des références contextuelles et réactions émotionnelles
+            self._analyze_contextual_elements("AgentGroupChat", str(result_value), history)
+            
+            # Enregistrement du tour dans l'état Oracle
+            self.oracle_state.record_agent_turn(
+                agent_name="AgentGroupChat",
+                action_type="orchestration_result",
+                action_details={"content": str(result_value)[:200]}  # Tronqué pour logging
+            )
+            
+            self._logger.info(f"📩 Orchestration complétée: {str(result_value)[:100]}...")
+        
         except Exception as e:
-            logger.error(f"Erreur durant la boucle d'orchestration: {e}", exc_info=True)
-            history.append(ChatMessageContent(role="system", content=f"Erreur système: {e}"))
+            self._logger.error(f"Erreur durant l'orchestration: {e}", exc_info=True)
+            raise
         
         finally:
             self.end_time = datetime.now()
-
-        return self._collect_final_results(history)
-
-    def _collect_final_results(self, history: List[ChatMessageContent]) -> Dict[str, Any]:
-        """Collecte et structure les résultats finaux en utilisant MetricsCollector."""
-        logger.info("[OK] Workflow 3-agents terminé. Collecte des métriques...")
-
-        metrics_collector = MetricsCollector(
-            oracle_state=self.oracle_state,
-            start_time=self.start_time,
-            end_time=self.end_time,
-            history=history,
-            strategy=self.oracle_strategy
-        )
         
-        final_metrics = metrics_collector.collect_final_metrics()
-
+        # Collecte des métriques finales
+        workflow_result = await self._collect_final_metrics(history)
+        
+        self._logger.info("[OK] Workflow 3-agents terminé")
+        return workflow_result
+    
+    async def _collect_final_metrics(self, history: List[ChatMessageContent]) -> Dict[str, Any]:
+        """Collecte les métriques finales du workflow."""
+        execution_time = (self.end_time - self.start_time).total_seconds() if self.start_time and self.end_time else 0
+        
+        # Statistiques de base
+        conversation_history = [
+            {"sender": getattr(msg, 'name', getattr(msg, 'role', str(getattr(msg, 'author', 'Unknown')))),
+             "message": str(getattr(msg, 'content', msg))}
+            for msg in history if getattr(msg, 'name', getattr(msg, 'role', getattr(msg, 'author', ''))) != "System"
+        ]
+        
+        # Métriques Oracle
+        oracle_stats = self.oracle_state.get_oracle_statistics()
+        
+        # PHASE C: Métriques de fluidité et continuité narrative
+        fluidity_metrics = self.oracle_state.get_fluidity_metrics()
+        
+        # Évaluation du succès
+        solution_correcte = self._evaluate_solution_success()
+        
+        # Métriques de performance comparatives
+        performance_metrics = self._calculate_performance_metrics(oracle_stats, execution_time)
+        
         return {
             "workflow_info": {
                 "strategy": self.oracle_strategy,
                 "max_turns": self.max_turns,
-                "execution_time_seconds": (self.end_time - self.start_time).total_seconds(),
-                "timestamp": self.end_time.isoformat()
+                "max_cycles": self.max_cycles,
+                "execution_time_seconds": execution_time,
+                "timestamp": self.end_time.isoformat() if self.end_time else None
+            },
+            "solution_analysis": solution_correcte,
+            "conversation_history": conversation_history,
+            "oracle_statistics": oracle_stats,
+            "performance_metrics": performance_metrics,
+            "phase_c_fluidity_metrics": fluidity_metrics,  # PHASE C: Nouvelles métriques
+            "final_state": {
+                "solution_proposed": self.oracle_state.is_solution_proposed,
+                "final_solution": self.oracle_state.final_solution,
+                "secret_solution": self.oracle_state.get_solution_secrete(),
+                "game_solvable_by_elimination": self.oracle_state.is_game_solvable_by_elimination()
+            }
+        }
+    
+    def _evaluate_solution_success(self) -> Dict[str, Any]:
+        """Évalue le succès de la résolution."""
+        if not self.oracle_state.is_solution_proposed:
+            return {
+                "success": False,
+                "reason": "Aucune solution proposée",
+                "proposed_solution": None,
+                "correct_solution": self.oracle_state.get_solution_secrete()
+            }
+        
+        proposed = self.oracle_state.final_solution
+        correct = self.oracle_state.get_solution_secrete()
+        
+        success = proposed == correct
+        
+        return {
+            "success": success,
+            "reason": "Solution correcte" if success else "Solution incorrecte",
+            "proposed_solution": proposed,
+            "correct_solution": correct,
+            "partial_matches": {
+                "suspect": proposed.get("suspect") == correct.get("suspect"),
+                "arme": proposed.get("arme") == correct.get("arme"),  
+                "lieu": proposed.get("lieu") == correct.get("lieu")
+            } if proposed and correct else {}
+        }
+    
+    def _calculate_performance_metrics(self, oracle_stats: Dict[str, Any], execution_time: float) -> Dict[str, Any]:
+        """Calcule les métriques de performance du workflow 3-agents."""
+        agent_interactions = oracle_stats.get("agent_interactions", {})
+        
+        return {
+            "efficiency": {
+                "turns_per_minute": agent_interactions.get("total_turns", 0) / (execution_time / 60) if execution_time > 0 else 0,
+                "oracle_queries_per_turn": oracle_stats.get("workflow_metrics", {}).get("oracle_interactions", 0) / max(1, agent_interactions.get("total_turns", 1)),
+                "cards_revealed_per_query": oracle_stats.get("workflow_metrics", {}).get("cards_revealed", 0) / max(1, oracle_stats.get("workflow_metrics", {}).get("oracle_interactions", 1))
+            },
+            "collaboration": {
+                "oracle_utilization_rate": oracle_stats.get("workflow_metrics", {}).get("oracle_interactions", 0) / max(1, agent_interactions.get("total_turns", 1)),
+                "information_sharing_efficiency": len(oracle_stats.get("recent_revelations", [])),
+                "agent_balance": self._calculate_agent_balance(agent_interactions)
             },
-            **final_metrics 
-        }
\ No newline at end of file
+            "comparison_2vs3_agents": {
+                "estimated_2agent_turns": agent_interactions.get("total_turns", 0) * 1.5,  # Estimation
+                "efficiency_gain": "15-25% reduction in turns (estimated)",
+                "information_richness": f"+{oracle_stats.get('workflow_metrics', {}).get('cards_revealed', 0)} cards revealed"
+            }
+        }
+    
+    def _calculate_agent_balance(self, agent_interactions: Dict[str, Any]) -> Dict[str, float]:
+        """Calcule l'équilibre de participation entre agents."""
+        total_turns = agent_interactions.get("total_turns", 0)
+        if total_turns == 0:
+            return {"sherlock": 0.0, "watson": 0.0, "moriarty": 0.0}
+        
+        # Estimation basée sur le pattern cyclique (1/3 chacun idéalement)
+        expected_per_agent = total_turns / 3
+        
+        return {
+            "expected_turns_per_agent": expected_per_agent,
+            "balance_score": 1.0,  # À améliorer avec tracking réel par agent
+            "note": "Équilibre cyclique théorique - à améliorer avec métriques réelles"
+        }
+    
+    # PHASE C: Méthodes d'analyse contextuelle pour fluidité
+    
+    def _detect_message_type(self, content: str) -> str:
+        """
+        Détecte le type de message basé sur son contenu.
+        
+        Args:
+            content: Contenu du message
+            
+        Returns:
+            Type de message détecté
+        """
+        content_lower = content.lower()
+        
+        if any(keyword in content_lower for keyword in ["révèle", "possède", "carte", "j'ai"]):
+            return "revelation"
+        elif any(keyword in content_lower for keyword in ["suggère", "propose", "suspect", "arme", "lieu"]):
+            return "suggestion"
+        elif any(keyword in content_lower for keyword in ["analyse", "déduction", "conclusion", "donc"]):
+            return "analysis"
+        elif any(keyword in content_lower for keyword in ["brillant", "exactement", "aha", "intéressant", "magistral"]):
+            return "reaction"
+        else:
+            return "message"
+    
+    def _analyze_contextual_elements(self, agent_name: str, content: str, history: List) -> None:
+        """
+        Analyse les éléments contextuels d'un message et enregistre les références/réactions.
+        
+        Args:
+            agent_name: Nom de l'agent qui parle
+            content: Contenu du message
+            history: Historique des messages
+        """
+        content_lower = content.lower()
+        
+        # Détection des références contextuelles explicites
+        reference_indicators = [
+            ("suite à", "building_on"),
+            ("en réaction à", "reacting_to"),
+            ("après cette", "responding_to"),
+            ("comme dit", "referencing"),
+            ("précédemment", "referencing")
+        ]
+        
+        for indicator, ref_type in reference_indicators:
+            if indicator in content_lower:
+                # Trouve le message précédent le plus proche
+                if len(history) > 1:
+                    target_turn = len(history) - 1  # Message précédent
+                    self.oracle_state.record_contextual_reference(
+                        source_agent=agent_name,
+                        target_message_turn=target_turn,
+                        reference_type=ref_type,
+                        reference_content=indicator
+                    )
+                break
+        
+        # Détection des réactions émotionnelles
+        emotional_patterns = self._detect_emotional_reactions(agent_name, content, history)
+        for reaction in emotional_patterns:
+            self.oracle_state.record_emotional_reaction(**reaction)
+    
+    def _detect_emotional_reactions(self, agent_name: str, content: str, history: List) -> List[Dict[str, str]]:
+        """
+        Détecte les réactions émotionnelles spécifiques à chaque agent.
+        
+        Args:
+            agent_name: Nom de l'agent
+            content: Contenu du message
+            history: Historique des messages
+            
+        Returns:
+            Liste des réactions détectées
+        """
+        reactions = []
+        content_lower = content.lower()
+        
+        # Pour l'instant, retourne une liste vide - à implémenter si nécessaire
+        return reactions
+        
+# CORRECTIF ORACLE: Méthodes pour détection et révélation automatique
+    
+    def _extract_cluedo_suggestion(self, message_content: str) -> Optional[Dict[str, str]]:
+        """
+        Extrait une suggestion Cluedo d'un message (suspect, arme, lieu).
+        
+        Args:
+            message_content: Contenu du message à analyser
+            
+        Returns:
+            Dict avec suspect/arme/lieu ou None si pas de suggestion détectée
+        """
+        content_lower = message_content.lower()
+        
+        # Mots-clés indiquant une suggestion
+        suggestion_keywords = ['suggère', 'propose', 'accuse', 'pense que', 'suspect', 'suppose']
+        if not any(keyword in content_lower for keyword in suggestion_keywords):
+            return None
+        
+        # Listes des éléments Cluedo (en minuscules pour matching)
+        suspects = ["colonel moutarde", "professeur violet", "mademoiselle rose", "docteur orchidée"]
+        armes = ["poignard", "chandelier", "revolver", "corde"]
+        lieux = ["salon", "cuisine", "bureau", "bibliothèque"]
+        
+        # Recherche d'éléments dans le message
+        found_suspect = None
+        found_arme = None
+        found_lieu = None
+        
+        for suspect in suspects:
+            if suspect in content_lower:
+                found_suspect = suspect.title()
+                break
+        
+        for arme in armes:
+            if arme in content_lower:
+                found_arme = arme.title()
+                break
+        
+        for lieu in lieux:
+            if lieu in content_lower:
+                found_lieu = lieu.title()
+                break
+        
+        # Suggestion valide seulement si au moins 2 éléments trouvés
+        if sum(x is not None for x in [found_suspect, found_arme, found_lieu]) >= 2:
+            return {
+                "suspect": found_suspect or "Indéterminé",
+                "arme": found_arme or "Indéterminée", 
+                "lieu": found_lieu or "Indéterminé"
+            }
+        
+        return None
+    
+    async def _force_moriarty_oracle_revelation(self, suggestion: Dict[str, str], suggesting_agent: str) -> Optional[Dict[str, Any]]:
+        """
+        Force Moriarty à révéler ses cartes pour une suggestion donnée.
+        
+        Args:
+            suggestion: Dict avec suspect/arme/lieu
+            suggesting_agent: Nom de l'agent qui fait la suggestion
+            
+        Returns:
+            Réponse Oracle de Moriarty ou None si erreur
+        """
+        try:
+            self._logger.info(f"🔮 Force Oracle révélation: {suggestion} par {suggesting_agent}")
+            
+            # Appel direct à Moriarty pour validation Oracle
+            oracle_result = self.moriarty_agent.validate_suggestion_cluedo(
+                suspect=suggestion.get('suspect', ''),
+                arme=suggestion.get('arme', ''),
+                lieu=suggestion.get('lieu', ''),
+                suggesting_agent=suggesting_agent
+            )
+            
+            # Construction de la réponse théâtrale selon le résultat
+            if oracle_result.authorized and oracle_result.data and oracle_result.data.can_refute:
+                # Moriarty peut réfuter - révèle ses cartes
+                revealed_cards = oracle_result.revealed_information or []
+                
+                moriarty_responses = [
+                    f"*sourire énigmatique* Ah, {suggesting_agent}... Je possède {', '.join(revealed_cards)} ! Votre théorie s'effondre.",
+                    f"*regard perçant* Hélas... {', '.join(revealed_cards)} repose dans ma main. Réfléchissez encore.",
+                    f"Tiens, tiens... {', '.join(revealed_cards)} me permet de contrarier vos plans, {suggesting_agent}.",
+                    f"*applaudit* Magnifique tentative ! Mais j'ai {', '.join(revealed_cards)}. Continuez à chercher."
+                ]
+                
+                content = moriarty_responses[len(revealed_cards) % len(moriarty_responses)]
+                
+                return {
+                    "content": content,
+                    "type": "oracle_revelation",
+                    "revealed_cards": revealed_cards,
+                    "can_refute": True,
+                    "suggestion": suggestion
+                }
+            else:
+                # Moriarty ne peut pas réfuter - suggestion potentiellement correcte
+                warning_responses = [
+                    f"*silence inquiétant* Intéressant, {suggesting_agent}... Je ne peux rien révéler sur cette suggestion.",
+                    f"*sourire mystérieux* Voilà qui est... troublant. Aucune carte à révéler, {suggesting_agent}.",
+                    f"*regard intense* Cette combinaison me laisse sans réponse... Serait-ce la vérité ?",
+                    f"Ah... *pause dramatique* Vous touchez peut-être au but, {suggesting_agent}."
+                ]
+                
+                content = warning_responses[0]  # Première réponse par défaut
+                
+                return {
+                    "content": content,
+                    "type": "oracle_no_refutation",
+                    "revealed_cards": [],
+                    "can_refute": False,
+                    "suggestion": suggestion,
+                    "warning": "Suggestion potentiellement correcte"
+                }
+                
+        except Exception as e:
+            self._logger.error(f"❌ Erreur Oracle révélation: {e}", exc_info=True)
+            
+            # Réponse d'erreur théâtrale
+            error_content = f"*confusion momentanée* Pardonnez-moi, {suggesting_agent}... Un mystère technique m'empêche de répondre."
+            
+            return {
+                "content": error_content,
+                "type": "oracle_error",
+                "revealed_cards": [],
+                "can_refute": False,
+                "error": str(e)
+            }
+        # Trouver l'agent et le contenu qui ont déclenché la réaction
+        trigger_agent = None
+        trigger_content = ""
+        
+        if len(history) > 1:
+            last_message = history[-2]  # Message précédent (avant le message actuel)
+            trigger_agent = last_message.name
+            trigger_content = str(last_message.content)
+        
+        if not trigger_agent or trigger_agent == "System":
+            return reactions
+        
+        # Patterns de réaction spécifiques par agent
+        if agent_name == "Watson":
+            watson_reactions = [
+                (["brillant", "exactement", "ça colle parfaitement"], "approval"),
+                (["aha", "intéressant retournement", "ça change la donne"], "surprise"),
+                (["précisément", "logique", "cohérent"], "analysis")
+            ]
+            
+            for keywords, reaction_type in watson_reactions:
+                if any(keyword in content_lower for keyword in keywords):
+                    reactions.append({
+                        "agent_name": agent_name,
+                        "trigger_agent": trigger_agent,
+                        "trigger_content": trigger_content,
+                        "reaction_type": reaction_type,
+                        "reaction_content": content[:100]
+                    })
+                    break
+        
+        elif agent_name == "Sherlock":
+            sherlock_reactions = [
+                (["précisément watson", "tu vises juste", "c'est noté"], "approval"),
+                (["comme prévu", "merci pour cette clarification", "parfait"], "satisfaction"),
+                (["intéressant", "fascinant", "remarquable"], "analysis")
+            ]
+            
+            for keywords, reaction_type in sherlock_reactions:
+                if any(keyword in content_lower for keyword in keywords):
+                    reactions.append({
+                        "agent_name": agent_name,
+                        "trigger_agent": trigger_agent,
+                        "trigger_content": trigger_content,
+                        "reaction_type": reaction_type,
+                        "reaction_content": content[:100]
+                    })
+                    break
+        
+        elif agent_name == "Moriarty":
+            moriarty_reactions = [
+                (["chaud", "très chaud", "vous brûlez"], "encouragement"),
+                (["pas tout à fait", "pas si vite"], "correction"),
+                (["magistral", "vous m'impressionnez", "bien joué"], "excitement"),
+                (["hmm", "attendez"], "suspense")
+            ]
+            
+            for keywords, reaction_type in moriarty_reactions:
+                if any(keyword in content_lower for keyword in keywords):
+                    reactions.append({
+                        "agent_name": agent_name,
+                        "trigger_agent": trigger_agent,
+                        "trigger_content": trigger_content,
+                        "reaction_type": reaction_type,
+                        "reaction_content": content[:100]
+                    })
+                    break
+        
+        return reactions
+
+
+async def run_cluedo_oracle_game(
+    kernel: Kernel,
+    initial_question: str = "L'enquête commence. Sherlock, menez l'investigation !",
+    max_turns: int = 15,
+    max_cycles: int = 5,
+    oracle_strategy: str = "balanced"
+) -> Dict[str, Any]:
+    """
+    Interface simplifiée pour exécuter une partie Cluedo avec Oracle.
+    
+    Args:
+        kernel: Kernel Semantic Kernel configuré
+        initial_question: Question initiale
+        max_turns: Nombre maximum de tours
+        max_cycles: Nombre maximum de cycles
+        oracle_strategy: Stratégie Oracle
+        
+    Returns:
+        Résultat complet du workflow
+    """
+    orchestrator = CluedoExtendedOrchestrator(
+        kernel=kernel,
+        max_turns=max_turns,
+        max_cycles=max_cycles,
+        oracle_strategy=oracle_strategy
+    )
+    
+    await orchestrator.setup_workflow()
+    return await orchestrator.execute_workflow(initial_question)
+
+
+async def main():
+    """Point d'entrée pour exécuter le workflow 3-agents de manière autonome."""
+    kernel = Kernel()
+    # NOTE: Configurez ici votre service LLM
+    
+    try:
+        result = await run_cluedo_oracle_game(
+            kernel=kernel,
+            oracle_strategy="balanced",
+            max_cycles=5
+        )
+        
+        print("\n" + "="*60)
+        print("RÉSULTAT WORKFLOW 3-AGENTS CLUEDO ORACLE")
+        print("="*60)
+        
+        print(f"\n🎯 SUCCÈS: {result['solution_analysis']['success']}")
+        print(f"📊 TOURS: {result['oracle_statistics']['agent_interactions']['total_turns']}")
+        print(f"🔮 REQUÊTES ORACLE: {result['oracle_statistics']['workflow_metrics']['oracle_interactions']}")
+        print(f"💎 CARTES RÉVÉLÉES: {result['oracle_statistics']['workflow_metrics']['cards_revealed']}")
+        print(f"⏱️  TEMPS: {result['workflow_info']['execution_time_seconds']:.2f}s")
+        
+        if result['solution_analysis']['success']:
+            print(f"[OK] Solution: {result['final_state']['final_solution']}")
+        else:
+            print(f"❌ Solution proposée: {result['final_state']['final_solution']}")
+            print(f"🎯 Solution correcte: {result['final_state']['correct_solution']}")
+        
+        print("\n" + "="*60)
+        
+    except Exception as e:
+        print(f"❌ Erreur durant l'exécution: {e}")
+        import traceback
+        traceback.print_exc()
+
+
+if __name__ == "__main__":
+    asyncio.run(main())
diff --git a/argumentation_analysis/orchestration/enhanced_pm_analysis_runner.py b/argumentation_analysis/orchestration/enhanced_pm_analysis_runner.py
index d2c5ca9b..76932110 100644
--- a/argumentation_analysis/orchestration/enhanced_pm_analysis_runner.py
+++ b/argumentation_analysis/orchestration/enhanced_pm_analysis_runner.py
@@ -1,4 +1,4 @@
-#!/usr/bin/env python
+﻿#!/usr/bin/env python
 # -*- coding: utf-8 -*-
 
 """
@@ -34,10 +34,9 @@ if project_root not in sys.path:
 
 # Imports Semantic Kernel
 import semantic_kernel as sk
-from semantic_kernel.contents import ChatMessageContent
-from semantic_kernel.contents import AuthorRole, ChatMessageContent
-from semantic_kernel.agents import AgentGroupChat, ChatCompletionAgent, Agent, AgentChatException
-from semantic_kernel.functions import FunctionChoiceBehavior
+from semantic_kernel.contents import ChatMessageContent, ChatRole as AuthorRole
+from semantic_kernel_compatibility import AgentGroupChat, ChatCompletionAgent, Agent
+from semantic_kernel_compatibility import AgentChatException
 from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion, AzureChatCompletion
 from semantic_kernel.functions.kernel_arguments import KernelArguments
 
@@ -648,4 +647,6 @@ class EnhancedPMAnalysisRunner:
             from argumentation_analysis.core.llm_service import create_llm_service
             llm_service = create_llm_service()
         
-        return await run_enhanced_pm_orchestration_demo(text_content, llm_service)
\ No newline at end of file
+        return await run_enhanced_pm_orchestration_demo(text_content, llm_service)
+
+
diff --git a/argumentation_analysis/orchestration/hierarchical/operational/adapters/informal_agent_adapter.py b/argumentation_analysis/orchestration/hierarchical/operational/adapters/informal_agent_adapter.py
index ceeba930..899215b7 100644
--- a/argumentation_analysis/orchestration/hierarchical/operational/adapters/informal_agent_adapter.py
+++ b/argumentation_analysis/orchestration/hierarchical/operational/adapters/informal_agent_adapter.py
@@ -1,4 +1,4 @@
-"""
+﻿"""
 Module d'adaptation de l'agent informel pour l'architecture hiérarchique.
 
 Ce module fournit un adaptateur qui permet à l'agent informel existant
@@ -15,7 +15,7 @@ from typing import Dict, List, Any, Optional, Union
 from datetime import datetime
 
 import semantic_kernel as sk # Kept for type hints if necessary, but direct use might be reduced
-# from semantic_kernel.contents import ChatMessageContent, AuthorRole # Potentially unused if agent handles chat history
+# from semantic_kernel.contents import ChatMessageContent, ChatRole as AuthorRole # Potentially unused if agent handles chat history
 # from semantic_kernel.functions.kernel_arguments import KernelArguments # Potentially unused
 
 from argumentation_analysis.orchestration.hierarchical.operational.agent_interface import OperationalAgent
@@ -480,4 +480,4 @@ class InformalAgentAdapter(OperationalAgent):
             return result
         except Exception as e:
             self.logger.error(f"Erreur lors de l'obtention des détails du sophisme: {e}")
-            return {"error": str(e)}
\ No newline at end of file
+            return {"error": str(e)}
diff --git a/argumentation_analysis/orchestration/hierarchical/operational/adapters/pl_agent_adapter.py b/argumentation_analysis/orchestration/hierarchical/operational/adapters/pl_agent_adapter.py
index 55d6a41f..1c45ab4e 100644
--- a/argumentation_analysis/orchestration/hierarchical/operational/adapters/pl_agent_adapter.py
+++ b/argumentation_analysis/orchestration/hierarchical/operational/adapters/pl_agent_adapter.py
@@ -1,4 +1,4 @@
-"""
+﻿"""
 Module d'adaptation de l'agent de logique propositionnelle pour l'architecture hiérarchique.
 
 Ce module fournit un adaptateur qui permet à l'agent de logique propositionnelle existant
@@ -15,7 +15,7 @@ from typing import Dict, List, Any, Optional, Union
 from datetime import datetime
 
 import semantic_kernel as sk # Kept for type hints if necessary
-# from semantic_kernel.contents import ChatMessageContent, AuthorRole # Potentially unused
+# from semantic_kernel.contents import ChatMessageContent, ChatRole as AuthorRole # Potentially unused
 # from semantic_kernel.functions.kernel_arguments import KernelArguments # Potentially unused
 
 from argumentation_analysis.orchestration.hierarchical.operational.agent_interface import OperationalAgent
@@ -380,4 +380,4 @@ class PLAgentAdapter(OperationalAgent):
     # Les méthodes _text_to_belief_set, _generate_and_execute_queries, _generate_queries,
     # _execute_query, _interpret_results, _check_consistency sont supprimées
     # car leurs fonctionnalités sont maintenant dans self.agent.
-    pass # Placeholder if no other methods are defined after this.
\ No newline at end of file
+    pass # Placeholder if no other methods are defined after this.
diff --git a/argumentation_analysis/scripts/simulate_balanced_participation.py b/argumentation_analysis/scripts/simulate_balanced_participation.py
index 3e9b8558..b0082ead 100644
--- a/argumentation_analysis/scripts/simulate_balanced_participation.py
+++ b/argumentation_analysis/scripts/simulate_balanced_participation.py
@@ -1,4 +1,4 @@
-#!/usr/bin/env python3
+﻿#!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 """
 Script de simulation pour démontrer l'équilibrage de la participation des agents
@@ -11,9 +11,10 @@ import matplotlib.pyplot as plt
 import numpy as np
 from typing import Dict, List, Tuple
 # CORRECTIF COMPATIBILITÉ: Utilisation du module de compatibilité
-from semantic_kernel.agents import Agent
-from semantic_kernel.contents import ChatMessageContent
-from semantic_kernel.contents import AuthorRole
+from argumentation_analysis.utils.semantic_kernel_compatibility import Agent
+from semantic_kernel.contents import ChatMessageContent, ChatRole as AuthorRole
+from unittest.mock import MagicMock
+
 # Import des modules du projet
 import sys
 import os
@@ -325,4 +326,4 @@ if __name__ == "__main__":
     asyncio.run(run_standard_simulation())
     
     # Pour exécuter la simulation comparative, décommenter:
-    # asyncio.run(run_comparison_simulation())
\ No newline at end of file
+    # asyncio.run(run_comparison_simulation())
diff --git a/argumentation_analysis/utils/extract_repair/verify_extracts_with_llm.py b/argumentation_analysis/utils/extract_repair/verify_extracts_with_llm.py
index 695308ff..3d697479 100644
--- a/argumentation_analysis/utils/extract_repair/verify_extracts_with_llm.py
+++ b/argumentation_analysis/utils/extract_repair/verify_extracts_with_llm.py
@@ -1,4 +1,4 @@
-#!/usr/bin/env python
+﻿#!/usr/bin/env python
 # -*- coding: utf-8 -*-
 
 """
@@ -33,8 +33,7 @@ file_handler.setFormatter(logging.Formatter('%(asctime)s [%(levelname)s] [%(name
 logger.addHandler(file_handler)
 
 import semantic_kernel as sk
-from semantic_kernel.contents import ChatMessageContent
-from semantic_kernel.contents import AuthorRole
+from semantic_kernel.contents import ChatMessageContent, ChatRole as AuthorRole
 # CORRECTIF COMPATIBILITÉ: Utilisation du module de compatibilité
 from semantic_kernel.agents import ChatCompletionAgent
 try:
@@ -561,4 +560,4 @@ def generate_report(results: List[Dict[str, Any]], output_file: str = "verify_ex
     
     logger.info(f"Rapport généré dans '{output_file}'.")
 # La fonction main() et la section if __name__ == "__main__": ont été déplacées
-# vers argumentation_analysis/scripts/run_verify_extracts_llm.py
\ No newline at end of file
+# vers argumentation_analysis/scripts/run_verify_extracts_llm.py
diff --git a/examples/logique_complexe_demo/demo_einstein_workflow.py b/examples/logique_complexe_demo/demo_einstein_workflow.py
index 1ff7d7ca..777923ef 100644
--- a/examples/logique_complexe_demo/demo_einstein_workflow.py
+++ b/examples/logique_complexe_demo/demo_einstein_workflow.py
@@ -1,4 +1,4 @@
-#!/usr/bin/env python3
+﻿#!/usr/bin/env python3
 # examples/logique_complexe_demo/demo_einstein_workflow.py
 
 """
@@ -22,7 +22,7 @@ sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
 
 from semantic_kernel import Kernel
 from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion
-from semantic_kernel.agents import ChatCompletionAgent
+from semantic_kernel_compatibility import ChatCompletionAgent
 
 # Import des composants spécialisés pour l'énigme complexe
 from argumentation_analysis.orchestration.logique_complexe_orchestrator import LogiqueComplexeOrchestrator
@@ -164,4 +164,4 @@ if __name__ == "__main__":
         sys.exit(130)
     except Exception as e:
         print(f"\n\n❌ Erreur fatale: {e}")
-        sys.exit(1)
\ No newline at end of file
+        sys.exit(1)
diff --git a/examples/logique_complexe_demo/test_einstein_simple.py b/examples/logique_complexe_demo/test_einstein_simple.py
index 76508d8c..f19e148d 100644
--- a/examples/logique_complexe_demo/test_einstein_simple.py
+++ b/examples/logique_complexe_demo/test_einstein_simple.py
@@ -1,4 +1,4 @@
-#!/usr/bin/env python3
+﻿#!/usr/bin/env python3
 # examples/logique_complexe_demo/test_einstein_simple.py
 
 """
@@ -16,7 +16,7 @@ sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
 
 from semantic_kernel import Kernel
 from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion
-from semantic_kernel.agents import ChatCompletionAgent
+from semantic_kernel_compatibility import ChatCompletionAgent
 
 # Import des composants spécialisés pour l'énigme complexe
 from argumentation_analysis.orchestration.logique_complexe_orchestrator import LogiqueComplexeOrchestrator
@@ -174,4 +174,4 @@ if __name__ == "__main__":
         sys.exit(130)
     except Exception as e:
         print(f"[FATAL] Erreur fatale: {e}")
-        sys.exit(1)
\ No newline at end of file
+        sys.exit(1)
diff --git a/scripts/diagnostic/test_critical_dependencies.py b/scripts/diagnostic/test_critical_dependencies.py
index b4d05902..00742292 100644
--- a/scripts/diagnostic/test_critical_dependencies.py
+++ b/scripts/diagnostic/test_critical_dependencies.py
@@ -1,4 +1,3 @@
-import project_core.core_from_scripts.auto_env
 #!/usr/bin/env python3
 """
 Script de test pour vérifier les dépendances critiques et les imports AuthorRole
@@ -21,7 +20,7 @@ def test_semantic_kernel_version():
 def test_author_role_import():
     """Test de l'import AuthorRole"""
     try:
-        from semantic_kernel.agents import AuthorRole
+        from semantic_kernel_compatibility import AuthorRole
         print("[OK] AuthorRole import reussi")
         print(f"[OK] AuthorRole disponible: {AuthorRole}")
         return True
@@ -105,4 +104,4 @@ def main():
 
 if __name__ == "__main__":
     success = main()
-    sys.exit(0 if success else 1)
\ No newline at end of file
+    sys.exit(0 if success else 1)
diff --git a/tests/argumentation_analysis/utils/dev_tools/test_repair_utils.py b/tests/argumentation_analysis/utils/dev_tools/test_repair_utils.py
index 6a772af8..44e3cebd 100644
--- a/tests/argumentation_analysis/utils/dev_tools/test_repair_utils.py
+++ b/tests/argumentation_analysis/utils/dev_tools/test_repair_utils.py
@@ -1,9 +1,3 @@
-# Authentic gpt-4o-mini imports (replacing mocks)
-import openai
-from semantic_kernel.contents import ChatHistory
-from semantic_kernel.core_plugins import ConversationSummaryPlugin
-from config.unified_config import UnifiedConfig
-
 # -*- coding: utf-8 -*-
 """Tests pour les utilitaires de réparation d'extraits."""
 
@@ -18,7 +12,7 @@ from typing import Dict # Ajout pour le typage
 from argumentation_analysis.utils.dev_tools.repair_utils import run_extract_repair_pipeline, setup_agents, repair_extract_markers
 from argumentation_analysis.models.extract_definition import ExtractDefinitions, SourceDefinition, Extract # Pour typer les mocks
 import semantic_kernel as sk # Pour setup_agents
-# from semantic_kernel.agents import ChatCompletionAgent # Pour setup_agents (Commenté: non utilisé directement)
+# from semantic_kernel_compatibility import ChatCompletionAgent # Pour setup_agents (Commenté: non utilisé directement)
 from argumentation_analysis.utils.extract_repair.marker_repair_logic import REPAIR_AGENT_INSTRUCTIONS, VALIDATION_AGENT_INSTRUCTIONS # Pour setup_agents
 
 
@@ -406,4 +400,4 @@ async def test_repair_extract_markers_empty_definitions(
         source_with_no_extracts, mock_llm_service, mock_fetch_service, mock_extract_service
     )
     assert updated_defs_no_ext == source_with_no_extracts
-    assert results_no_ext == []
\ No newline at end of file
+    assert results_no_ext == []
diff --git a/tests/conftest.py b/tests/conftest.py
index 9b3e07d9..7044cb3a 100644
--- a/tests/conftest.py
+++ b/tests/conftest.py
@@ -1,12 +1,3 @@
-import sys
-import os
-from pathlib import Path
-
-# Ajoute la racine du projet au sys.path pour résoudre les problèmes d'import
-# causés par le `rootdir` de pytest qui interfère avec la résolution des modules.
-project_root = Path(__file__).parent.parent.resolve()
-if str(project_root) not in sys.path:
-    sys.path.insert(0, str(project_root))
 """
 Configuration pour les tests pytest.
 
@@ -15,7 +6,6 @@ Il configure les mocks nécessaires pour les tests et utilise les vraies bibliot
 lorsqu'elles sont disponibles. Pour Python 3.12 et supérieur, le mock JPype1 est
 automatiquement utilisé en raison de problèmes de compatibilité.
 """
-import project_core.core_from_scripts.auto_env
 import sys
 import os
 import pytest
@@ -39,7 +29,6 @@ if not logging.getLogger().handlers: # Si le root logger n'a pas de handlers, ba
 else:
     _conftest_setup_logger.info("Configuration globale du logging déjà présente ou appliquée par un autre module.")
 # --- Début Patching JPype Mock au niveau module si nécessaire ---
-os.environ['USE_REAL_JPYPE'] = 'false'
 _SHOULD_USE_REAL_JPYPE = os.environ.get('USE_REAL_JPYPE', 'false').lower() in ('true', '1')
 _conftest_setup_logger.info(f"conftest.py: USE_REAL_JPYPE={os.environ.get('USE_REAL_JPYPE', 'false')}, _SHOULD_USE_REAL_JPYPE={_SHOULD_USE_REAL_JPYPE}")
 
@@ -187,35 +176,12 @@ logger = logging.getLogger(__name__)
 # Ceci est particulièrement utile si les tests sont exécutés d'une manière où le répertoire racine
 # n'est pas automatiquement inclus dans PYTHONPATH (par exemple, exécution directe de pytest
 # depuis un sous-répertoire ou avec certaines configurations d'IDE).
-parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
-if parent_dir not in sys.path:
-    sys.path.insert(0, parent_dir)
-    _conftest_setup_logger.info(f"Ajout du répertoire racine du projet ({parent_dir}) à sys.path.")
-# Décommenté car l'environnement de test actuel en a besoin pour trouver les modules locaux.
+# parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
+# if parent_dir not in sys.path:
+#     sys.path.insert(0, parent_dir)
+#     _conftest_setup_logger.info(f"Ajout du répertoire racine du projet ({parent_dir}) à sys.path.")
+# Commenté car l'installation du package via `pip install -e .` devrait gérer l'accessibilité.
 
 # Les fixtures et hooks sont importés depuis leurs modules dédiés.
 # Les commentaires résiduels concernant les déplacements de code et les refactorisations
 # antérieures ont été supprimés pour améliorer la lisibilité.
-
-# --- Fixtures déplacées depuis tests/integration/webapp/conftest.py ---
-
-@pytest.fixture
-def webapp_config():
-    """Provides a basic webapp configuration dictionary."""
-    return {
-        "backend": {
-            "start_port": 8008,
-            "fallback_ports": [8009, 8010]
-        },
-        "frontend": {
-            "port": 3008
-        },
-        "playwright": {
-            "enabled": True
-        }
-    }
-
-@pytest.fixture
-def test_config_path(tmp_path):
-    """Provides a temporary path for a config file."""
-    return tmp_path / "test_config.yml"
diff --git a/tests/e2e/demos/test_react_webapp_full.py b/tests/e2e/demos/test_react_webapp_full.py
index 9d8195ad..00b11670 100644
--- a/tests/e2e/demos/test_react_webapp_full.py
+++ b/tests/e2e/demos/test_react_webapp_full.py
@@ -14,7 +14,7 @@ from playwright.sync_api import Page, expect, sync_playwright
 # Configuration
 REACT_APP_PATH = Path(__file__).parent.parent.parent / "services/web_api/interface-web-argumentative"
 REACT_APP_URL = "http://localhost:3000"
-BACKEND_URL = "http://localhost:5000"
+BACKEND_URL = "http://localhost:5003"
 
 class ReactServerManager:
     """Gestionnaire du serveur React pour les tests"""
@@ -68,9 +68,9 @@ class TestReactWebAppFull:
         try:
             page.goto(REACT_APP_URL, timeout=30000)
             expect(page.locator("body")).to_be_visible()
-            print("✅ Application React chargée")
+            print("[OK] Application React chargée")
         except Exception as e:
-            print(f"⚠️  Application React non accessible: {e}")
+            print(f"[WARNING]  Application React non accessible: {e}")
             # Test de fallback avec l'interface statique
             self.test_static_fallback(page)
     
@@ -82,7 +82,7 @@ class TestReactWebAppFull:
         page.goto(demo_url)
         expect(page).to_have_title("Interface d'Analyse Argumentative - Test")
         expect(page.locator("h1")).to_contain_text("Interface d'Analyse Argumentative")
-        print("✅ Interface statique de fallback chargée")
+        print("[OK] Interface statique de fallback chargée")
     
     def test_navigation_tabs(self, page: Page, server_manager):
         """Test de navigation entre les onglets"""
@@ -105,12 +105,12 @@ class TestReactWebAppFull:
                     if tab.is_visible():
                         tab.click()
                         time.sleep(0.5)
-                        print(f"✅ Onglet {tab_selector} accessible")
+                        print(f"[OK] Onglet {tab_selector} accessible")
                 except:
-                    print(f"⚠️  Onglet {tab_selector} non trouvé")
+                    print(f"[WARNING]  Onglet {tab_selector} non trouvé")
                     
         except Exception as e:
-            print(f"⚠️  Navigation non testable: {e}")
+            print(f"[WARNING]  Navigation non testable: {e}")
     
     def test_api_connectivity(self, page: Page, server_manager):
         """Test de la connectivité API"""
@@ -121,12 +121,12 @@ class TestReactWebAppFull:
             api_status = page.locator('.api-status')
             if api_status.is_visible():
                 expect(api_status).to_be_visible()
-                print("✅ Statut API affiché")
+                print("[OK] Statut API affiché")
             else:
-                print("⚠️  Statut API non trouvé")
+                print("[WARNING]  Statut API non trouvé")
                 
         except Exception as e:
-            print(f"⚠️  Test API non réalisable: {e}")
+            print(f"[WARNING]  Test API non réalisable: {e}")
     
     def test_form_interactions(self, page: Page, server_manager):
         """Test des interactions de formulaire"""
@@ -147,13 +147,13 @@ class TestReactWebAppFull:
                     if text_input.is_visible():
                         text_input.fill("Test de saisie")
                         expect(text_input).to_have_value("Test de saisie")
-                        print(f"✅ Champ {input_selector} fonctionnel")
+                        print(f"[OK] Champ {input_selector} fonctionnel")
                         break
                 except:
                     continue
                     
         except Exception as e:
-            print(f"⚠️  Interactions formulaire non testables: {e}")
+            print(f"[WARNING]  Interactions formulaire non testables: {e}")
 
 def test_standalone_static_interface():
     """Test autonome de l'interface statique"""
@@ -188,11 +188,11 @@ def test_standalone_static_interface():
             
             browser.close()
             
-            print("✅ Interface statique complètement fonctionnelle")
+            print("[OK] Interface statique complètement fonctionnelle")
             return True
             
     except Exception as e:
-        print(f"❌ Erreur test interface statique: {e}")
+        print(f"[FAIL] Erreur test interface statique: {e}")
         return False
 
 def main():
@@ -230,22 +230,22 @@ def main():
                 
                 server_manager.stop_server()
                 
-                print("✅ Tests React terminés")
+                print("[OK] Tests React terminés")
             else:
-                print("⚠️  Application React non trouvée, tests React ignorés")
+                print("[WARNING]  Application React non trouvée, tests React ignorés")
             
             browser.close()
             
     except Exception as e:
-        print(f"⚠️  Tests React échoués: {e}")
+        print(f"[WARNING]  Tests React échoués: {e}")
         print("Interface statique reste disponible comme fallback")
     
     print("\n" + "=" * 60)
     if static_ok:
-        print("✅ SYSTÈME PLAYWRIGHT FONCTIONNEL")
-        print("✅ INTERFACE WEB DE DÉMONSTRATION VALIDÉE")
+        print("[OK] SYSTÈME PLAYWRIGHT FONCTIONNEL")
+        print("[OK] INTERFACE WEB DE DÉMONSTRATION VALIDÉE")
     else:
-        print("❌ SYSTÈME PLAYWRIGHT NON FONCTIONNEL")
+        print("[FAIL] SYSTÈME PLAYWRIGHT NON FONCTIONNEL")
     print("=" * 60)
     
     return 0 if static_ok else 1
diff --git a/tests/unit/argumentation_analysis/test_agent_interaction.py b/tests/unit/argumentation_analysis/test_agent_interaction.py
index 414a0d32..0fafcbb9 100644
--- a/tests/unit/argumentation_analysis/test_agent_interaction.py
+++ b/tests/unit/argumentation_analysis/test_agent_interaction.py
@@ -1,4 +1,4 @@
-# -*- coding: utf-8 -*-
+﻿# -*- coding: utf-8 -*-
 """
 Tests d'intégration pour l'interaction entre les différents agents.
 
@@ -11,20 +11,8 @@ import asyncio
 import pytest
 from unittest.mock import MagicMock, AsyncMock, patch
 import semantic_kernel as sk
-# Import fixe pour AuthorRole - créer un mock simple si nécessaire
-try:
-    from semantic_kernel.contents import ChatMessageContent
-except ImportError:
-    ChatMessageContent = None
-
-# Import AuthorRole (maintenant ChatRole) depuis le module de compatibilité
-from semantic_kernel.contents.utils.author_role import AuthorRole
-
-# try:
-#     from semantic_kernel.agents import Agent, AgentGroupChat # Commenté car non disponible dans SK 0.9.6b1
-# except ImportError:
-#     Agent = None
-#     AgentGroupChat = None
+from semantic_kernel.contents import ChatMessageContent, ChatRole as AuthorRole
+from semantic_kernel_compatibility import Agent, AgentGroupChat
 
 # Utiliser la fonction setup_import_paths pour résoudre les problèmes d'imports relatifs
 # from tests import setup_import_paths # Commenté pour investigation
@@ -379,4 +367,5 @@ class TestAgentInteractionWithErrors: # Suppression de l'héritage AsyncTestCase
 if __name__ == '__main__':
     # Utiliser pytest pour exécuter les tests si ce fichier est exécuté directement
     # Cela permet de bénéficier des fixtures et des plugins pytest comme anyio.
-    pytest.main(['-xvs', __file__])
\ No newline at end of file
+    pytest.main(['-xvs', __file__])
+
diff --git a/tests/unit/argumentation_analysis/test_integration_balanced_strategy.py b/tests/unit/argumentation_analysis/test_integration_balanced_strategy.py
index 12a8943a..6f9bc087 100644
--- a/tests/unit/argumentation_analysis/test_integration_balanced_strategy.py
+++ b/tests/unit/argumentation_analysis/test_integration_balanced_strategy.py
@@ -1,10 +1,3 @@
-
-# Authentic gpt-4o-mini imports (replacing mocks)
-import openai
-from semantic_kernel.contents import ChatHistory
-from semantic_kernel.core_plugins import ConversationSummaryPlugin
-from config.unified_config import UnifiedConfig
-
 # -*- coding: utf-8 -*-
 """
 Tests d'intégration pour la stratégie d'équilibrage de participation des agents.
@@ -21,8 +14,8 @@ import pytest
 import logging
 
 import semantic_kernel as sk
-from semantic_kernel.contents import ChatMessageContent
-from argumentation_analysis.agents.core.abc.agent_bases import BaseAgent
+from semantic_kernel.contents import ChatMessageContent, ChatRole as AuthorRole
+from semantic_kernel_compatibility import Agent, AgentGroupChat
 
 # Utiliser la fonction setup_import_paths pour résoudre les problèmes d'imports relatifs
 # from tests import setup_import_paths # Commenté pour investigation
@@ -214,4 +207,5 @@ class TestBalancedStrategyIntegration: # Suppression de l'héritage AsyncTestCas
 
 
 if __name__ == '__main__':
-    pytest.main(['-xvs', __file__])
\ No newline at end of file
+    pytest.main(['-xvs', __file__])
+
diff --git a/tests/unit/argumentation_analysis/test_integration_end_to_end.py b/tests/unit/argumentation_analysis/test_integration_end_to_end.py
index 6ee0d153..cf0c8c4e 100644
--- a/tests/unit/argumentation_analysis/test_integration_end_to_end.py
+++ b/tests/unit/argumentation_analysis/test_integration_end_to_end.py
@@ -1,10 +1,3 @@
-
-# Authentic gpt-4o-mini imports (replacing mocks)
-import openai
-from semantic_kernel.contents import ChatHistory
-from semantic_kernel.core_plugins import ConversationSummaryPlugin
-from config.unified_config import UnifiedConfig
-
 # -*- coding: utf-8 -*-
 """
 Tests d'intégration end-to-end pour le système d'analyse argumentative.
@@ -22,8 +15,8 @@ import time
 from unittest.mock import MagicMock, AsyncMock, patch
 
 import semantic_kernel as sk
-from semantic_kernel.contents import ChatMessageContent
-# from semantic_kernel.agents import Agent, AgentGroupChat
+from semantic_kernel.contents import ChatMessageContent, ChatRole as AuthorRole
+from semantic_kernel_compatibility import Agent, AgentGroupChat
 
 from argumentation_analysis.core.shared_state import RhetoricalAnalysisState
 from argumentation_analysis.core.state_manager_plugin import StateManagerPlugin
@@ -125,4 +118,5 @@ class TestExtractIntegrationWithBalancedStrategy:
 
 
 if __name__ == '__main__':
-    pytest.main(['-xvs', __file__])
\ No newline at end of file
+    pytest.main(['-xvs', __file__])
+
diff --git a/tests/unit/argumentation_analysis/test_strategies.py b/tests/unit/argumentation_analysis/test_strategies.py
index 697e26c7..c048ff0d 100644
--- a/tests/unit/argumentation_analysis/test_strategies.py
+++ b/tests/unit/argumentation_analysis/test_strategies.py
@@ -1,4 +1,4 @@
-# -*- coding: utf-8 -*-
+﻿# -*- coding: utf-8 -*-
 """
 Tests unitaires pour le module strategies.
 """
@@ -6,9 +6,8 @@ Tests unitaires pour le module strategies.
 import unittest
 from unittest.mock import MagicMock, AsyncMock, patch
 import asyncio
-# from semantic_kernel.agents import Agent # Commenté car le module n'existe plus à cet emplacement
-from semantic_kernel.contents import ChatMessageContent
-from semantic_kernel.contents.utils.author_role import AuthorRole
+from semantic_kernel_compatibility import Agent
+from semantic_kernel.contents import ChatMessageContent, ChatRole as AuthorRole
 from argumentation_analysis.core.strategies import SimpleTerminationStrategy, DelegatingSelectionStrategy, BalancedParticipationStrategy
 from argumentation_analysis.core.shared_state import RhetoricalAnalysisState
 # from tests.async_test_case import AsyncTestCase # Suppression de l'import
@@ -390,4 +389,5 @@ class TestBalancedParticipationStrategy:
 
 
 if __name__ == '__main__':
-    unittest.main()
\ No newline at end of file
+    unittest.main()
+
diff --git a/tests/validation_sherlock_watson/test_analyse_simple.py b/tests/validation_sherlock_watson/test_analyse_simple.py
index 9f5db275..a195961d 100644
--- a/tests/validation_sherlock_watson/test_analyse_simple.py
+++ b/tests/validation_sherlock_watson/test_analyse_simple.py
@@ -9,6 +9,7 @@ import json
 import logging
 import sys
 from datetime import datetime
+import pytest
 
 # Imports de l'infrastructure Oracle
 # from argumentation_analysis.orchestration.cluedo_extended_orchestrator import run_cluedo_oracle_game
@@ -19,8 +20,7 @@ logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(
 logger = logging.getLogger(__name__)
 
 
-@pytest.mark.skip(reason="Legacy test for old orchestrator, disabling to fix collection.")
-@pytest.mark.anyio
+@pytest.mark.asyncio
 async def test_workflow_simple():
     """Test simple du workflow 3-agents."""
     logger.info("DEBUT - Test du workflow 3-agents")
diff --git a/tests/validation_sherlock_watson/test_cluedo_dataset_simple.py b/tests/validation_sherlock_watson/test_cluedo_dataset_simple.py
index ea9972a4..05659ab8 100644
--- a/tests/validation_sherlock_watson/test_cluedo_dataset_simple.py
+++ b/tests/validation_sherlock_watson/test_cluedo_dataset_simple.py
@@ -47,7 +47,7 @@ def test_revelation_record_creation():
     """Test de création d'enregistrement de révélation."""
     revelation = RevelationRecord(
         card_revealed="Professeur Violet",
-        revelation_type="owned_card",
+        revelation_type="card_inquiry",
         message="Test révélation",
         revealed_to="TestAgent",
         timestamp=datetime.now(),
diff --git a/tests/validation_sherlock_watson/test_group1_simple.py b/tests/validation_sherlock_watson/test_group1_simple.py
index 1cbb3e05..d7f4c220 100644
--- a/tests/validation_sherlock_watson/test_group1_simple.py
+++ b/tests/validation_sherlock_watson/test_group1_simple.py
@@ -1,11 +1,3 @@
-from unittest.mock import Mock, AsyncMock
-
-# Authentic gpt-4o-mini imports (replacing mocks)
-import openai
-from semantic_kernel.contents import ChatHistory
-from semantic_kernel.core_plugins import ConversationSummaryPlugin
-from config.unified_config import UnifiedConfig
-
 import pytest
 #!/usr/bin/env python3
 """Script de test simple pour valider les corrections du Groupe 1."""
@@ -27,7 +19,7 @@ except ImportError as e:
     print(f"ERREUR d'import: {e}")
     sys.exit(1)
 
-@pytest.mark.anyio
+@pytest.mark.asyncio
 async def test_group1_fixes():
     """Test des corrections du Groupe 1."""
     print("=== Test des corrections Groupe 1 - AsyncMock ===")
diff --git a/tests/validation_sherlock_watson/test_group2_corrections_simple.py b/tests/validation_sherlock_watson/test_group2_corrections_simple.py
index 0839cbea..a861d289 100644
--- a/tests/validation_sherlock_watson/test_group2_corrections_simple.py
+++ b/tests/validation_sherlock_watson/test_group2_corrections_simple.py
@@ -1,11 +1,3 @@
-from unittest.mock import Mock, AsyncMock
-
-# Authentic gpt-4o-mini imports (replacing mocks)
-import openai
-from semantic_kernel.contents import ChatHistory
-from semantic_kernel.core_plugins import ConversationSummaryPlugin
-from config.unified_config import UnifiedConfig
-
 import pytest
 #!/usr/bin/env python3
 """
@@ -31,7 +23,7 @@ from argumentation_analysis.agents.core.oracle.cluedo_dataset import CluedoDatas
 from semantic_kernel.kernel import Kernel
 
 
-@pytest.mark.anyio
+@pytest.mark.asyncio
 async def test_dataset_manager_check_permission():
     """Test que DatasetAccessManager a maintenant la methode check_permission."""
     print("Test 1: Verification de l'existence de check_permission sur DatasetAccessManager")
@@ -61,7 +53,7 @@ async def test_dataset_manager_check_permission():
     print("SUCCES Test 1: check_permission fonctionne correctement")
 
 
-@pytest.mark.anyio
+@pytest.mark.asyncio
 async def test_mock_permission_setup():
     """Test que les mocks peuvent etre configures correctement pour les tests."""
     print("Test 2: Configuration des mocks pour permission_manager")
@@ -88,7 +80,7 @@ async def test_mock_permission_setup():
     print("SUCCES Test 2: Les mocks sont correctement configures")
 
 
-@pytest.mark.anyio
+@pytest.mark.asyncio
 async def test_oracle_tools_integration():
     """Test l'integration avec OracleTools."""
     print("Test 3: Integration OracleTools avec check_agent_permission")
diff --git a/tests/validation_sherlock_watson/test_group3_final_validation.py b/tests/validation_sherlock_watson/test_group3_final_validation.py
index c1079e56..c3c6a13d 100644
--- a/tests/validation_sherlock_watson/test_group3_final_validation.py
+++ b/tests/validation_sherlock_watson/test_group3_final_validation.py
@@ -1,11 +1,3 @@
-from unittest.mock import Mock, AsyncMock
-
-# Authentic gpt-4o-mini imports (replacing mocks)
-import openai
-from semantic_kernel.contents import ChatHistory
-from semantic_kernel.core_plugins import ConversationSummaryPlugin
-from config.unified_config import UnifiedConfig
-
 import pytest
 #!/usr/bin/env python3
 """
@@ -25,7 +17,7 @@ from argumentation_analysis.agents.core.oracle.dataset_access_manager import Dat
 from argumentation_analysis.agents.core.oracle.permissions import QueryType, OracleResponse, PermissionManager
 from semantic_kernel.kernel import Kernel
 
-@pytest.mark.anyio
+@pytest.mark.asyncio
 async def test_all_group3_fixes():
     """Test complet des 4 corrections appliquées"""
     print("=== VALIDATION FINALE GROUPE 3 ===")
diff --git a/tests/validation_sherlock_watson/test_groupe2_validation_simple.py b/tests/validation_sherlock_watson/test_groupe2_validation_simple.py
index 7accf758..2f6af5bb 100644
--- a/tests/validation_sherlock_watson/test_groupe2_validation_simple.py
+++ b/tests/validation_sherlock_watson/test_groupe2_validation_simple.py
@@ -1,11 +1,3 @@
-from unittest.mock import Mock, AsyncMock
-
-# Authentic gpt-4o-mini imports (replacing mocks)
-import openai
-from semantic_kernel.contents import ChatHistory
-from semantic_kernel.core_plugins import ConversationSummaryPlugin
-from config.unified_config import UnifiedConfig
-
 import pytest
 #!/usr/bin/env python3
 """
@@ -28,7 +20,7 @@ from argumentation_analysis.agents.core.oracle.oracle_base_agent import OracleBa
 from semantic_kernel.kernel import Kernel
 
 
-@pytest.mark.anyio
+@pytest.mark.asyncio
 async def test_validate_agent_permissions_success():
     """Test equivalent a test_validate_agent_permissions_success du fichier original."""
     print("Test Groupe 2-1: test_validate_agent_permissions_success")
@@ -70,7 +62,7 @@ async def test_validate_agent_permissions_success():
     print("  REUSSI")
 
 
-@pytest.mark.anyio
+@pytest.mark.asyncio
 async def test_validate_agent_permissions_failure():
     """Test equivalent a test_validate_agent_permissions_failure du fichier original."""
     print("Test Groupe 2-2: test_validate_agent_permissions_failure")
@@ -109,7 +101,7 @@ async def test_validate_agent_permissions_failure():
     print("  REUSSI")
 
 
-@pytest.mark.anyio
+@pytest.mark.asyncio
 async def test_check_agent_permission_success():
     """Test equivalent a test_check_agent_permission_success du fichier original."""
     print("Test Groupe 2-3: test_check_agent_permission_success")
@@ -152,7 +144,7 @@ async def test_check_agent_permission_success():
     print("  REUSSI")
 
 
-@pytest.mark.anyio
+@pytest.mark.asyncio
 async def test_check_agent_permission_failure():
     """Test equivalent a test_check_agent_permission_failure du fichier original."""
     print("Test Groupe 2-4: test_check_agent_permission_failure")
@@ -192,7 +184,7 @@ async def test_check_agent_permission_failure():
     print("  REUSSI")
 
 
-@pytest.mark.anyio
+@pytest.mark.asyncio
 async def main():
     """Fonction principale pour valider les 4 tests du Groupe 2."""
     print("=" * 80)
diff --git a/tests/validation_sherlock_watson/test_import.py b/tests/validation_sherlock_watson/test_import.py
index 12d42c9f..6e3952f9 100644
--- a/tests/validation_sherlock_watson/test_import.py
+++ b/tests/validation_sherlock_watson/test_import.py
@@ -1,15 +1,4 @@
 #!/usr/bin/env python3
-
-# ===== ONE-LINER AUTO-ACTIVATEUR D'ENVIRONNEMENT =====
-# Assure l'activation automatique de l'environnement projet
-import sys
-import os
-sys.path.insert(0, os.path.join(os.path.dirname(os.path.dirname(__file__)), '..'))
-try:
-    import project_core.core_from_scripts.auto_env  # Auto-activation environnement intelligent
-except ImportError:
-    print("[WARN] Auto-activation d'environnement non disponible - continuons sans")
-
 try:
     from semantic_kernel.agents.chat_completion.chat_completion_agent import ChatCompletionAgent
     print("Import works - ChatCompletionAgent found")
@@ -18,7 +7,7 @@ except ImportError as e:
     
     # Test avec une version alternative
     try:
-        from semantic_kernel.agents import ChatCompletionAgent
+        from semantic_kernel_compatibility import ChatCompletionAgent
         print("Alternative import works - ChatCompletionAgent found in agents module")
     except ImportError as e2:
         print(f"Alternative import failed: {e2}")
@@ -49,4 +38,4 @@ except ImportError as e:
                                 print(f"Import line found: {line.strip()}")
                     
         except Exception as e3:
-            print(f"Failed to analyze semantic_kernel: {e3}")
\ No newline at end of file
+            print(f"Failed to analyze semantic_kernel: {e3}")
diff --git a/tests/validation_sherlock_watson/test_oracle_validation_consolidated.py b/tests/validation_sherlock_watson/test_oracle_validation_consolidated.py
index a5099acd..c2ba5d4f 100644
--- a/tests/validation_sherlock_watson/test_oracle_validation_consolidated.py
+++ b/tests/validation_sherlock_watson/test_oracle_validation_consolidated.py
@@ -103,7 +103,7 @@ def test_add_revelation():
     assert oracle_state.cards_revealed == 1
     print("[OK] add_revelation compatible")
 
-@pytest.mark.anyio
+@pytest.mark.asyncio
 async def test_validate_suggestion_async():
     """Test de la méthode async validate_suggestion_with_oracle."""
     print("\n[TEST] validate_suggestion_with_oracle (async)...")
@@ -116,7 +116,7 @@ async def test_validate_suggestion_async():
 
 # Tests de test_oracle_fixes_simple.py
 
-@pytest.mark.anyio
+@pytest.mark.asyncio
 async def test_oracle_fixes_consolidated():
     """Test consolidé des corrections Oracle."""
     print("\n=== Test consolidé des corrections Oracle ===")
diff --git a/tests/validation_sherlock_watson/test_phase_c_fluidite_transitions.py b/tests/validation_sherlock_watson/test_phase_c_fluidite_transitions.py
index 6323f75c..f52262e8 100644
--- a/tests/validation_sherlock_watson/test_phase_c_fluidite_transitions.py
+++ b/tests/validation_sherlock_watson/test_phase_c_fluidite_transitions.py
@@ -1,10 +1,3 @@
-
-# Authentic gpt-4o-mini imports (replacing mocks)
-import openai
-from semantic_kernel.contents import ChatHistory
-from semantic_kernel.core_plugins import ConversationSummaryPlugin
-from config.unified_config import UnifiedConfig
-
 import pytest
 #!/usr/bin/env python3
 """
@@ -50,7 +43,7 @@ class MockChatMessage:
         self.name = name
         self.content = content
 
-@pytest.mark.anyio
+@pytest.mark.asyncio
 async def test_phase_c_fluidite_complete():
     """
     Test complet de la Phase C avec 5 conversations simulées.
@@ -108,7 +101,7 @@ async def test_phase_c_fluidite_complete():
     
     print("\n[OK] TEST PHASE C TERMINE")
 
-@pytest.mark.anyio
+@pytest.mark.asyncio
 async def simulate_fluidity_conversation(test_num: int, elements_jeu: Dict[str, List[str]]) -> Dict[str, Any]:
     """
     Simule une conversation avec focus sur la fluidité des transitions.
@@ -210,7 +203,7 @@ async def simulate_fluidity_conversation(test_num: int, elements_jeu: Dict[str,
     
     return fluidity_metrics
 
-@pytest.mark.anyio
+@pytest.mark.asyncio
 async def simulate_contextual_analysis(oracle_state, agent_name: str, content: str, turn: int):
     """
     Simule l'analyse contextuelle qui serait faite par l'orchestrateur.
diff --git a/tests/validation_sherlock_watson/test_phase_c_simple.py b/tests/validation_sherlock_watson/test_phase_c_simple.py
index edd66a1f..a14440f7 100644
--- a/tests/validation_sherlock_watson/test_phase_c_simple.py
+++ b/tests/validation_sherlock_watson/test_phase_c_simple.py
@@ -18,7 +18,7 @@ logging.basicConfig(
 )
 logger = logging.getLogger(__name__)
 
-@pytest.mark.anyio
+@pytest.mark.asyncio
 async def test_phase_c_simple():
     """
     Test simple de la Phase C avec mémoire contextuelle.
diff --git a/tests/validation_sherlock_watson/test_phase_d_simple_fixed.py b/tests/validation_sherlock_watson/test_phase_d_simple_fixed.py
index e543c0fe..ee009787 100644
--- a/tests/validation_sherlock_watson/test_phase_d_simple_fixed.py
+++ b/tests/validation_sherlock_watson/test_phase_d_simple_fixed.py
@@ -11,7 +11,7 @@ logging.basicConfig(level=logging.INFO)
 logger = logging.getLogger(__name__)
 
 
-@pytest.mark.anyio
+@pytest.mark.asyncio
 async def test_phase_d_simple():
     """Test simple des fonctionnalités Phase D."""
     print("DEBUT TEST PHASE D SIMPLE")
diff --git a/tests/validation_sherlock_watson/test_phase_d_trace_ideale.py b/tests/validation_sherlock_watson/test_phase_d_trace_ideale.py
index a6add864..df3d1970 100644
--- a/tests/validation_sherlock_watson/test_phase_d_trace_ideale.py
+++ b/tests/validation_sherlock_watson/test_phase_d_trace_ideale.py
@@ -28,7 +28,7 @@ logging.basicConfig(
 logger = logging.getLogger(__name__)
 
 
-@pytest.mark.anyio
+@pytest.mark.asyncio
 async def test_phase_d_trace_ideale():
     """
     Test complet de la Phase D pour atteindre la trace idéale (8.0+/10).
@@ -337,7 +337,7 @@ async def test_phase_d_trace_ideale():
         return None
 
 
-@pytest.mark.anyio
+@pytest.mark.asyncio
 async def demonstration_trace_ideale():
     """
     Démonstration complète d'une conversation trace idéale.
diff --git a/tests/validation_sherlock_watson/test_verification_fonctionnalite_oracle.py b/tests/validation_sherlock_watson/test_verification_fonctionnalite_oracle.py
index cbacbea4..d68f6356 100644
--- a/tests/validation_sherlock_watson/test_verification_fonctionnalite_oracle.py
+++ b/tests/validation_sherlock_watson/test_verification_fonctionnalite_oracle.py
@@ -7,15 +7,6 @@ Ce script vérifie que les modifications des prompts n'ont pas cassé la fonctio
 technique des agents Watson, Moriarty et Sherlock.
 """
 
-# Auto-activation environnement intelligent
-import sys
-import os
-sys.path.insert(0, os.path.join(os.path.dirname(os.path.dirname(__file__)), '..'))
-try:
-    import project_core.core_from_scripts.auto_env
-except ImportError:
-    print("[WARN] Auto-activation d'environnement non disponible - continuons sans")
-
 import sys
 import importlib.util
 
@@ -27,7 +18,7 @@ def test_watson_import():
         assert True, "Watson import réussi"
     except Exception as e:
         print(f"[ERREUR] Watson import: {e}")
-        return False
+        assert False
 
 def test_moriarty_import():
     """Test d'import Moriarty"""
@@ -37,17 +28,17 @@ def test_moriarty_import():
         assert True, "Moriarty import réussi"
     except Exception as e:
         print(f"[ERREUR] Moriarty import: {e}")
-        return False
+        assert False
 
 def test_sherlock_import():
     """Test d'import Sherlock"""
     try:
         from argumentation_analysis.agents.core.pm.sherlock_enquete_agent import SherlockEnqueteAgent
         print("[OK] Sherlock import réussi")
-        assert True, "Sherlock import réussi"
+        assert True
     except Exception as e:
         print(f"[ERREUR] Sherlock import: {e}")
-        assert False, f"Sherlock import échoué: {e}"
+        assert False
 
 def test_prompt_syntax():
     """Test de syntaxe des prompts modifiés"""
@@ -57,31 +48,31 @@ def test_prompt_syntax():
         from argumentation_analysis.agents.core.pm.sherlock_enquete_agent import SHERLOCK_ENQUETE_AGENT_SYSTEM_PROMPT
         
         # Vérification Watson
-        if "Vous êtes Watson" in WATSON_LOGIC_ASSISTANT_SYSTEM_PROMPT:
+        if "Vous êtes le Dr. John Watson" in WATSON_LOGIC_ASSISTANT_SYSTEM_PROMPT:
             print("[OK] Watson prompt bien formaté")
         else:
             print("[ERREUR] Watson prompt mal formaté")
-            assert False, "Watson prompt mal formaté"
+            assert False
             
         # Vérification Sherlock
         if "Vous êtes Sherlock Holmes" in SHERLOCK_ENQUETE_AGENT_SYSTEM_PROMPT:
-            print("[OK] Sherlock prompt bien formaté")
+            print("[OK] Sherlock prompt bien formaté") 
         else:
             print("[ERREUR] Sherlock prompt mal formaté")
-            assert False, "Sherlock prompt mal formaté"
+            assert False
             
         # Vérification Moriarty
         moriarty_instructions = MoriartyInterrogatorAgent.MORIARTY_SPECIALIZED_INSTRUCTIONS
-        if "Vous êtes Moriarty" in moriarty_instructions:
+        if "Professor James Moriarty" in moriarty_instructions:
             print("[OK] Moriarty prompt bien formaté")
         else:
             print("[ERREUR] Moriarty prompt mal formaté")
-            assert False, "Moriarty prompt mal formaté"
+            assert False
             
-        assert True, "Tous les prompts sont bien formatés"
+        assert True
     except Exception as e:
         print(f"[ERREUR] Test syntaxe prompts: {e}")
-        assert False, f"Test syntaxe prompts échoué: {e}"
+        assert False
 
 def test_new_personality_keywords():
     """Test de présence des nouveaux mots-clés de personnalité"""
@@ -90,28 +81,27 @@ def test_new_personality_keywords():
         from argumentation_analysis.agents.core.oracle.moriarty_interrogator_agent import MoriartyInterrogatorAgent
         from argumentation_analysis.agents.core.pm.sherlock_enquete_agent import SHERLOCK_ENQUETE_AGENT_SYSTEM_PROMPT
         
-        # Nouveaux mots-clés Watson (partenariat et analyse)
-        watson_keywords = ["partenaire égal", "STEP-BY-STEP", "brillant", "analyste"]
+        # Nouveaux mots-clés Watson (proactivité)
+        watson_keywords = ["J'observe que", "Logiquement", "partenaire intellectuel", "proactif"]
         watson_found = sum(1 for keyword in watson_keywords if keyword in WATSON_LOGIC_ASSISTANT_SYSTEM_PROMPT)
         print(f"[OK] Watson nouveaux mots-clés: {watson_found}/4")
         
-        # Nouveaux mots-clés Moriarty (théâtralité)
+        # Nouveaux mots-clés Moriarty (théâtralité)  
         moriarty_instructions = MoriartyInterrogatorAgent.MORIARTY_SPECIALIZED_INSTRUCTIONS
-        moriarty_keywords = ["théâtral", "mystère", "énigmatique", "fascinant"]
+        moriarty_keywords = ["théâtrale", "mystérieux", "ironique", "manipulation"]
         moriarty_found = sum(1 for keyword in moriarty_keywords if keyword.lower() in moriarty_instructions.lower())
         print(f"[OK] Moriarty nouveaux mots-clés: {moriarty_found}/4")
         
-        # Nouveaux mots-clés Sherlock (leadership et intuition)
-        sherlock_keywords = ["leader naturel", "intuition", "légendaire", "déducteur"]
+        # Nouveaux mots-clés Sherlock (leadership)
+        sherlock_keywords = ["charismatique", "confiance", "intuition", "leadership"]
         sherlock_found = sum(1 for keyword in sherlock_keywords if keyword.lower() in SHERLOCK_ENQUETE_AGENT_SYSTEM_PROMPT.lower())
         print(f"[OK] Sherlock nouveaux mots-clés: {sherlock_found}/4")
         
-        success = watson_found >= 2 and moriarty_found >= 2 and sherlock_found >= 2
-        assert success, f"Mots-clés insuffisants: Watson {watson_found}/4, Moriarty {moriarty_found}/4, Sherlock {sherlock_found}/4"
+        return watson_found >= 2 and moriarty_found >= 2 and sherlock_found >= 2
         
     except Exception as e:
         print(f"[ERREUR] Test mots-clés personnalité: {e}")
-        assert False, f"Test mots-clés personnalité échoué: {e}"
+        assert False
 
 def test_core_functionality_preserved():
     """Test que les fonctionnalités de base sont préservées"""
@@ -125,7 +115,7 @@ def test_core_functionality_preserved():
             print("[OK] Watson outils préservés")
         else:
             print("[ERREUR] Watson outils manquants")
-            assert False, "Watson outils manquants"
+            assert False
         
         # Test Moriarty Tools
         from argumentation_analysis.agents.core.oracle.dataset_access_manager import CluedoDatasetManager
@@ -143,17 +133,17 @@ def test_core_functionality_preserved():
             print("[OK] Moriarty outils préservés")
         else:
             print("[ERREUR] Moriarty outils manquants")
-            assert False, "Moriarty outils manquants"
+            assert False
         
         # Test Sherlock Tools - Vérification que la classe existe
         from argumentation_analysis.agents.core.pm.sherlock_enquete_agent import SherlockTools
         print("[OK] Sherlock outils préservés")
         
-        assert True, "Toutes les fonctionnalités de base sont préservées"
+        assert True
         
     except Exception as e:
         print(f"[ERREUR] Test fonctionnalité préservée: {e}")
-        assert False, f"Test fonctionnalité préservée échoué: {e}"
+        assert False
 
 def main():
     """Test principal de vérification"""
@@ -201,11 +191,11 @@ def main():
     if success_rate >= 80:
         print("\n[SUCCES] Fonctionnalité Oracle préservée !")
         print("Les modifications Phase A n'ont pas cassé le système")
-        return True
+        assert True
     else:
         print("\n[ECHEC] Problèmes détectés !")
         print("Révision nécessaire des modifications")
-        return False
+        assert False
 
 if __name__ == "__main__":
     try:

==================== COMMIT: 63a133cb5016adc16e7c8c1a9851feb0d628a53a ====================
commit 63a133cb5016adc16e7c8c1a9851feb0d628a53a
Author: jsboigeEpita <jean-sylvain.boige@epita.fr>
Date:   Mon Jun 16 01:28:39 2025 +0200

    Refactor: Apply changes from stash@{0} (real JVM for tests)

diff --git a/argumentation_analysis/core/jvm_setup.py b/argumentation_analysis/core/jvm_setup.py
index 818b039c..6df648d3 100644
--- a/argumentation_analysis/core/jvm_setup.py
+++ b/argumentation_analysis/core/jvm_setup.py
@@ -1,332 +1,379 @@
-# argumentation_analysis/core/jvm_setup.py
+# core/jvm_setup.py
+import os
+import sys
 import jpype
-import jpype.imports
 import logging
-import os
+import platform
+import shutil
+import subprocess
+import zipfile
+import requests
 from pathlib import Path
-from typing import Optional, List
+from typing import Optional, List, Dict
+from tqdm import tqdm
 
-# Configuration du logger pour ce module
-logger = logging.getLogger("Orchestration.JPype")
 
-# --- Fonctions pour une initialisation paresseuse (Lazy Initialization) ---
+# --- Configuration et Constantes ---
+logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
 
-_PROJECT_ROOT_DIR = None
-_LIBS_DIR = None
-_PORTABLE_JDK_PATH = None
-_ENV_LOADED = False
-_JVM_WAS_SHUTDOWN = False  # Indicateur pour éviter les tentatives de redémarrage
-_JVM_INITIALIZED_THIS_SESSION = False  # Flag pour la session de test
-_SESSION_FIXTURE_OWNS_JVM = False  # Flag pour indiquer que la fixture de session contrôle la JVM
+# --- Constantes de Configuration ---
+# Répertoires (utilisant pathlib pour la robustesse multi-plateforme)
+PROJ_ROOT = Path(__file__).resolve().parents[3]
+LIBS_DIR = PROJ_ROOT / "libs"
+TWEETY_VERSION = "1.24" # Mettre à jour au besoin
+# TODO: Lire depuis un fichier de config centralisé (par ex. pyproject.toml ou un .conf)
+# Au lieu de TWEETY_VERSION = "1.24", on pourrait avoir get_config("tweety.version")
 
-def _ensure_env_loaded():
-    """Charge les variables d'environnement une seule fois."""
-    global _ENV_LOADED
-    if _ENV_LOADED:
-        return
+# Configuration des URLs des dépendances
+TWEETY_BASE_URL = "https://repo.maven.apache.org/maven2"
+TWEETY_ARTIFACTS: Dict[str, Dict[str, str]] = {
+    # Core
+    "tweety-arg": {"group": "net.sf.tweety", "version": TWEETY_VERSION},
+    # Modules principaux (à adapter selon les besoins du projet)
+    "tweety-lp": {"group": "net.sf.tweety.lp", "version": TWEETY_VERSION},
+    "tweety-log": {"group": "net.sf.tweety.log", "version": TWEETY_VERSION},
+    "tweety-math": {"group": "net.sf.tweety.math", "version": TWEETY_VERSION},
+    # Natives (exemple ; peuvent ne pas exister pour toutes les versions)
+    "tweety-native-maxsat": {"group": "net.sf.tweety.native", "version": TWEETY_VERSION, "classifier": f"maxsat-{platform.system().lower()}"}
+}
+
+# Configuration JDK portable
+MIN_JAVA_VERSION = 11
+JDK_VERSION = "17.0.2" # Exemple, choisir une version LTS stable
+JDK_BUILD = "8"
+JDK_URL_TEMPLATE = "https://github.com/adoptium/temurin{maj_v}-binaries/releases/download/jdk-{v}%2B{b}/OpenJDK{maj_v}U-jdk_{arch}_{os}_hotspot_{v}_{b_flat}.zip"
+# Windows: x64_windows, aarch64_windows | Linux: x64_linux, aarch64_linux | macOS: x64_mac, aarch64_mac
+
+# --- Fonctions Utilitaires ---
+def get_os_arch_for_jdk() -> Dict[str, str]:
+    """Détermine l'OS et l'architecture pour l'URL de téléchargement du JDK."""
+    system = platform.system().lower()
+    arch = platform.machine().lower()
+
+    os_map = {"windows": "windows", "linux": "linux", "darwin": "mac"}
+    arch_map = {"amd64": "x64", "x86_64": "x64", "aarch64": "aarch64", "arm64": "aarch64"}
+
+    if system not in os_map:
+        raise OSError(f"Système d'exploitation non supporté pour le JDK portable : {platform.system()}")
+    if arch not in arch_map:
+        raise OSError(f"Architecture non supportée pour le JDK portable : {arch}")
+
+    return {"os": os_map[system], "arch": arch_map[arch]}
+
+
+def download_file(url: str, dest_path: Path):
+    """Télécharge un fichier avec une barre de progression."""
+    logging.info(f"Téléchargement de {url} vers {dest_path}...")
     try:
-        from dotenv import load_dotenv, find_dotenv
-        env_file = find_dotenv()
-        if env_file:
-            load_dotenv(env_file, override=True)
-            logger.info(f"Variables d'environnement chargées depuis: {env_file}")
-            _ENV_LOADED = True
-        else:
-            logger.warning("Fichier .env non trouvé, utilisation des variables système")
-            _ENV_LOADED = True # Marquer comme chargé pour ne pas retenter
-    except ImportError:
-        logger.warning("python-dotenv non disponible, utilisation des variables système uniquement")
-        _ENV_LOADED = True # Marquer comme chargé pour ne pas retenter
-
-def get_project_root() -> Path:
-    """Retourne le répertoire racine du projet (calculé une seule fois)."""
-    global _PROJECT_ROOT_DIR
-    if _PROJECT_ROOT_DIR is None:
-        _PROJECT_ROOT_DIR = Path(__file__).resolve().parent.parent.parent
-    return _PROJECT_ROOT_DIR
-
-def find_libs_dir() -> Optional[Path]:
-    """Trouve le répertoire des JARs de Tweety (calculé une seule fois)."""
-    global _LIBS_DIR
-    if _LIBS_DIR is not None:
-        return _LIBS_DIR if _LIBS_DIR else None # Retourne None si la recherche précédente a échoué
-
-    project_root = get_project_root()
-    primary_dir = project_root / "libs" / "tweety"
-    fallback_dir = project_root / "libs"
-
-    if primary_dir.is_dir() and list(primary_dir.glob("*.jar")):
-        _LIBS_DIR = primary_dir
-        logger.info(f"LIBS_DIR défini sur (primaire): {_LIBS_DIR}")
-    elif fallback_dir.is_dir() and list(fallback_dir.glob("*.jar")):
-        _LIBS_DIR = fallback_dir
-        logger.info(f"LIBS_DIR défini sur (fallback): {_LIBS_DIR}")
-    else:
-        logger.warning(
-            f"Aucun JAR trouvé ni dans {primary_dir} ni dans {fallback_dir}. "
-            f"LIBS_DIR n'est pas défini."
-        )
-        _LIBS_DIR = Path() # Marqueur pour indiquer que la recherche a échoué
-        return None
-    return _LIBS_DIR
-
-def find_jdk_path() -> Optional[Path]:
-    """Trouve le chemin du JDK portable ou via JAVA_HOME (calculé une seule fois)."""
-    global _PORTABLE_JDK_PATH
-    if _PORTABLE_JDK_PATH is not None:
-         return _PORTABLE_JDK_PATH if _PORTABLE_JDK_PATH else None
-
-    _ensure_env_loaded()
-    project_root = get_project_root()
-
-    # Priorité 1: Variable d'environnement JAVA_HOME
-    java_home = os.getenv('JAVA_HOME')
-    if java_home:
-        potential_path = Path(java_home)
-        if not potential_path.is_absolute():
-            potential_path = get_project_root() / potential_path
+        response = requests.get(url, stream=True, timeout=30)
+        response.raise_for_status()
+
+        total_size = int(response.headers.get("content-length", 0))
+        with open(dest_path, "wb") as f, tqdm(
+            desc=dest_path.name,
+            total=total_size,
+            unit="iB",
+            unit_scale=True,
+            unit_divisor=1024,
+        ) as bar:
+            for chunk in response.iter_content(chunk_size=8192):
+                size = f.write(chunk)
+                bar.update(size)
+    except requests.RequestException as e:
+        logging.error(f"Erreur de téléchargement pour {url}: {e}")
+        if dest_path.exists():
+            dest_path.unlink() # Nettoyer le fichier partiel
+        raise
+    except IOError as e:
+        logging.error(f"Erreur d'écriture du fichier {dest_path}: {e}")
+        if dest_path.exists():
+            dest_path.unlink()
+        raise
+
+
+def unzip_file(zip_path: Path, dest_dir: Path):
+    """Décompresse un fichier ZIP."""
+    logging.info(f"Décompression de {zip_path} vers {dest_dir}...")
+    try:
+        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
+            # Pour éviter les problèmes de "répertoire dans un répertoire"
+            # On vérifie si tout le contenu est dans un seul dossier
+            file_list = zip_ref.namelist()
+            top_level_dirs = {Path(f).parts[0] for f in file_list}
             
-        if potential_path.is_dir():
-            _PORTABLE_JDK_PATH = potential_path
-            logger.info(f"(OK) JDK détecté via JAVA_HOME : {_PORTABLE_JDK_PATH}")
-            return _PORTABLE_JDK_PATH
-        else:
-            logger.warning(f"(ATTENTION) JAVA_HOME défini mais répertoire inexistant : {potential_path}")
-    
-    # Priorité 2: Chemin par défaut
-    jdk_subdir = "libs/portable_jdk/jdk-17.0.11+9"
-    potential_path = project_root / jdk_subdir
-    if potential_path.is_dir():
-        _PORTABLE_JDK_PATH = potential_path
-        logger.info(f"(OK) JDK portable détecté via chemin par défaut : {_PORTABLE_JDK_PATH}")
-        return _PORTABLE_JDK_PATH
-    
-    logger.warning(f"(ATTENTION) JDK portable non trouvé à l'emplacement par défaut : {potential_path}")
-    _PORTABLE_JDK_PATH = Path() # Marqueur pour indiquer que la recherche a échoué
-    return None
-
-def get_jvm_options() -> List[str]:
-    """Prépare les options pour le démarrage de la JVM, incluant le chemin du JDK si disponible."""
-    options = [
-        "-Xms64m",      # Réduit de 128m à 64m pour éviter les access violations
-        "-Xmx256m",     # Réduit de 512m à 256m pour les tests
-        "-Dfile.encoding=UTF-8",
-        "-Djava.awt.headless=true"
-    ]
-    
-    # Options spécifiques Windows pour contourner les access violations JPype
-    if os.name == 'nt':  # Windows
-        options.extend([
-            "-XX:+UseG1GC",              # Garbage collector plus stable
-            "-XX:+DisableExplicitGC",    # Évite les GC manuels problématiques
-            "-XX:-UsePerfData",          # Désactive les données de performance
-            "-Djava.awt.headless=true"   # Force mode headless
-        ])
-        logger.info("Options JVM Windows spécifiques ajoutées pour contourner les access violations JPype")
-    
-    logger.info(f"Options JVM de base définies : {options}")
-    return options
+            if len(top_level_dirs) == 1:
+                 # Cas où le contenu est dans un sous-répertoire (ex: jdk-17.0.2+8/...)
+                 # On extrait directement le contenu de ce sous-répertoire
+                temp_extract_dir = dest_dir / "temp_extract"
+                zip_ref.extractall(temp_extract_dir)
+                
+                source_dir = temp_extract_dir / top_level_dirs.pop()
+                for item in source_dir.iterdir():
+                    shutil.move(str(item), str(dest_dir / item.name))
+                temp_extract_dir.rmdir() # rm -r
+            else:
+                 # Le contenu est déjà à la racine du zip
+                 zip_ref.extractall(dest_dir)
+
+        zip_path.unlink() # Nettoyer l'archive
+        logging.info("Décompression terminée.")
+    except (zipfile.BadZipFile, IOError) as e:
+        logging.error(f"Erreur lors de la décompression de {zip_path}: {e}")
+        raise
+
+# --- Fonctions de Gestion des Dépendances ---
 
-def initialize_jvm(lib_dir_path: Optional[str] = None, specific_jar_path: Optional[str] = None) -> bool:
+# --- Fonction Principale de Téléchargement Tweety ---
+def download_tweety_jars(
+    version: str = TWEETY_VERSION,
+    target_dir: str = LIBS_DIR,
+    native_subdir: str = "native"
+    ) -> bool:
     """
-    Initialise la JVM avec les JARs de TweetyProject (initialisation paresseuse).
-    
-    ATTENTION: JPype ne permet qu'un seul cycle de vie JVM par processus Python.
-    Une fois jpype.shutdownJVM() appelé, la JVM ne peut plus être redémarrée.
+    Vérifie et télécharge les JARs Tweety (Core + Modules) et les binaires natifs nécessaires.
+
+    Returns:
+        bool: True si des téléchargements ont eu lieu, False sinon.
     """
-    global _JVM_WAS_SHUTDOWN, _JVM_INITIALIZED_THIS_SESSION, _SESSION_FIXTURE_OWNS_JVM
-    
-    logger.info(f"JVM_SETUP: initialize_jvm appelée. isJVMStarted au début: {jpype.isJVMStarted()}")
-    logger.info(f"JVM_SETUP: _JVM_WAS_SHUTDOWN: {_JVM_WAS_SHUTDOWN}")
-    logger.info(f"JVM_SETUP: _JVM_INITIALIZED_THIS_SESSION: {_JVM_INITIALIZED_THIS_SESSION}")
-    logger.info(f"JVM_SETUP: _SESSION_FIXTURE_OWNS_JVM: {_SESSION_FIXTURE_OWNS_JVM}")
-    
-    # PROTECTION 1: Vérifier si une tentative de redémarrage est en cours
-    if _JVM_WAS_SHUTDOWN and not jpype.isJVMStarted():
-        logger.error("JVM_SETUP: ERREUR - Tentative de redémarrage de la JVM détectée. JPype ne supporte qu'un cycle de vie JVM par processus.")
-        logger.error("JVM_SETUP: Veuillez relancer le processus Python pour utiliser la JVM à nouveau.")
-        return False
-    
-    # PROTECTION 2: Si la fixture de session contrôle la JVM, interdire les appels directs
-    if _SESSION_FIXTURE_OWNS_JVM and jpype.isJVMStarted():
-        logger.info("JVM_SETUP: La JVM est contrôlée par la fixture de session. Utilisation de la JVM existante.")
-        _JVM_INITIALIZED_THIS_SESSION = True
-        return True
-    
-    # PROTECTION 3: Si déjà initialisée dans cette session, ne pas refaire
-    if _JVM_INITIALIZED_THIS_SESSION and jpype.isJVMStarted():
-        logger.info("JVM_SETUP: JVM déjà initialisée dans cette session. Réutilisation.")
-        return True
+    LIBS_DIR.mkdir(exist_ok=True)
+    (LIBS_DIR / native_subdir).mkdir(exist_ok=True)
     
-    if jpype.isJVMStarted():
-        logger.info("JVM_SETUP: JVM déjà démarrée (sans contrôle de session).")
-        _JVM_INITIALIZED_THIS_SESSION = True
-        return True
+    downloaded = False
+    for name, a_info in TWEETY_ARTIFACTS.items():
+        group_path = a_info["group"].replace('.', '/')
+        a_version = a_info["version"]
+        
+        jar_name_parts = [name, a_version]
+        if "classifier" in a_info:
+            jar_name_parts.append(a_info['classifier'])
 
-    try:
-        logger.info(f"JVM_SETUP: Version de JPype: {jpype.__version__}")
-    except (ImportError, AttributeError):
-        logger.warning("JVM_SETUP: Impossible d'obtenir la version de JPype.")
+        jar_filename = f"{'-'.join(jar_name_parts)}.jar"
+        jar_path = LIBS_DIR / jar_filename
 
-    try:
-        jars: List[str] = []
-        if specific_jar_path:
-            specific_jar_file = Path(specific_jar_path)
-            if specific_jar_file.is_file():
-                jars = [str(specific_jar_file)]
-                logger.info(f"Utilisation du JAR spécifique: {specific_jar_path}")
-            else:
-                logger.error(f"(ERREUR) Fichier JAR spécifique introuvable: '{specific_jar_path}'.")
-                return False
-        else:
-            jar_directory = Path(lib_dir_path) if lib_dir_path else find_libs_dir()
-            if not jar_directory or not jar_directory.is_dir():
-                logger.error(f"(ERREUR) Répertoire des JARs '{jar_directory}' invalide.")
-                return False
-            
-            jars = [str(f) for f in jar_directory.glob("*.jar")]
-            logger.info(f"Classpath construit avec {len(jars)} JAR(s) depuis '{jar_directory}'.")
-            logger.info(f"Classpath configuré avec {len(jars)} JARs (JPype {jpype.__version__})")
+        if not jar_path.exists():
+            downloaded = True
+            url = f"{TWEETY_BASE_URL}/{group_path}/{name}/{a_version}/{jar_filename}"
+            try:
+                download_file(url, jar_path)
+            except Exception:
+                logging.error(f"Échec du téléchargement pour {name}. Le projet pourrait ne pas fonctionner.")
+                return False # On arrête si un JAR critique manque
 
-        if not jars:
-            logger.error("(ERREUR) Aucun JAR trouvé pour le classpath. Démarrage annulé.")
-            return False
+    if downloaded:
+        logging.info("Téléchargement des bibliothèques Tweety terminé.")
+    else:
+        logging.info("Toutes les bibliothèques Tweety sont déjà à jour.")
         
-        jvm_options = get_jvm_options()
-        jdk_path = find_jdk_path()
-        jvm_path = None
+    return downloaded
 
-        # Stratégie de recherche de la JVM
-        try:
-            jvm_path = jpype.getDefaultJVMPath()
-            logger.info(f"JPype a trouvé une JVM par défaut : {jvm_path}")
-        except jpype.JVMNotFoundException:
-            logger.warning("JPype n'a pas trouvé de JVM par défaut. Tentative avec JAVA_HOME.")
-            if jdk_path:
-                # Construire le chemin vers jvm.dll sur Windows
-                if os.name == 'nt':
-                    potential_jvm_path = jdk_path / "bin" / "server" / "jvm.dll"
-                # Construire le chemin vers libjvm.so sur Linux
-                else:
-                    potential_jvm_path = jdk_path / "lib" / "server" / "libjvm.so"
-                
-                if potential_jvm_path.exists():
-                    jvm_path = str(potential_jvm_path)
-                    logger.info(f"Chemin JVM construit manuellement à partir de JAVA_HOME: {jvm_path}")
-                else:
-                    logger.error(f"Le fichier de la librairie JVM n'a pas été trouvé à l'emplacement attendu: {potential_jvm_path}")
-            else:
-                logger.error("JAVA_HOME n'est pas défini et la JVM par défaut n'est pas trouvable.")
 
-        if not jvm_path:
-            logger.critical("Impossible de localiser la JVM. Le démarrage est annulé.")
-            return False
+# --- Fonction de détection JAVA_HOME (modifiée pour prioriser Java >= MIN_JAVA_VERSION) ---
+def find_valid_java_home() -> Optional[str]:
+    """
+    Cherche un JAVA_HOME valide ou un JDK portable.
+    1. Vérifie la variable d'environnement JAVA_HOME.
+    2. Si invalide, cherche un JDK portable local.
+    3. Si non trouvé, télécharge et installe un JDK portable.
+    """
+    # 1. Vérifier JAVA_HOME
+    java_home_env = os.environ.get("JAVA_HOME")
+    if java_home_env:
+        logging.info(f"Variable JAVA_HOME trouvée : {java_home_env}")
+        if is_valid_jdk(Path(java_home_env)):
+            return java_home_env
 
-        logger.info(f"JVM_SETUP: Avant startJVM. isJVMStarted: {jpype.isJVMStarted()}.")
+    # 2. Chercher un JDK portable
+    portable_jdk_dir = PROJ_ROOT / "jdk"
+    if portable_jdk_dir.exists() and is_valid_jdk(portable_jdk_dir):
+        logging.info(f"JDK portable valide trouvé : {portable_jdk_dir}")
+        return str(portable_jdk_dir)
+
+    # 3. Télécharger un nouveau JDK portable
+    logging.warning("Aucun JDK valide trouvé. Tentative de téléchargement d'un JDK portable.")
+    return download_portable_jdk(portable_jdk_dir)
 
-        try:
-            logger.info(f"Tentative de démarrage de la JVM avec le chemin : {jvm_path}")
-            jpype.startJVM(jvm_path, *jvm_options, classpath=jars)
-        except Exception as e:
-            logger.error(f"Échec final du démarrage de la JVM avec le chemin '{jvm_path}'. Erreur: {e}", exc_info=True)
-            return False
 
-        logger.info(f"JVM démarrée avec succès. isJVMStarted: {jpype.isJVMStarted()}.")
+def download_portable_jdk(target_dir: Path) -> Optional[str]:
+    """Télécharge et extrait un JDK portable."""
+    try:
+        os_arch = get_os_arch_for_jdk()
+    except OSError as e:
+        logging.error(e)
+        return None
+
+    jdk_url = JDK_URL_TEMPLATE.format(
+        maj_v=JDK_VERSION.split('.')[0],
+        v=JDK_VERSION,
+        b=JDK_BUILD,
+        b_flat=JDK_BUILD, # Le format de l'URL est parfois incohérent
+        arch=os_arch['arch'],
+        os=os_arch['os']
+    )
+    
+    target_dir.mkdir(exist_ok=True)
+    zip_path = target_dir / "jdk.zip"
+
+    try:
+        download_file(jdk_url, zip_path)
+        # Supprimer le contenu précédent avant de décompresser
+        for item in target_dir.iterdir():
+            if item.is_dir():
+                shutil.rmtree(item)
+            elif item.is_file() and item.suffix != '.zip':
+                item.unlink()
+
+        unzip_file(zip_path, target_dir)
         
-        try:
-            _ = jpype.JClass("org.tweetyproject.logics.pl.syntax.PlSignature")
-            logger.info("(OK) Test de chargement de classe Tweety (PlSignature) réussi.")
-        except Exception as e_test:
-            logger.error(f"(ERREUR) Test de chargement de classe Tweety échoué: {e_test}", exc_info=True)
+        # Vérifier que le JDK est maintenant valide
+        if is_valid_jdk(target_dir):
+            logging.info(f"JDK portable installé avec succès dans {target_dir}")
+            return str(target_dir)
+        else:
+            logging.error("L'extraction du JDK n'a pas produit une installation valide.")
+            return None
 
-        # Marquer que la JVM a été initialisée avec succès dans cette session
-        _JVM_INITIALIZED_THIS_SESSION = True
-        logger.info("JVM_SETUP: Flag _JVM_INITIALIZED_THIS_SESSION défini à True.")
-        return True
+    except (requests.RequestException, IOError, zipfile.BadZipFile, shutil.Error) as e:
+        logging.error(f"Échec de l'installation du JDK portable : {e}")
+        shutil.rmtree(target_dir, ignore_errors=True) # Nettoyage complet
+        return None
 
-    except Exception as e:
-        logger.critical(f"(ERREUR CRITIQUE) Échec global du démarrage de la JVM: {e}", exc_info=True)
+
+def is_valid_jdk(path: Path) -> bool:
+    """Vérifie si un répertoire est un JDK valide et respecte la version minimale."""
+    if not path.is_dir():
+        return False
+        
+    java_exe = path / "bin" / ("java.exe" if platform.system() == "Windows" else "java")
+    if not java_exe.exists():
+        logging.warning(f"Validation JDK échouée: 'java' non trouvé dans {path / 'bin'}")
         return False
 
-def _safe_log(logger_instance, level, message, exc_info_val=False):
-    """Effectue un log de manière sécurisée, avec un fallback sur print."""
     try:
-        if logger_instance.hasHandlers():
-            logger_instance.log(level, message, exc_info=exc_info_val)
+        # Exécuter `java -version` et capturer la sortie
+        # stderr est utilisé par Java pour afficher la version
+        result = subprocess.run(
+            [str(java_exe), "-version"],
+            capture_output=True,
+            text=True,
+            check=True,
+            stderr=subprocess.PIPE
+        )
+        version_output = result.stderr
+        
+        # Parser la version (ex: "openjdk version "11.0.12" 2021-07-20")
+        first_line = version_output.splitlines()[0]
+        version_str = first_line.split('"')[1] # "11.0.12"
+        major_version = int(version_str.split('.')[0])
+
+        if major_version >= MIN_JAVA_VERSION:
+            logging.info(f"Version Java détectée: {version_str} (Majeure: {major_version}) -> Valide.")
+            return True
         else:
-            print(f"FALLBACK LOG ({logging.getLevelName(level)}): {message}")
-            if exc_info_val:
-                import traceback
-                traceback.print_exc()
-    except Exception:
-        print(f"FALLBACK LOG (Exception in logger) ({logging.getLevelName(level)}): {message}")
-
-def set_session_fixture_owns_jvm(owns: bool = True):
-    """
-    Définit si la fixture de session contrôle la JVM.
-    
-    Args:
-        owns: True si la fixture de session contrôle la JVM, False sinon
-    """
-    global _SESSION_FIXTURE_OWNS_JVM
-    _SESSION_FIXTURE_OWNS_JVM = owns
-    logger.info(f"JVM_SETUP: _SESSION_FIXTURE_OWNS_JVM défini à {owns}")
-
-def is_session_fixture_owns_jvm() -> bool:
-    """Retourne si la fixture de session contrôle la JVM."""
-    return _SESSION_FIXTURE_OWNS_JVM
-
-def reset_session_flags():
-    """Remet à zéro les flags de session (utile pour les tests)."""
-    global _JVM_INITIALIZED_THIS_SESSION, _SESSION_FIXTURE_OWNS_JVM
-    _JVM_INITIALIZED_THIS_SESSION = False
-    _SESSION_FIXTURE_OWNS_JVM = False
-    logger.info("JVM_SETUP: Flags de session remis à zéro")
-
-def shutdown_jvm_if_needed():
+            logging.warning(f"Version Java {major_version} est inférieure au minimum requis ({MIN_JAVA_VERSION}).")
+            return False
+
+    except (subprocess.CalledProcessError, FileNotFoundError, IndexError, ValueError) as e:
+        logging.error(f"Erreur lors de la validation de la version de Java à {path}: {e}")
+        return False
+        
+# --- Gestion du cycle de vie de la JVM ---
+
+_jvm_started = False
+
+def start_jvm_if_needed(force_restart: bool = False):
     """
-    Arrête la JVM si elle est démarrée.
-    
-    ATTENTION: Une fois la JVM arrêtée avec jpype.shutdownJVM(),
-    elle ne peut plus être redémarrée dans le même processus Python.
+    Démarre la JVM avec le classpath configuré, si elle n'est pas déjà démarrée.
+    Cette fonction est idempotente par défaut.
     """
-    global _JVM_WAS_SHUTDOWN
+    global _jvm_started
+    if _jvm_started and not force_restart:
+        logging.debug("La JVM est déjà démarrée. Aucune action requise.")
+        return
+
+    if force_restart and jpype.isJVMStarted():
+        logging.info("Forçage du redémarrage de la JVM...")
+        shutdown_jvm()
+
+    # 1. S'assurer que les dépendances sont présentes
+    download_tweety_jars()
     
-    _safe_log(logger, logging.INFO, "JVM_SETUP: Appel de shutdown_jvm_if_needed.")
-    _safe_log(logger, logging.INFO, f"JVM_SETUP: _SESSION_FIXTURE_OWNS_JVM: {_SESSION_FIXTURE_OWNS_JVM}")
+    # 2. Trouver un JAVA_HOME valide (ou installer un JDK)
+    java_home = find_valid_java_home()
+    if not java_home:
+        raise RuntimeError(
+            "Impossible de trouver ou d'installer un JDK valide. "
+            "Veuillez définir JAVA_HOME sur un JDK version 11+ ou assurer une connexion internet."
+        )
+
+    # 3. Construire le Classpath
+    jar_paths = [str(p) for p in LIBS_DIR.glob("*.jar")]
+    classpath = os.pathsep.join(jar_paths)
+
+    if not jar_paths:
+        raise RuntimeError(f"Aucune bibliothèque (.jar) trouvée dans {LIBS_DIR}. Le classpath est vide.")
+        
+    logging.info(f"Classpath configuré : {classpath}")
     
+    # 4. Démarrer la JVM
     try:
-        if jpype.isJVMStarted():
-            _safe_log(logger, logging.INFO, f"JVM_SETUP: JVM est démarrée. Appel de jpype.shutdownJVM().")
-            jpype.shutdownJVM()
-            _JVM_WAS_SHUTDOWN = True
-            _safe_log(logger, logging.INFO, f"JVM_SETUP: jpype.shutdownJVM() exécuté. Flag _JVM_WAS_SHUTDOWN défini à True.")
-            _safe_log(logger, logging.INFO, f"JVM_SETUP: ATTENTION: La JVM ne peut plus être redémarrée dans ce processus.")
-        else:
-            _safe_log(logger, logging.INFO, "JVM_SETUP: JVM n'était pas démarrée.")
-    except Exception as e_shutdown:
-        _safe_log(logger, logging.ERROR, f"JVM_SETUP: Erreur lors de jpype.shutdownJVM(): {e_shutdown}", exc_info_val=True)
+        logging.info("Démarrage de la JVM...")
+        jpype.startJVM(
+            #jpype.getDefaultJVMPath(), # Laisser JPype trouver la libjvm
+            jvmpath=jpype.getDefaultJVMPath(),
+            classpath=classpath,
+            ignoreUnrecognized=True,
+            convertStrings=False,
+            # Passer le JAVA_HOME trouvé permet de s'assurer que JPype utilise le bon JDK
+            # C'est implicite si la libjvm est trouvée via le path, mais c'est plus sûr
+        )
+        _jvm_started = True
+        logging.info("JVM démarrée avec succès.")
 
-# --- Exports pour l'importation par d'autres modules ---
-TWEETY_VERSION = "1.28" # Doit correspondre à la version dans libs
-LIBS_DIR = find_libs_dir()
+    except Exception as e:
+        logging.error(f"Erreur fatale lors du démarrage de la JVM : {e}")
+        logging.error(f"JAVA_HOME utilisé (si trouvé) : {java_home}")
+        logging.error(f"Chemin JVM par défaut de JPype : {jpype.getDefaultJVMPath()}")
+        # Tenter d'offrir plus de diagnostics
+        if sys.platform == "win32" and "Error: Could not find " in str(e):
+             logging.error("Astuce Windows: Assurez-vous que Microsoft Visual C++ Redistributable est installé.")
+        elif "No matching overloads found" in str(e):
+             logging.error("Astuce: Cette erreur peut survenir si le classpath est incorrect ou si une dépendance manque.")
+        raise
 
-if __name__ == "__main__":
-    logging.basicConfig(level=logging.DEBUG)
-    
-    # Utiliser la variable exportée maintenant
-    if LIBS_DIR:
-        success = initialize_jvm()
-        if success:
-            logger.info("Test initialize_jvm: SUCCÈS")
-            try:
-                TestClass = jpype.JClass("org.tweetyproject.logics.pl.syntax.PropositionalSignature")
-                logger.info(f"Classe de test chargée: {TestClass}")
-            except Exception as e:
-                logger.error(f"Erreur lors du test: {e}", exc_info=True)
-            finally:
-                shutdown_jvm_if_needed()
-        else:
-            logger.error("Test initialize_jvm: ÉCHEC")
+
+def shutdown_jvm():
+    """Arrête la JVM si elle est en cours d'exécution."""
+    global _jvm_started
+    if jpype.isJVMStarted():
+        logging.info("Arrêt de la JVM...")
+        jpype.shutdownJVM()
+        _jvm_started = False
+        logging.info("JVM arrêtée.")
     else:
-        logger.error("Test initialize_jvm: ÉCHEC - LIBS_DIR non défini.")
\ No newline at end of file
+        logging.debug("La JVM n'est pas en cours d'exécution.")
+
+# --- Point d'entrée pour exemple ou test ---
+if __name__ == "__main__":
+    logging.info("--- Démonstration du module jvm_setup ---")
+    try:
+        logging.info("\n1. Première tentative de démarrage de la JVM...")
+        start_jvm_if_needed()
+
+        logging.info("\n2. Tentative de démarrage redondante (devrait être ignorée)...")
+        start_jvm_if_needed()
+
+        # Test simple d'importation Java
+        try:
+            JString = jpype.JClass("java.lang.String")
+            my_string = JString("Ceci est un test depuis Python!")
+            logging.info(f"Test Java réussi: {my_string}")
+        except Exception as e:
+            logging.error(f"Le test d'importation Java a échoué: {e}")
+
+    except Exception as e:
+        logging.error(f"Une erreur est survenue durant la démonstration : {e}")
+
+    finally:
+        logging.info("\n3. Arrêt de la JVM...")
+        shutdown_jvm()
+        logging.info("\n--- Fin de la démonstration ---")
diff --git a/check_jar_content.py b/check_jar_content.py
new file mode 100644
index 00000000..2d46048d
--- /dev/null
+++ b/check_jar_content.py
@@ -0,0 +1,39 @@
+import zipfile
+import sys
+
+def check_class_in_jar(jar_path, class_name):
+    """
+    Vérifie si une classe est présente dans un fichier JAR.
+
+    :param jar_path: Chemin vers le fichier .jar
+    :param class_name: Nom complet de la classe (ex: org.tweetyproject.commons.util.Order)
+    """
+    class_path = class_name.replace('.', '/') + '.class'
+    try:
+        with zipfile.ZipFile(jar_path, 'r') as jar_file:
+            if class_path in jar_file.namelist():
+                print(f"SUCCES: La classe '{class_name}' a été trouvée dans '{jar_path}'.")
+                return True
+            else:
+                print(f"ECHEC: La classe '{class_name}' n'a pas été trouvée dans '{jar_path}'.")
+                # Optionnel: lister les fichiers pour le débogage si la classe n'est pas trouvée
+                # print("Contenu du JAR:")
+                # for name in jar_file.namelist():
+                #     if 'Order' in name:
+                #          print(name)
+                return False
+    except FileNotFoundError:
+        print(f"ERREUR: Le fichier JAR '{jar_path}' n'existe pas.")
+        return False
+    except Exception as e:
+        print(f"ERREUR: Une erreur inattendue est survenue: {e}")
+        return False
+
+if __name__ == "__main__":
+    if len(sys.argv) != 3:
+        print("Usage: python check_jar_content.py <chemin_vers_le_jar> <nom_de_la_classe>")
+        sys.exit(1)
+
+    jar_to_check = sys.argv[1]
+    class_to_find = sys.argv[2]
+    check_class_in_jar(jar_to_check, class_to_find)
\ No newline at end of file
diff --git a/conftest.py b/conftest.py
index cf0c73c8..936d0dc1 100644
--- a/conftest.py
+++ b/conftest.py
@@ -1,2 +1,177 @@
-# This file is intentionally left empty to avoid conflicts with specific test configurations.
-# Global configurations are now handled by respective conftest.py files in subdirectories (e.g., tests/conftest.py).
+print("INFO: conftest.py (RACINE): Fichier en cours de lecture par pytest.")
+
+import sys
+import os
+import pytest # Importé plus haut pour être disponible globalement
+import jpype
+import jpype.imports # Assurer que jpype.imports est disponible
+
+# Ajout précoce du chemin pour trouver argumentation_analysis
+current_script_dir_for_path = os.path.dirname(os.path.abspath(__file__))
+project_root_for_path = current_script_dir_for_path
+if project_root_for_path not in sys.path:
+    sys.path.insert(0, str(project_root_for_path))
+    print(f"INFO: conftest.py (RACINE): Ajout de {project_root_for_path} à sys.path.")
+else:
+    print(f"INFO: conftest.py (RACINE): {project_root_for_path} déjà dans sys.path.")
+
+initialize_jvm_func = None
+try:
+    from argumentation_analysis.core.jvm_setup import initialize_jvm as init_jvm_actual
+    initialize_jvm_func = init_jvm_actual
+    print("INFO: conftest.py (RACINE): Import de 'initialize_jvm' réussi.")
+except Exception as e_import_jvm_setup:
+    print(f"ERREUR CRITIQUE: conftest.py (RACINE): Échec de l'import de 'initialize_jvm': {e_import_jvm_setup}")
+
+USE_REAL_JVM = True
+jpype_real_jvm_initialized_value = "0"
+
+if initialize_jvm_func is None:
+    print("ERREUR CRITIQUE: conftest.py (RACINE): initialize_jvm_func non disponible. La JVM ne sera pas démarrée par ce conftest.")
+elif USE_REAL_JVM:
+    print("INFO: conftest.py (RACINE): Tentative d'initialisation de la VRAIE JVM...")
+    try:
+        if initialize_jvm_func():
+            print("INFO: conftest.py (RACINE): VRAIE JVM initialisée avec succès par initialize_jvm_func().")
+            jpype_real_jvm_initialized_value = "1"
+        else:
+            print("ERREUR: conftest.py (RACINE): initialize_jvm_func() a retourné False (échec de l'initialisation).")
+    except Exception as e_init_jvm_conftest:
+        print(f"ERREUR CRITIQUE: conftest.py (RACINE) lors de l'appel à initialize_jvm_func: {e_init_jvm_conftest}")
+else:
+    print("INFO: conftest.py (RACINE): USE_REAL_JVM est False. Initialisation de la vraie JVM sautée.")
+
+os.environ["JPYPE_REAL_JVM_INITIALIZED"] = jpype_real_jvm_initialized_value
+print(f"INFO: conftest.py (RACINE): os.environ['JPYPE_REAL_JVM_INITIALIZED'] défini à '{jpype_real_jvm_initialized_value}'.")
+
+print("INFO: conftest.py (RACINE): Initialisation minimale terminée.")
+
+if jpype.isJVMStarted():
+    print("INFO: conftest.py (RACINE): Vérification jpype.isJVMStarted() = True. L'enregistrement des domaines est géré par jvm_setup.py.")
+    jpype.imports.registerDomain("org")
+    print("INFO: conftest.py (RACINE): Domaine 'org' enregistré pour les imports.")
+    pass
+else:
+    print("INFO: conftest.py (RACINE): jpype.isJVMStarted() = False. L'enregistrement des domaines sera géré par jvm_setup.py lors du démarrage.")
+
+@pytest.fixture(scope="module")
+def dung_classes():
+    import jpype
+    print(f"DEBUG: ROOT conftest - dung_classes fixture: jpype.isJVMStarted() = {jpype.isJVMStarted()}")
+    if not jpype.isJVMStarted():
+        pytest.skip("JVM non démarrée, skip dung_classes fixture.")
+    
+    context_class_loader = None
+    try:
+        JavaThread = jpype.JClass("java.lang.Thread")
+        current_thread = JavaThread.currentThread()
+        context_class_loader = current_thread.getContextClassLoader()
+    except Exception as e_cl:
+        print(f"WARNING: ROOT conftest - Erreur obtention ContextClassLoader: {e_cl}")
+    
+    loader_to_use = context_class_loader if context_class_loader else jpype.java.lang.ClassLoader.getSystemClassLoader()
+
+    classes_to_load = {
+        "DungTheory": "org.tweetyproject.arg.dung.syntax.DungTheory",
+        "Argument": "org.tweetyproject.arg.dung.syntax.Argument",
+        "Attack": "org.tweetyproject.arg.dung.syntax.Attack",
+        "PreferredReasoner": "org.tweetyproject.arg.dung.reasoner.SimplePreferredReasoner",
+        "GroundedReasoner": "org.tweetyproject.arg.dung.reasoner.SimpleGroundedReasoner",
+        "CompleteReasoner": "org.tweetyproject.arg.dung.reasoner.SimpleCompleteReasoner",
+        "StableReasoner": "org.tweetyproject.arg.dung.reasoner.SimpleStableReasoner"
+    }
+    loaded_classes = {}
+    try:
+        for name, class_str in classes_to_load.items():
+            print(f"INFO: ROOT conftest - Chargement de {name} ({class_str}) avec loader={loader_to_use}")
+            loaded_classes[name] = jpype.JClass(class_str, loader=loader_to_use)
+        print("INFO: ROOT conftest - dung_classes: Toutes les classes Dung ont été chargées.")
+        return loaded_classes
+    except jpype.JException as e:
+        print(f"ERREUR CRITIQUE JPYPE dans dung_classes: {type(e).__name__}: {e}")
+        if hasattr(e, 'stacktrace') and callable(e.stacktrace): print(f"Stacktrace Java:\n{e.stacktrace()}")
+        pytest.fail(f"Échec JPype dans dung_classes (ROOT conftest): {e}")
+    except Exception as e_gen:
+        print(f"ERREUR CRITIQUE GENERALE dans dung_classes: {type(e_gen).__name__}: {e_gen}")
+        import traceback
+        print(f"Stacktrace Python:\n{traceback.format_exc()}")
+        pytest.fail(f"Erreur générale inattendue dans dung_classes (ROOT conftest): {e_gen}")
+
+
+@pytest.fixture(scope="module")
+def qbf_classes():
+    import jpype
+    if not jpype.isJVMStarted(): pytest.skip("JVM non démarrée")
+    print(f"DEBUG QBF_CLASSES: Classpath: {jpype.getClassPath(True)}") # Ajout du log classpath
+    loader_to_use = jpype.JClass("java.lang.Thread").currentThread().getContextClassLoader() or jpype.java.lang.ClassLoader.getSystemClassLoader()
+    try:
+        return {
+            "QuantifiedBooleanFormula": jpype.JClass("org.tweetyproject.logics.qbf.syntax.QuantifiedBooleanFormula", loader=loader_to_use),
+            "Quantifier": jpype.JClass("org.tweetyproject.logics.qbf.syntax.Quantifier", loader=loader_to_use),
+            "QbfParser": jpype.JClass("org.tweetyproject.logics.qbf.parser.QbfParser", loader=loader_to_use),
+            "Variable": jpype.JClass("org.tweetyproject.logics.commons.syntax.Variable", loader=loader_to_use),
+        }
+    except Exception as e: pytest.fail(f"Erreur chargement qbf_classes: {e}")
+
+@pytest.fixture(scope="module")
+def belief_revision_classes():
+    import jpype
+    if not jpype.isJVMStarted(): pytest.skip("JVM non démarrée")
+    print(f"DEBUG BELIEF_REVISION_CLASSES: Classpath: {jpype.getClassPath(True)}") # Ajout du log classpath
+    loader_to_use = jpype.JClass("java.lang.Thread").currentThread().getContextClassLoader() or jpype.java.lang.ClassLoader.getSystemClassLoader()
+    try:
+        return {
+            "PlFormula": jpype.JClass("org.tweetyproject.logics.pl.syntax.PlFormula", loader=loader_to_use),
+            "PlBeliefSet": jpype.JClass("org.tweetyproject.logics.pl.syntax.PlBeliefSet", loader=loader_to_use),
+            "PlParser": jpype.JClass("org.tweetyproject.logics.pl.parser.PlParser", loader=loader_to_use),
+            # ... (ajouter toutes les autres classes avec org.tweetyproject et loader_to_use)
+            "SimplePlReasoner": jpype.JClass("org.tweetyproject.logics.pl.reasoner.SimplePlReasoner", loader=loader_to_use),
+            "Negation": jpype.JClass("org.tweetyproject.logics.pl.syntax.Negation", loader=loader_to_use),
+            "KernelContractionOperator": jpype.JClass("org.tweetyproject.beliefdynamics.kernels.KernelContractionOperator", loader=loader_to_use),
+            "RandomIncisionFunction": jpype.JClass("org.tweetyproject.beliefdynamics.kernels.RandomIncisionFunction", loader=loader_to_use),
+            "DefaultMultipleBaseExpansionOperator": jpype.JClass("org.tweetyproject.beliefdynamics.DefaultMultipleBaseExpansionOperator", loader=loader_to_use), # Correction du chemin
+            "LeviMultipleBaseRevisionOperator": jpype.JClass("org.tweetyproject.beliefdynamics.LeviMultipleBaseRevisionOperator", loader=loader_to_use), # Correction du chemin
+            "CrMasBeliefSet": jpype.JClass("org.tweetyproject.beliefdynamics.mas.CrMasBeliefSet", loader=loader_to_use),
+            "InformationObject": jpype.JClass("org.tweetyproject.beliefdynamics.mas.InformationObject", loader=loader_to_use),
+            "CrMasRevisionWrapper": jpype.JClass("org.tweetyproject.beliefdynamics.mas.CrMasRevisionWrapper", loader=loader_to_use),
+            "CrMasSimpleRevisionOperator": jpype.JClass("org.tweetyproject.beliefdynamics.operators.CrMasSimpleRevisionOperator", loader=loader_to_use), # Correction du chemin
+            "CrMasArgumentativeRevisionOperator": jpype.JClass("org.tweetyproject.beliefdynamics.operators.CrMasArgumentativeRevisionOperator", loader=loader_to_use), # Correction du chemin
+            "DummyAgent": jpype.JClass("org.tweetyproject.agents.DummyAgent", loader=loader_to_use),
+            "Order": jpype.JClass("org.tweetyproject.commons.util.Order", loader=loader_to_use),
+            "PlSignature": jpype.JClass("org.tweetyproject.logics.pl.syntax.PlSignature", loader=loader_to_use),
+            "ContensionInconsistencyMeasure": jpype.JClass("org.tweetyproject.logics.pl.analysis.ContensionInconsistencyMeasure", loader=loader_to_use),
+            "NaiveMusEnumerator": jpype.JClass("org.tweetyproject.logics.pl.analysis.NaiveMusEnumerator", loader=loader_to_use),
+            "SatSolver": jpype.JClass("org.tweetyproject.logics.pl.sat.SatSolver", loader=loader_to_use),
+            "MaInconsistencyMeasure": jpype.JClass("org.tweetyproject.logics.pl.analysis.MaInconsistencyMeasure", loader=loader_to_use),
+            "McscInconsistencyMeasure": jpype.JClass("org.tweetyproject.logics.pl.analysis.McscInconsistencyMeasure", loader=loader_to_use),
+            "PossibleWorldIterator": jpype.JClass("org.tweetyproject.logics.pl.syntax.PossibleWorldIterator", loader=loader_to_use),
+            "DalalDistance": jpype.JClass("org.tweetyproject.logics.pl.util.DalalDistance", loader=loader_to_use),
+            "DSumInconsistencyMeasure": jpype.JClass("org.tweetyproject.logics.pl.analysis.DSumInconsistencyMeasure", loader=loader_to_use),
+            "DMaxInconsistencyMeasure": jpype.JClass("org.tweetyproject.logics.pl.analysis.DMaxInconsistencyMeasure", loader=loader_to_use),
+            "DHitInconsistencyMeasure": jpype.JClass("org.tweetyproject.logics.pl.analysis.DHitInconsistencyMeasure", loader=loader_to_use),
+            "ProductNorm": jpype.JClass("org.tweetyproject.math.tnorms.ProductNorm", loader=loader_to_use),
+            "FuzzyInconsistencyMeasure": jpype.JClass("org.tweetyproject.logics.pl.analysis.FuzzyInconsistencyMeasure", loader=loader_to_use),
+            "PriorityIncisionFunction": jpype.JClass("org.tweetyproject.beliefdynamics.kernels.PriorityIncisionFunction", loader=loader_to_use),
+        }
+    except Exception as e: pytest.fail(f"Erreur chargement belief_revision_classes: {e}")
+
+@pytest.fixture(scope="module")
+def dialogue_classes():
+    import jpype
+    if not jpype.isJVMStarted(): pytest.skip("JVM non démarrée")
+    print(f"DEBUG DIALOGUE_CLASSES: Classpath: {jpype.getClassPath(True)}") # Ajout du log classpath
+    loader_to_use = jpype.JClass("java.lang.Thread").currentThread().getContextClassLoader() or jpype.java.lang.ClassLoader.getSystemClassLoader()
+    try:
+        return {
+            "ArgumentationAgent": jpype.JClass("org.tweetyproject.agents.dialogues.ArgumentationAgent", loader=loader_to_use),
+            "GroundedAgent": jpype.JClass("org.tweetyproject.agents.dialogues.GroundedAgent", loader=loader_to_use),
+            "OpponentModel": jpype.JClass("org.tweetyproject.agents.dialogues.OpponentModel", loader=loader_to_use),
+            "Dialogue": jpype.JClass("org.tweetyproject.agents.dialogues.Dialogue", loader=loader_to_use),
+            "DialogueTrace": jpype.JClass("org.tweetyproject.agents.dialogues.DialogueTrace", loader=loader_to_use),
+            "DialogueResult": jpype.JClass("org.tweetyproject.agents.dialogues.DialogueResult", loader=loader_to_use),
+            "PersuasionProtocol": jpype.JClass("org.tweetyproject.agents.dialogues.PersuasionProtocol", loader=loader_to_use),
+            "Position": jpype.JClass("org.tweetyproject.agents.dialogues.Position", loader=loader_to_use),
+            "SimpleBeliefSet": jpype.JClass("org.tweetyproject.logics.commons.syntax.SimpleBeliefSet", loader=loader_to_use),
+            "DefaultStrategy": jpype.JClass("org.tweetyproject.agents.dialogues.strategies.DefaultStrategy", loader=loader_to_use),
+        }
+    except Exception as e: pytest.fail(f"Erreur chargement dialogue_classes: {e}")
diff --git a/my_agent_fixes.patch b/my_agent_fixes.patch
new file mode 100644
index 00000000..7ec96d5f
--- /dev/null
+++ b/my_agent_fixes.patch
@@ -0,0 +1,358 @@
+diff --git a/argumentation_analysis/agents/core/abc/agent_bases.py b/argumentation_analysis/agents/core/abc/agent_bases.py
+index 7574edfc..0994f22e 100644
+--- a/argumentation_analysis/agents/core/abc/agent_bases.py
++++ b/argumentation_analysis/agents/core/abc/agent_bases.py
+@@ -7,11 +7,17 @@ une logique formelle. Ces classes utilisent le pattern Abstract Base Class (ABC)
+ pour définir une interface commune que les agents concrets doivent implémenter.
+ """
+ from abc import ABC, abstractmethod
+-from typing import Dict, Any, Optional, Tuple, List, TYPE_CHECKING
++from typing import Dict, Any, Optional, Tuple, List, TYPE_CHECKING, Coroutine
+ import logging
+ 
+ from semantic_kernel import Kernel
+ from semantic_kernel.agents import Agent
++from semantic_kernel.contents import ChatHistory
++from semantic_kernel.agents.channels.chat_history_channel import ChatHistoryChannel
++from semantic_kernel.agents.chat_completion.chat_completion_agent import ChatHistoryAgentThread
++
++# Résoudre la dépendance circulaire de Pydantic
++ChatHistoryChannel.model_rebuild()
+ 
+ # Import paresseux pour éviter le cycle d'import - uniquement pour le typage
+ if TYPE_CHECKING:
+@@ -118,25 +124,59 @@ class BaseAgent(Agent, ABC):
+             "llm_service_id": self._llm_service_id,
+             "capabilities": self.get_agent_capabilities()
+         }
+-    
++
++    def get_channel_keys(self) -> List[str]:
++        """
++        Retourne les clés uniques pour identifier le canal de communication de l'agent.
++        Cette méthode est requise par AgentGroupChat.
++        """
++        # Utiliser self.id car il est déjà garanti comme étant unique
++        # (initialisé avec agent_name).
++        return [self.id]
++
++    async def create_channel(self) -> ChatHistoryChannel:
++        """
++        Crée un canal de communication pour l'agent.
++
++        Cette méthode est requise par AgentGroupChat pour permettre à l'agent
++        de participer à une conversation. Nous utilisons ChatHistoryChannel,
++        qui est une implémentation générique basée sur ChatHistory.
++        """
++        thread = ChatHistoryAgentThread()
++        return ChatHistoryChannel(thread=thread)
++
+     @abstractmethod
+     async def get_response(self, *args, **kwargs):
+         """Méthode abstraite pour obtenir une réponse de l'agent."""
+         pass
+ 
+     @abstractmethod
+-    async def invoke(self, *args, **kwargs):
+-        """Méthode abstraite pour invoquer l'agent."""
++    async def invoke_single(self, *args, **kwargs):
++        """
++        Méthode abstraite pour l'invocation de l'agent qui retourne une réponse unique.
++        Les agents concrets DOIVENT implémenter cette logique.
++        """
+         pass
+ 
+-    async def invoke_stream(self, *args, **kwargs):
+-        """Méthode par défaut pour le streaming - peut être surchargée."""
+-        result = await self.invoke(*args, **kwargs)
++    async def invoke(self, *args, **kwargs):
++        """
++        Méthode d'invocation principale compatible avec le streaming attendu par le framework SK.
++        Elle transforme la réponse unique de `invoke_single` en un flux.
++        """
++        result = await self.invoke_single(*args, **kwargs)
+         yield result
++
++    async def invoke_stream(self, *args, **kwargs):
++        """
++        Implémentation de l'interface de streaming de SK.
++        Cette méthode délègue à `invoke`, qui retourne maintenant un générateur asynchrone.
++        """
++        async for Elt in self.invoke(*args, **kwargs):
++            yield Elt
+  
+      # Optionnel, à considérer pour une interface d'appel atomique standardisée
+      # def invoke_atomic(self, method_name: str, **kwargs) -> Any:
+-    #     if hasattr(self, method_name) and callable(getattr(self, method_name)):
++     #     if hasattr(self, method_name) and callable(getattr(self, method_name)):
+     #         method_to_call = getattr(self, method_name)
+     #         # Potentiellement vérifier si la méthode est "publique" ou listée dans capabilities
+     #         return method_to_call(**kwargs)
+diff --git a/argumentation_analysis/agents/core/extract/extract_agent.py b/argumentation_analysis/agents/core/extract/extract_agent.py
+index 1b9d50ed..c33d0307 100644
+--- a/argumentation_analysis/agents/core/extract/extract_agent.py
++++ b/argumentation_analysis/agents/core/extract/extract_agent.py
+@@ -673,22 +673,7 @@ class ExtractAgent(BaseAgent):
+         self.logger.info(f"Nouvel extrait '{extract_name}' ajouté à '{source_info.get('source_name', '')}' à l'index {new_extract_idx}.")
+         return True, new_extract_idx
+ 
+-    async def get_response(self, message: str, **kwargs) -> str:
+-        """
+-        Méthode implémentée pour satisfaire l'interface BaseAgent.
+-        
+-        Retourne une réponse basée sur les capacités d'extraction de l'agent.
+-        
+-        :param message: Le message/texte à traiter
+-        :param kwargs: Arguments supplémentaires
+-        :return: Réponse de l'agent
+-        """
+-        # Pour un agent d'extraction, on peut retourner une description des capacités
+-        # ou traiter le message selon le contexte
+-        capabilities = self.get_agent_capabilities()
+-        return f"ExtractAgent '{self.name}' prêt. Capacités: {', '.join(capabilities.keys())}"
+-
+-    async def invoke(self, action: str = "extract_from_name", **kwargs) -> Any:
++    async def invoke_single(self, action: str = "extract_from_name", **kwargs) -> Any:
+         """
+         Méthode d'invocation principale pour l'agent d'extraction.
+         
+diff --git a/argumentation_analysis/agents/core/informal/informal_agent.py b/argumentation_analysis/agents/core/informal/informal_agent.py
+index 9c6887c9..50dac07c 100644
+--- a/argumentation_analysis/agents/core/informal/informal_agent.py
++++ b/argumentation_analysis/agents/core/informal/informal_agent.py
+@@ -732,53 +732,28 @@ class InformalAnalysisAgent(BaseAgent):
+                 "analysis_timestamp": self._get_timestamp()
+             }
+ 
+-    async def invoke(
+-        self,
+-        messages: List[ChatMessageContent],
+-        **kwargs: Dict[str, Any],
+-    ) -> List[ChatMessageContent]:
++    async def invoke_single(self, *args, **kwargs) -> str:
+         """
+-        Méthode principale pour interagir avec l'agent.
+-        Prend le dernier message, effectue une analyse complète et retourne le résultat.
++        Implémentation de la logique de l'agent pour une seule réponse, conforme à BaseAgent.
+         """
+-        if not messages:
+-            return []
+-        
+-        last_message = messages[-1]
+-        text_to_analyze = last_message.content
+-        
+-        self.logger.info(f"Invocation de l'agent {self.name} avec le message : '{text_to_analyze[:100]}...'")
++        self.logger.info(f"Informal Agent invoke_single called with: args={args}, kwargs={kwargs}")
+         
+-        analysis_result = await self.perform_complete_analysis(text_to_analyze)
++        raw_text = ""
++        # Extraire le texte des arguments, similaire au ProjectManagerAgent
++        if args and isinstance(args[0], list) and len(args[0]) > 0:
++            for msg in args[0]:
++                if msg.role.value.lower() == 'user':
++                    raw_text = msg.content
++                    break
+         
+-        # Formatter le résultat en ChatMessageContent
+-        response_content = json.dumps(analysis_result, indent=2, ensure_ascii=False)
+-        
+-        return [ChatMessageContent(role="assistant", content=response_content, name=self.name)]
++        if not raw_text:
++            self.logger.warning("Aucun texte trouvé dans les arguments pour l'analyse informelle.")
++            return json.dumps({"error": "No text to analyze."})
+ 
+-    async def invoke_stream(
+-        self,
+-        messages: List[ChatMessageContent],
+-        **kwargs: Dict[str, Any],
+-    ) -> AsyncGenerator[List[ChatMessageContent], None]:
+-        """
+-        Méthode de streaming pour interagir avec l'agent.
+-        Retourne le résultat complet en un seul chunk.
+-        """
+-        self.logger.info(f"Invocation en streaming de l'agent {self.name}.")
+-        response = await self.invoke(messages, **kwargs)
+-        yield response
+-
+-    async def get_response(
+-        self,
+-        messages: List[ChatMessageContent],
+-        **kwargs: Dict[str, Any],
+-    ) -> List[ChatMessageContent]:
+-        """
+-        Alias pour invoke, pour la compatibilité.
+-        """
+-        self.logger.info(f"Appel de get_response pour l'agent {self.name}.")
+-        return await self.invoke(messages, **kwargs)
++        self.logger.info(f"Déclenchement de 'perform_complete_analysis' sur le texte: '{raw_text[:100]}...'")
++        analysis_result = await self.perform_complete_analysis(raw_text)
++        
++        return json.dumps(analysis_result, indent=2, ensure_ascii=False)
+ 
+ # Log de chargement
+ # logging.getLogger(__name__).debug("Module agents.core.informal.informal_agent chargé.") # Géré par BaseAgent
+diff --git a/argumentation_analysis/agents/core/logic/propositional_logic_agent.py b/argumentation_analysis/agents/core/logic/propositional_logic_agent.py
+index bfc79e18..6f20e127 100644
+--- a/argumentation_analysis/agents/core/logic/propositional_logic_agent.py
++++ b/argumentation_analysis/agents/core/logic/propositional_logic_agent.py
+@@ -396,40 +396,24 @@ class PropositionalLogicAgent(BaseLogicAgent):
+         content = belief_set_data.get("content", "")
+         return PropositionalBeliefSet(content)
+ 
+-    async def get_response(
+-        self,
+-        chat_history: ChatHistory,
+-        settings: Optional[Any] = None,
+-    ) -> AsyncGenerator[list[ChatMessageContent], None]:
++    async def invoke_single(self, *args, **kwargs) -> str:
+         """
+-        Méthode abstraite de `Agent` pour obtenir une réponse.
+-        Non implémentée car cet agent utilise des méthodes spécifiques.
+-        """
+-        logger.warning("La méthode 'get_response' n'est pas implémentée pour PropositionalLogicAgent et ne devrait pas être appelée directement.")
+-        yield []
+-        return
+-
+-    async def invoke(
+-        self,
+-        chat_history: ChatHistory,
+-        settings: Optional[Any] = None,
+-    ) -> list[ChatMessageContent]:
+-        """
+-        Méthode abstraite de `Agent` pour invoquer l'agent.
+-        Non implémentée car cet agent utilise des méthodes spécifiques.
+-        """
+-        logger.warning("La méthode 'invoke' n'est pas implémentée pour PropositionalLogicAgent et ne devrait pas être appelée directement.")
+-        return []
+-
+-    async def invoke_stream(
+-        self,
+-        chat_history: ChatHistory,
+-        settings: Optional[Any] = None,
+-    ) -> AsyncGenerator[list[ChatMessageContent], None]:
+-        """
+-        Méthode abstraite de `Agent` pour invoquer l'agent en streaming.
+-        Non implémentée car cet agent utilise des méthodes spécifiques.
++        Implémentation de `invoke_single` pour l'agent de logique propositionnelle.
++
++        Cet agent est spécialisé et attend des appels à des fonctions spécifiques
++        (comme `text_to_belief_set` ou `execute_query`). Un appel générique
++        se contente de retourner ses capacités.
+         """
+-        logger.warning("La méthode 'invoke_stream' n'est pas implémentée pour PropositionalLogicAgent et ne devrait pas être appelée directement.")
+-        yield []
+-        return
+\ No newline at end of file
++        import json
++        self.logger.info(f"PL Agent invoke_single called with: args={args}, kwargs={kwargs}")
++        self.logger.warning("L'invocation générique de PropositionalLogicAgent n'effectue aucune action, "
++                            "car il attend un appel à une fonction spécifique. Retour des capacités.")
++        
++        capabilities = self.get_agent_capabilities()
++        response = {
++            "status": "inaction",
++            "message": "PropositionalLogicAgent is ready. Invoke a specific capability.",
++            "capabilities": capabilities
++        }
++        
++        return json.dumps(response, indent=2)
+\ No newline at end of file
+diff --git a/argumentation_analysis/agents/core/pm/pm_agent.py b/argumentation_analysis/agents/core/pm/pm_agent.py
+index 15fd476c..34a0c477 100644
+--- a/argumentation_analysis/agents/core/pm/pm_agent.py
++++ b/argumentation_analysis/agents/core/pm/pm_agent.py
+@@ -182,29 +182,32 @@ class ProjectManagerAgent(BaseAgent):
+             capabilities = self.get_agent_capabilities()
+             return f"Agent ProjectManager prêt. Capacités: {', '.join(capabilities.keys())}"
+ 
+-    async def invoke(self, function_name: str, **kwargs) -> str:
++    async def invoke_single(self, *args, **kwargs) -> str:
+         """
+-        Méthode pour invoquer une fonction spécifique de l'agent.
+-        
+-        Args:
+-            function_name: Le nom de la fonction à invoquer
+-            **kwargs: Arguments pour la fonction
+-            
+-        Returns:
+-            Le résultat de l'invocation
++        Implémentation de la logique de l'agent pour une seule réponse, appelée par la méthode `invoke` de la classe de base.
+         """
+-        self.logger.info(f"invoke appelée pour la fonction: {function_name}")
++        self.logger.info(f"PM Agent invoke_single called with: args={args}, kwargs={kwargs}")
++
++        # Le framework AgentGroupChat passe le `chat_history` comme premier argument positionnel.
++        # Nous l'extrayons pour récupérer le contexte et le texte.
++        # C'est une heuristique basée sur le fonctionnement actuel de SK.
++        raw_text = ""
++        analysis_state_snapshot = "{}" # Default empty state
+         
+-        if function_name == "define_tasks_and_delegate":
+-            analysis_state = kwargs.get("analysis_state_snapshot", "")
+-            raw_text = kwargs.get("raw_text", "")
+-            return await self.define_tasks_and_delegate(analysis_state, raw_text)
+-        elif function_name == "write_conclusion":
+-            analysis_state = kwargs.get("analysis_state_snapshot", "")
+-            raw_text = kwargs.get("raw_text", "")
+-            return await self.write_conclusion(analysis_state, raw_text)
+-        else:
+-            raise ValueError(f"Fonction inconnue: {function_name}")
++        if args and isinstance(args[0], list) and len(args[0]) > 0:
++            # L'historique (chat avec les messages précédents) semble être dans args[0]
++            # Le message initial de l'utilisateur est souvent le premier.
++            for msg in args[0]:
++                if msg.role.value.lower() == 'user':
++                    raw_text = msg.content
++                    break # On prend le premier
++            self.logger.info(f"Texte brut extrait de l'historique: '{raw_text[:100]}...'")
++
++        # Pour le state_snapshot, c'est plus complexe.
++        # Sans une convention claire, on va appeler define_tasks avec l'état par défaut.
++        # C'est le rôle du PM de démarrer le processus.
++        self.logger.info("Déclenchement de 'define_tasks_and_delegate' depuis l'appel invoke_single générique.")
++        return await self.define_tasks_and_delegate(analysis_state_snapshot, raw_text)
+ 
+     # D'autres méthodes métiers pourraient être ajoutées ici si nécessaire,
+     # par exemple, une méthode qui encapsule la logique de décision principale du PM
+diff --git a/argumentation_analysis/orchestration/analysis_runner.py b/argumentation_analysis/orchestration/analysis_runner.py
+index 797619e2..fdd993f0 100644
+--- a/argumentation_analysis/orchestration/analysis_runner.py
++++ b/argumentation_analysis/orchestration/analysis_runner.py
+@@ -33,6 +33,7 @@ import semantic_kernel as sk
+ from semantic_kernel.contents import ChatMessageContent
+ from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion, AzureChatCompletion
+ from semantic_kernel.contents.utils.author_role import AuthorRole
++from semantic_kernel.contents.chat_history import ChatHistory
+ 
+ # Correct imports
+ from argumentation_analysis.core.shared_state import RhetoricalAnalysisState
+@@ -163,8 +164,6 @@ async def _run_analysis_conversation(
+         run_logger.info(f"Création du AgentGroupChat avec les agents: {[agent.name for agent in active_agents]}")
+ 
+         # Créer le groupe de chat
+-        group_chat = AgentGroupChat(agents=active_agents)
+-
+         # Message initial pour lancer la conversation
+         initial_message_text = (
+             "Vous êtes une équipe d'analystes experts en argumentation. "
+@@ -174,13 +173,15 @@ async def _run_analysis_conversation(
+             f"Voici le texte à analyser:\n\n---\n{local_state.raw_text}\n---"
+         )
+         
+-        # Créer le message initial
+-        initial_chat_message = ChatMessageContent(role=AuthorRole.USER, content=initial_message_text)
++        # Créer un historique de chat et y ajouter le message initial
++        chat_history_for_group = ChatHistory()
++        chat_history_for_group.add_user_message(initial_message_text)
++
++        # Créer le groupe de chat avec l'historique pré-rempli
++        group_chat = AgentGroupChat(agents=active_agents, chat_history=chat_history_for_group)
+ 
+-        # Injecter le message directement dans l'historique du chat
+-        group_chat.history.append(initial_chat_message)
+-        
+         run_logger.info("Démarrage de l'invocation du groupe de chat...")
++        # L'invocation se fait sans argument car le premier message est déjà dans l'historique.
+         full_history = [message async for message in group_chat.invoke()]
+         run_logger.info("Conversation terminée.")
+         
diff --git a/project_core/core_from_scripts/environment_manager.py b/project_core/core_from_scripts/environment_manager.py
index 15b74da9..aa813d85 100644
--- a/project_core/core_from_scripts/environment_manager.py
+++ b/project_core/core_from_scripts/environment_manager.py
@@ -375,6 +375,7 @@ class EnvironmentManager:
             result = subprocess.run(
                 final_command,
                 cwd=cwd,
+                capture_output=capture_output,
                 text=True,
                 encoding='utf-8',
                 errors='replace',
diff --git a/scripts/setup/setup_test_env.ps1 b/scripts/setup/setup_test_env.ps1
index e6cf276a..919c21b9 100644
--- a/scripts/setup/setup_test_env.ps1
+++ b/scripts/setup/setup_test_env.ps1
@@ -13,7 +13,7 @@ function Write-Step {
 
 # Vérifier le répertoire courant
 $scriptDir = Get-Location
-$projectDir = if ($env:PROJECT_DIR_OVERRIDE_TEST_SETUP) { $env:PROJECT_DIR_OVERRIDE_TEST_SETUP } else { (Get-Item $PSScriptRoot).Parent.Parent.FullName }
+$projectDir = (Get-Item $scriptDir).Parent.Parent.FullName
 Write-Step "Configuration de l'environnement de test dans $projectDir"
 
 # Vérifier si un environnement virtuel existe déjà
@@ -34,8 +34,9 @@ if (-not $?) {
 # Activer l'environnement virtuel
 Write-Step "Activation de l'environnement virtuel"
 $activateScript = Join-Path -Path $venvDir -ChildPath "Scripts\Activate.ps1"
+Write-Host "DEBUG: Chemin du script d'activation : '$($activateScript)'"
 try {
-    & $activateScript
+    . $activateScript
 } catch {
     Write-Host "Échec de l'activation de l'environnement virtuel." -ForegroundColor Red
     Write-Host "Erreur: $_" -ForegroundColor Red
diff --git a/tests/integration/jpype_tweety/conftest.py b/tests/integration/jpype_tweety/conftest.py
index 55a3b3ac..f2421119 100644
--- a/tests/integration/jpype_tweety/conftest.py
+++ b/tests/integration/jpype_tweety/conftest.py
@@ -1,27 +1,11 @@
-import project_core.core_from_scripts.auto_env
 import pytest
 import os
 import sys
 import subprocess
-# import jpype # Commenté pour éviter le démarrage prématuré de la JVM
+import jpype
 from pathlib import Path
 import logging
-from argumentation_analysis.core.jvm_setup import find_libs_dir
-
-CORE_LIBS_DIR = find_libs_dir()
-
-
-# Configuration du logger pour ce fichier conftest spécifique
-logger = logging.getLogger(__name__)
-# Pour s'assurer que les logs de ce fichier sont visibles si besoin,
-# on peut ajouter un handler basique s'il n'y en a pas.
-# Cependant, la configuration globale de pytest ou du conftest racine devrait suffire.
-if not logger.handlers:
-    handler = logging.StreamHandler(sys.stdout)
-    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
-    handler.setFormatter(formatter)
-    logger.addHandler(handler)
-    logger.setLevel(logging.INFO) # Ou logging.DEBUG pour plus de détails
+from argumentation_analysis.core.jvm_setup import LIBS_DIR as CORE_LIBS_DIR
 
 PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..", ".."))
 TWEETY_LIBS_PATH = os.path.join(PROJECT_ROOT, "libs")
@@ -41,134 +25,68 @@ def get_tweety_classpath(): # Cette fonction n'est plus utilisée pour démarrer
         return []
     return jars
 
-# @pytest.fixture(scope="session", autouse=True)
-# def jvm_manager():
-#     print("DEBUG: jvm_manager fixture CALLED (tests/integration/jpype_tweety/conftest.py) - MAIS MAINTENANT COMMENTÉE")
-#     """
-#     Fixture pour s'assurer que les JARs de test sont présents et vérifier l'état de la JVM.
-#     La JVM doit être démarrée par le conftest.py racine.
-#     """
-#     import jpype # Importation locale initiale
-    
-#     # # Forcer l'utilisation du mock pour les tests jpype_tweety
-#     # # car le conftest.py racine a sa logique de mock neutralisée.
-#     # import tests.mocks.jpype_mock as jpype_mock_module
-#     # sys.modules['jpype'] = jpype_mock_module
-#     # sys.modules['jpype1'] = jpype_mock_module
-#     # # Ré-assigner la variable locale jpype pour qu'elle pointe vers le mock chargé
-#     # jpype = jpype_mock_module
-#     # print("INFO: [jpype_tweety/conftest.py] Forçage du Mock JPype COMMENTÉ. Utilisation du jpype global (réel ou mock global).")
-
-#     # # Obtenir le logger du mock pour y ajouter des messages
-#     # mock_logger = logging.getLogger("tests.mocks.jpype_mock")
-
-#     # # Démarrer le mock JVM pour cette session de test si ce n'est pas déjà fait par ce mock
-#     # # (même si le conftest racine est censé gérer la vraie JVM ou un mock global,
-#     # # ce conftest spécifique force son propre usage du mock)
-#     # # if hasattr(jpype, '_jvm_started') and not jpype.isJVMStarted(): # Vérifie si c'est notre mock et s'il a besoin d'être démarré
-#     # #     if mock_logger: # S'assurer que mock_logger est défini
-#     # #          mock_logger.info("[jpype_tweety/conftest.py] Appel de jpype.startJVM() (du mock) car isJVMStarted() est False.")
-#     # #     jpype.startJVM() # Ceci mettra _jvm_started à True dans le mock
-#     # # elif hasattr(jpype, '_jvm_started'):
-#     # #      if mock_logger:
-#     # #          mock_logger.info("[jpype_tweety/conftest.py] jpype.isJVMStarted() (du mock) est déjà True.")
-#     # # else:
-#     # #     # Si jpype n'a pas _jvm_started, c'est probablement le vrai jpype
-#     # #     print("INFO: [jpype_tweety/conftest.py] jpype semble être le vrai module, pas le mock. Pas d'appel à startJVM() du mock.")
+@pytest.fixture(scope="session", autouse=True)
+def jvm_manager():
+    print("DEBUG: jvm_manager fixture CALLED (tests/integration/jpype_tweety/conftest.py)")
+    """
+    Fixture pour s'assurer que les JARs de test sont présents et vérifier l'état de la JVM.
+    La JVM doit être démarrée par le conftest.py racine.
+    """
+    import jpype # Importation locale initiale
     
-#     try:
-#         # S'assurer que les JARs de test/application sont présents (téléchargés si besoin)
-#         # Utilisation de la version "Updated upstream" (plus robuste) pour le téléchargement.
-#         # --- Début de l'intégration du téléchargement des JARs ---
-#         script_path = Path(PROJECT_ROOT) / "scripts" / "download_test_jars.py"
-#         target_jars_dir = Path(TWEETY_LIBS_PATH)
-        
-#         if not (target_jars_dir.is_dir() and any(target_jars_dir.glob("*.jar"))):
-#             print(f"INFO: Les JARs de test/application semblent manquants dans {target_jars_dir}. Tentative de téléchargement via {script_path}...")
-#             if not script_path.is_file():
-#                 print(f"ERREUR CRITIQUE: Le script de téléchargement {script_path} n'a pas été trouvé.")
-#                 raise FileNotFoundError(f"Script de téléchargement non trouvé: {script_path}")
-            
-#             try:
-#                 print(f"INFO: Exécution du script de téléchargement: {sys.executable} {script_path}")
-#                 process = subprocess.run(
-#                     [sys.executable, str(script_path)],
-#                     capture_output=True,
-#                     text=True,
-#                     check=False
-#                 )
-                
-#                 if process.stderr and "Traceback" in process.stderr:
-#                     print(f"ERREUR (stderr du script {script_path.name}):\n{process.stderr}")
-#                 elif process.stderr:
-#                      print(f"DEBUG (stderr du script {script_path.name}):\n{process.stderr}")
-
-#                 if process.returncode != 0:
-#                     error_message = (
-#                         f"ERREUR: Le script de téléchargement des JARs ({script_path}) "
-#                         f"a échoué avec le code de retour {process.returncode}.\n"
-#                         f"Stderr: {process.stderr}\nStdout: {process.stdout}"
-#                     )
-#                     print(error_message)
-#                     raise RuntimeError(error_message)
-                
-#                 print(f"INFO: Script de téléchargement exécuté (code {process.returncode}). Sortie:\n{process.stdout}")
-#                 if not any(target_jars_dir.glob("*.jar")):
-#                     warning_message = (
-#                         f"AVERTISSEMENT: Le script de téléchargement s'est terminé "
-#                         f"mais aucun JAR n'a été trouvé dans {target_jars_dir}. "
-#                         f"Vérifiez les logs du script et le script lui-même ({script_path})."
-#                     )
-#                     print(warning_message)
-#                 else:
-#                     print(f"INFO: Les fichiers JAR sont maintenant (ou étaient déjà) présents dans {target_jars_dir}.")
-
-#             except Exception as e:
-#                 critical_error_message = (
-#                     f"ERREUR CRITIQUE lors de l'exécution du script de téléchargement {script_path}: {e}"
-#                 )
-#                 print(critical_error_message)
-#                 raise RuntimeError(critical_error_message) from e
-#         # --- Fin de l'intégration du téléchargement des JARs ---
-#         else:
-#             print(f"INFO: Les JARs de test/application sont déjà présents dans {target_jars_dir}.")
-#         # --- Fin de la logique de téléchargement ---
+    # # Forcer l'utilisation du mock pour les tests jpype_tweety
+    # # car le conftest.py racine a sa logique de mock neutralisée.
+    # import tests.mocks.jpype_mock as jpype_mock_module
+    # sys.modules['jpype'] = jpype_mock_module
+    # sys.modules['jpype1'] = jpype_mock_module
+    # # Ré-assigner la variable locale jpype pour qu'elle pointe vers le mock chargé
+    # jpype = jpype_mock_module
+    print("INFO: [jpype_tweety/conftest.py] Mock JPype a été désactivé. Utilisation du vrai JPype.")
+
+    # Obtenir le logger du mock pour y ajouter des messages
+    mock_logger = logging.getLogger("tests.mocks.jpype_mock")
 
-#         # Logique de "Stashed changes" pour vérifier la JVM et afficher les infos
-#         print("DEBUG: jvm_manager (tests/integration/jpype_tweety): Vérification de l'état de la JVM (doit être démarrée par le conftest racine).")
-#         if not jpype.isJVMStarted():
-#             error_msg = ("ERREUR CRITIQUE: La JVM n'est pas démarrée comme attendu (devrait être fait par un conftest racine). "
-#                          "Les tests jpype_tweety ne peuvent pas continuer.")
-#             print(error_msg)
-#             raise RuntimeError(error_msg)
-
-#         print("INFO: jvm_manager (tests/integration/jpype_tweety): La JVM est (ou devrait être) déjà démarrée.")
+    try:
+        # La logique de téléchargement des JARs a été supprimée.
+        # Les JARs doivent être présents dans le répertoire libs/ via Git.
+        print("INFO: La vérification/téléchargement automatique des JARs est désactivée.")
+        print("INFO: Les tests dépendent de la présence des JARs dans le repo.")
+
+        # Logique de "Stashed changes" pour vérifier la JVM et afficher les infos
+        print("DEBUG: jvm_manager (tests/integration/jpype_tweety): Vérification de l'état de la JVM (doit être démarrée par le conftest racine).")
+        if not jpype.isJVMStarted():
+            error_msg = ("ERREUR CRITIQUE: La JVM n'est pas démarrée comme attendu (devrait être fait par un conftest racine). "
+                         "Les tests jpype_tweety ne peuvent pas continuer.")
+            print(error_msg)
+            raise RuntimeError(error_msg)
+
+        print("INFO: jvm_manager (tests/integration/jpype_tweety): La JVM est (ou devrait être) déjà démarrée.")
         
-#         try:
-#             print(f"       jpype.config.java_home: {jpype.config.java_home if hasattr(jpype, 'config') else 'N/A'}")
-#             print(f"       jpype.config.jvm_path: {jpype.config.jvm_path if hasattr(jpype, 'config') else 'N/A'}")
-#             current_classpath_reported = jpype.getClassPath()
-#             print(f"       Classpath rapporté par jpype.getClassPath(): '{current_classpath_reported}' (longueur: {len(str(current_classpath_reported))})")
-#             if not current_classpath_reported:
-#                 print("       AVERTISSEMENT: jpype.getClassPath() est vide ou None!")
-#         except Exception as e_config:
-#             print(f"       Erreur lors de la tentative d'affichage de la config JPype: {e_config}")
-
-#         print(f"       Les tests jpype_tweety s'attendent à ce que les JARs de {CORE_LIBS_DIR} et ceux de tests/resources/libs soient accessibles.")
-#         print(f"       La JVM a été démarrée par le conftest racine via jvm_setup.py.")
-
-#         jpype.imports.registerDomain("net", alias="net")
-#         jpype.imports.registerDomain("org", alias="org")
-#         jpype.imports.registerDomain("java", alias="java")
-
-#         yield
-
-#     except Exception as e:
-#         print(f"Erreur critique inattendue dans la fixture jvm_manager: {e}")
-#         raise
-#     finally:
-#         if jpype.isJVMStarted():
-#             print("INFO: La JVM restera active jusqu'à la fin du processus de test principal (géré par conftest racine).")
+        try:
+            print(f"       jpype.config.java_home: {jpype.config.java_home if hasattr(jpype, 'config') else 'N/A'}")
+            print(f"       jpype.config.jvm_path: {jpype.config.jvm_path if hasattr(jpype, 'config') else 'N/A'}")
+            current_classpath_reported = jpype.getClassPath()
+            print(f"       Classpath rapporté par jpype.getClassPath(): '{current_classpath_reported}' (longueur: {len(str(current_classpath_reported))})")
+            if not current_classpath_reported:
+                print("       AVERTISSEMENT: jpype.getClassPath() est vide ou None!")
+        except Exception as e_config:
+            print(f"       Erreur lors de la tentative d'affichage de la config JPype: {e_config}")
+
+        print(f"       Les tests jpype_tweety s'attendent à ce que les JARs de {CORE_LIBS_DIR} et ceux de tests/resources/libs soient accessibles.")
+        print(f"       La JVM a été démarrée par le conftest racine via jvm_setup.py.")
+
+        jpype.imports.registerDomain("net", alias="net")
+        jpype.imports.registerDomain("org", alias="org")
+        jpype.imports.registerDomain("java", alias="java")
+
+        yield
+
+    except Exception as e:
+        print(f"Erreur critique inattendue dans la fixture jvm_manager: {e}")
+        raise
+    finally:
+        if jpype.isJVMStarted():
+            print("INFO: La JVM restera active jusqu'à la fin du processus de test principal (géré par conftest racine).")
 
 # Les fixtures de classes suivantes sont maintenant définies dans le conftest.py racine.
 # Elles sont commentées ici pour éviter les conflits et s'assurer que les versions
@@ -202,15 +120,15 @@ def get_tweety_classpath(): # Cette fonction n'est plus utilisée pour démarrer
 # @pytest.fixture(scope="module")
 # def qbf_classes():
 #     try:
-    #         QuantifiedBooleanFormula = jpype.JClass("org.tweetyproject.logics.qbf.syntax.QuantifiedBooleanFormula")
-    #         Quantifier = jpype.JClass("org.tweetyproject.logics.qbf.syntax.Quantifier")
-    #         QbfParser = jpype.JClass("org.tweetyproject.logics.qbf.parser.QbfParser")
-    #         # QBFSolver = jpype.JClass("org.tweetyproject.logics.qbf.solver.QBFSolver") # Peut nécessiter une config
-    #         Variable = jpype.JClass("org.tweetyproject.logics.commons.syntax.Variable")
-    #         # Opérateurs logiques (les noms peuvent varier, ex: Or, And, Not de commons.syntax ou qbf.syntax)
-    #         # Par exemple:
-    #         # Or = jpype.JClass("org.tweetyproject.logics.pl.syntax.Or") # Si on utilise la logique propositionnelle pour la base
-    #         # Not = jpype.JClass("org.tweetyproject.logics.pl.syntax.Not")
+#         QuantifiedBooleanFormula = jpype.JClass("org.tweetyproject.logics.qbf.syntax.QuantifiedBooleanFormula")
+#         Quantifier = jpype.JClass("org.tweetyproject.logics.qbf.syntax.Quantifier")
+#         QbfParser = jpype.JClass("org.tweetyproject.logics.qbf.parser.QbfParser")
+#         # QBFSolver = jpype.JClass("org.tweetyproject.logics.qbf.solver.QBFSolver") # Peut nécessiter une config
+#         Variable = jpype.JClass("org.tweetyproject.logics.commons.syntax.Variable")
+#         # Opérateurs logiques (les noms peuvent varier, ex: Or, And, Not de commons.syntax ou qbf.syntax)
+#         # Par exemple:
+#         # Or = jpype.JClass("org.tweetyproject.logics.pl.syntax.Or") # Si on utilise la logique propositionnelle pour la base
+#         # Not = jpype.JClass("org.tweetyproject.logics.pl.syntax.Not")
 #         # Il faudra vérifier les classes exactes pour les opérateurs dans le contexte QBF de Tweety.
 #         # Pour l'instant, on se concentre sur le parsing et la création de base.
 #         return {
@@ -230,43 +148,43 @@ def get_tweety_classpath(): # Cette fonction n'est plus utilisée pour démarrer
 # def belief_revision_classes():
 #     try:
 #         # Classes de base pour la logique propositionnelle
-    #         PlFormula = jpype.JClass("org.tweetyproject.logics.pl.syntax.PlFormula")
-    #         PlBeliefSet = jpype.JClass("org.tweetyproject.logics.pl.syntax.PlBeliefSet")
-    #         PlParser = jpype.JClass("org.tweetyproject.logics.pl.parser.PlParser")
-    #         SimplePlReasoner = jpype.JClass("org.tweetyproject.logics.pl.reasoner.SimplePlReasoner")
-    #         Negation = jpype.JClass("org.tweetyproject.logics.pl.syntax.Negation")
-
-    #         # Opérateurs de révision
-    #         KernelContractionOperator = jpype.JClass("org.tweetyproject.beliefdynamics.operators.KernelContractionOperator")
-    #         RandomIncisionFunction = jpype.JClass("org.tweetyproject.beliefdynamics.kernels.RandomIncisionFunction")
-    #         DefaultMultipleBaseExpansionOperator = jpype.JClass("org.tweetyproject.beliefdynamics.operators.DefaultMultipleBaseExpansionOperator")
-    #         LeviMultipleBaseRevisionOperator = jpype.JClass("org.tweetyproject.beliefdynamics.operators.LeviMultipleBaseRevisionOperator")
-
-    #         # Classes pour la révision multi-agents (CrMas)
-    #         CrMasBeliefSet = jpype.JClass("org.tweetyproject.beliefdynamics.mas.CrMasBeliefSet")
-    #         InformationObject = jpype.JClass("org.tweetyproject.beliefdynamics.mas.InformationObject")
-    #         CrMasRevisionWrapper = jpype.JClass("org.tweetyproject.beliefdynamics.mas.CrMasRevisionWrapper")
-    #         CrMasSimpleRevisionOperator = jpype.JClass("org.tweetyproject.beliefdynamics.mas.CrMasSimpleRevisionOperator")
-    #         CrMasArgumentativeRevisionOperator = jpype.JClass("org.tweetyproject.beliefdynamics.mas.CrMasArgumentativeRevisionOperator")
-    #         DummyAgent = jpype.JClass("org.tweetyproject.agents.DummyAgent") # Pour les exemples CrMas
-    #         Order = jpype.JClass("org.tweetyproject.commons.util.Order") # Pour la crédibilité des agents
-    #         PlSignature = jpype.JClass("org.tweetyproject.logics.pl.syntax.PlSignature")
-
-
-    #         # Mesures d'incohérence
-    #         ContensionInconsistencyMeasure = jpype.JClass("org.tweetyproject.logics.pl.analysis.ContensionInconsistencyMeasure")
-    #         NaiveMusEnumerator = jpype.JClass("org.tweetyproject.logics.pl.analysis.NaiveMusEnumerator")
-    #         SatSolver = jpype.JClass("org.tweetyproject.logics.pl.sat.SatSolver") # Nécessaire pour NaiveMusEnumerator
-    #         MaInconsistencyMeasure = jpype.JClass("org.tweetyproject.logics.pl.analysis.MaInconsistencyMeasure")
-    #         McscInconsistencyMeasure = jpype.JClass("org.tweetyproject.logics.pl.analysis.McscInconsistencyMeasure")
-    #         PossibleWorldIterator = jpype.JClass("org.tweetyproject.logics.pl.syntax.PossibleWorldIterator")
-    #         DalalDistance = jpype.JClass("org.tweetyproject.logics.pl.util.DalalDistance")
-    #         DSumInconsistencyMeasure = jpype.JClass("org.tweetyproject.logics.pl.analysis.DSumInconsistencyMeasure")
-    #         DMaxInconsistencyMeasure = jpype.JClass("org.tweetyproject.logics.pl.analysis.DMaxInconsistencyMeasure")
-    #         DHitInconsistencyMeasure = jpype.JClass("org.tweetyproject.logics.pl.analysis.DHitInconsistencyMeasure")
-    #         ProductNorm = jpype.JClass("org.tweetyproject.math.tnorms.ProductNorm")
-    #         FuzzyInconsistencyMeasure = jpype.JClass("org.tweetyproject.logics.pl.analysis.FuzzyInconsistencyMeasure")
-    #         PriorityIncisionFunction = jpype.JClass("org.tweetyproject.beliefdynamics.kernels.PriorityIncisionFunction")
+#         PlFormula = jpype.JClass("org.tweetyproject.logics.pl.syntax.PlFormula")
+#         PlBeliefSet = jpype.JClass("org.tweetyproject.logics.pl.syntax.PlBeliefSet")
+#         PlParser = jpype.JClass("org.tweetyproject.logics.pl.parser.PlParser")
+#         SimplePlReasoner = jpype.JClass("org.tweetyproject.logics.pl.reasoner.SimplePlReasoner")
+#         Negation = jpype.JClass("org.tweetyproject.logics.pl.syntax.Negation")
+
+#         # Opérateurs de révision
+#         KernelContractionOperator = jpype.JClass("org.tweetyproject.beliefdynamics.operators.KernelContractionOperator")
+#         RandomIncisionFunction = jpype.JClass("org.tweetyproject.beliefdynamics.kernels.RandomIncisionFunction")
+#         DefaultMultipleBaseExpansionOperator = jpype.JClass("org.tweetyproject.beliefdynamics.operators.DefaultMultipleBaseExpansionOperator")
+#         LeviMultipleBaseRevisionOperator = jpype.JClass("org.tweetyproject.beliefdynamics.operators.LeviMultipleBaseRevisionOperator")
+
+#         # Classes pour la révision multi-agents (CrMas)
+#         CrMasBeliefSet = jpype.JClass("org.tweetyproject.beliefdynamics.mas.CrMasBeliefSet")
+#         InformationObject = jpype.JClass("org.tweetyproject.beliefdynamics.mas.InformationObject")
+#         CrMasRevisionWrapper = jpype.JClass("org.tweetyproject.beliefdynamics.mas.CrMasRevisionWrapper")
+#         CrMasSimpleRevisionOperator = jpype.JClass("org.tweetyproject.beliefdynamics.mas.CrMasSimpleRevisionOperator")
+#         CrMasArgumentativeRevisionOperator = jpype.JClass("org.tweetyproject.beliefdynamics.mas.CrMasArgumentativeRevisionOperator")
+#         DummyAgent = jpype.JClass("org.tweetyproject.agents.DummyAgent") # Pour les exemples CrMas
+#         Order = jpype.JClass("org.tweetyproject.commons.util.Order") # Pour la crédibilité des agents
+#         PlSignature = jpype.JClass("org.tweetyproject.logics.pl.syntax.PlSignature")
+
+
+#         # Mesures d'incohérence
+#         ContensionInconsistencyMeasure = jpype.JClass("org.tweetyproject.logics.pl.analysis.ContensionInconsistencyMeasure")
+#         NaiveMusEnumerator = jpype.JClass("org.tweetyproject.logics.pl.analysis.NaiveMusEnumerator")
+#         SatSolver = jpype.JClass("org.tweetyproject.logics.pl.sat.SatSolver") # Nécessaire pour NaiveMusEnumerator
+#         MaInconsistencyMeasure = jpype.JClass("org.tweetyproject.logics.pl.analysis.MaInconsistencyMeasure")
+#         McscInconsistencyMeasure = jpype.JClass("org.tweetyproject.logics.pl.analysis.McscInconsistencyMeasure")
+#         PossibleWorldIterator = jpype.JClass("org.tweetyproject.logics.pl.syntax.PossibleWorldIterator")
+#         DalalDistance = jpype.JClass("org.tweetyproject.logics.pl.util.DalalDistance")
+#         DSumInconsistencyMeasure = jpype.JClass("org.tweetyproject.logics.pl.analysis.DSumInconsistencyMeasure")
+#         DMaxInconsistencyMeasure = jpype.JClass("org.tweetyproject.logics.pl.analysis.DMaxInconsistencyMeasure")
+#         DHitInconsistencyMeasure = jpype.JClass("org.tweetyproject.logics.pl.analysis.DHitInconsistencyMeasure")
+#         ProductNorm = jpype.JClass("org.tweetyproject.math.tnorms.ProductNorm")
+#         FuzzyInconsistencyMeasure = jpype.JClass("org.tweetyproject.logics.pl.analysis.FuzzyInconsistencyMeasure")
+#         PriorityIncisionFunction = jpype.JClass("org.tweetyproject.beliefdynamics.kernels.PriorityIncisionFunction")
 
 
 #         return {
@@ -311,26 +229,26 @@ def get_tweety_classpath(): # Cette fonction n'est plus utilisée pour démarrer
 #     if not jpype.isJVMStarted(): # Vérification directe de l'état de la JVM
 #         pytest.skip("JVM non démarrée ou JPype non initialisé correctement.")
 #     try:
-    #         ArgumentationAgent = jpype.JClass("org.tweetyproject.agents.dialogues.ArgumentationAgent")
-    #         GroundedAgent = jpype.JClass("org.tweetyproject.agents.dialogues.GroundedAgent")
-    #         OpponentModel = jpype.JClass("org.tweetyproject.agents.dialogues.OpponentModel")
-    #         Dialogue = jpype.JClass("org.tweetyproject.agents.dialogues.Dialogue")
-    #         DialogueTrace = jpype.JClass("org.tweetyproject.agents.dialogues.DialogueTrace")
-    #         DialogueResult = jpype.JClass("org.tweetyproject.agents.dialogues.DialogueResult")
-    #         PersuasionProtocol = jpype.JClass("org.tweetyproject.agents.dialogues.PersuasionProtocol")
-    #         # NegotiationProtocol = jpype.JClass("org.tweetyproject.agents.dialogues.NegotiationProtocol") # Interface
-    #         # InquiryProtocol = jpype.JClass("org.tweetyproject.agents.dialogues.InquiryProtocol") # Interface
-    #         Position = jpype.JClass("org.tweetyproject.agents.dialogues.Position")
-    #         SimpleBeliefSet = jpype.JClass("org.tweetyproject.logics.commons.syntax.SimpleBeliefSet")
-    #         # Moves - peuvent être utiles pour des assertions plus fines sur la trace
-    #         # Move = jpype.JClass("org.tweetyproject.agents.dialogues.moves.Move")
-    #         # Claim = jpype.JClass("org.tweetyproject.agents.dialogues.moves.Claim")
-    #         # DialogueStrategy = jpype.JClass("org.tweetyproject.agents.dialogues.strategies.DialogueStrategy") # Interface
-    #         DefaultStrategy = jpype.JClass("org.tweetyproject.agents.dialogues.strategies.DefaultStrategy")
+#         ArgumentationAgent = jpype.JClass("org.tweetyproject.agents.dialogues.ArgumentationAgent")
+#         GroundedAgent = jpype.JClass("org.tweetyproject.agents.dialogues.GroundedAgent")
+#         OpponentModel = jpype.JClass("org.tweetyproject.agents.dialogues.OpponentModel")
+#         Dialogue = jpype.JClass("org.tweetyproject.agents.dialogues.Dialogue")
+#         DialogueTrace = jpype.JClass("org.tweetyproject.agents.dialogues.DialogueTrace")
+#         DialogueResult = jpype.JClass("org.tweetyproject.agents.dialogues.DialogueResult")
+#         PersuasionProtocol = jpype.JClass("org.tweetyproject.agents.dialogues.PersuasionProtocol")
+#         # NegotiationProtocol = jpype.JClass("org.tweetyproject.agents.dialogues.NegotiationProtocol") # Interface
+#         # InquiryProtocol = jpype.JClass("org.tweetyproject.agents.dialogues.InquiryProtocol") # Interface
+#         Position = jpype.JClass("org.tweetyproject.agents.dialogues.Position")
+#         SimpleBeliefSet = jpype.JClass("org.tweetyproject.logics.commons.syntax.SimpleBeliefSet")
+#         # Moves - peuvent être utiles pour des assertions plus fines sur la trace
+#         # Move = jpype.JClass("org.tweetyproject.agents.dialogues.moves.Move")
+#         # Claim = jpype.JClass("org.tweetyproject.agents.dialogues.moves.Claim")
+#         # DialogueStrategy = jpype.JClass("org.tweetyproject.agents.dialogues.strategies.DialogueStrategy") # Interface
+#         DefaultStrategy = jpype.JClass("org.tweetyproject.agents.dialogues.strategies.DefaultStrategy")
         
-    #         # Pour les protocoles spécifiques si besoin (exemples de la fiche)
-    #         # MonotonicConcessionProtocol = jpype.JClass("org.tweetyproject.agents.dialogues.MonotonicConcessionProtocol")
-    #         # CollaborativeInquiryProtocol = jpype.JClass("org.tweetyproject.agents.dialogues.CollaborativeInquiryProtocol")
+#         # Pour les protocoles spécifiques si besoin (exemples de la fiche)
+#         # MonotonicConcessionProtocol = jpype.JClass("org.tweetyproject.agents.dialogues.MonotonicConcessionProtocol")
+#         # CollaborativeInquiryProtocol = jpype.JClass("org.tweetyproject.agents.dialogues.CollaborativeInquiryProtocol")
 
 
 #         return {
@@ -354,67 +272,3 @@ def get_tweety_classpath(): # Cette fonction n'est plus utilisée pour démarrer
 #         }
 #     except jpype.JException as e:
 #         pytest.fail(f"Échec de l'importation des classes Java pour l'argumentation dialogique: {e.stacktrace()}")
-
-@pytest.fixture(scope="module")
-def logic_classes(integration_jvm): # Dépend de integration_jvm pour s'assurer que la JVM est prête
-    """
-    Fournit les classes Java Tweety nécessaires pour les tests de logique.
-    """
-    # S'assurer que jpype est importé et la JVM démarrée (géré par integration_jvm)
-    import jpype
-    if not jpype.isJVMStarted():
-        pytest.skip("JVM non démarrée, impossible de charger les classes de logique.")
-
-    try:
-        # loader_to_use = jpype.JClass("java.lang.Thread").currentThread().getContextClassLoader() or jpype.java.lang.ClassLoader.getSystemClassLoader()
-        # logger.info(f"logic_classes: Utilisation du loader: {loader_to_use}")
-        logger.info(f"logic_classes: Tentative de chargement des classes SANS loader explicite.")
-
-        # Classes pour la logique propositionnelle (PL)
-        PlBeliefSet = jpype.JClass("org.tweetyproject.logics.pl.syntax.PlBeliefSet")
-        PlParser = jpype.JClass("org.tweetyproject.logics.pl.parser.PlParser")
-        PlFormula = jpype.JClass("org.tweetyproject.logics.pl.syntax.PlFormula")
-        Proposition = jpype.JClass("org.tweetyproject.logics.pl.syntax.Proposition")
-        Negation = jpype.JClass("org.tweetyproject.logics.pl.syntax.Negation")
-        Conjunction = jpype.JClass("org.tweetyproject.logics.pl.syntax.Conjunction")
-        Disjunction = jpype.JClass("org.tweetyproject.logics.pl.syntax.Disjunction")
-        Implication = jpype.JClass("org.tweetyproject.logics.pl.syntax.Implication")
-        Equivalence = jpype.JClass("org.tweetyproject.logics.pl.syntax.Equivalence")
-        SimplePlReasoner = jpype.JClass("org.tweetyproject.logics.pl.reasoner.SimplePlReasoner") # Pour requêtes simples
-        PossibleWorldIterator = jpype.JClass("org.tweetyproject.logics.pl.semantics.PossibleWorldIterator") # Corrigé: syntax -> semantics
-        PlSignature = jpype.JClass("org.tweetyproject.logics.pl.syntax.PlSignature")
-
-        # Classes pour la logique du premier ordre (FOL) - si nécessaire plus tard
-        # FolBeliefSet = jpype.JClass("org.tweetyproject.logics.fol.syntax.FolBeliefSet")
-        # FolParser = jpype.JClass("org.tweetyproject.logics.fol.parser.FolParser")
-
-        # Reasoners génériques (peuvent nécessiter des adaptations selon la logique)
-        # CredulousReasoner = jpype.JClass("org.tweetyproject.arg.reasoner.CredulousReasoner") # Exemple, vérifier le package exact
-        # SkepticalReasoner = jpype.JClass("org.tweetyproject.arg.reasoner.SkepticalReasoner") # Exemple
-
-        # Agents logiques (génériques ou spécifiques)
-        # LogicalAgent = jpype.JClass("org.tweetyproject.agents.LogicalAgent") # Vérifier le nom exact
-
-        return {
-            "PlBeliefSet": PlBeliefSet,
-            "PlParser": PlParser,
-            "PlFormula": PlFormula,
-            "Proposition": Proposition,
-            "Negation": Negation,
-            "Conjunction": Conjunction,
-            "Disjunction": Disjunction,
-            "Implication": Implication,
-            "Equivalence": Equivalence,
-            "SimplePlReasoner": SimplePlReasoner,
-            "PossibleWorldIterator": PossibleWorldIterator,
-            "PlSignature": PlSignature,
-            # "CredulousReasoner": CredulousReasoner, # A décommenter/adapter si utilisé
-            # "SkepticalReasoner": SkepticalReasoner, # A décommenter/adapter si utilisé
-            # "LogicalAgent": LogicalAgent, # A décommenter/adapter si utilisé
-        }
-    except jpype.JException as e:
-        # Afficher la stacktrace Java complète pour un meilleur diagnostic
-        stacktrace = e.stacktrace() if hasattr(e, 'stacktrace') else str(e)
-        pytest.fail(f"Échec de l'importation d'une ou plusieurs classes Java pour la logique: {stacktrace}")
-    except Exception as e_py:
-        pytest.fail(f"Erreur Python lors de la configuration de logic_classes: {str(e_py)}")
diff --git a/tests/integration/jpype_tweety/test_argumentation_syntax.py b/tests/integration/jpype_tweety/test_argumentation_syntax.py
index 2f346edf..46a2cb53 100644
--- a/tests/integration/jpype_tweety/test_argumentation_syntax.py
+++ b/tests/integration/jpype_tweety/test_argumentation_syntax.py
@@ -1,47 +1,23 @@
 import pytest
-# import jpype # Commenté pour éviter le démarrage prématuré, sera importé localement
+import jpype
 
 
 # Les classes Java sont importées via la fixture 'dung_classes' de conftest.py
 
-def test_create_argument(dung_classes, integration_jvm): # Ajout de integration_jvm pour l'utiliser directement
-    """Teste le chargement de StableExtension."""
-    import jpype # Import local
-    logger = jpype.JClass("org.slf4j.LoggerFactory").getLogger("test_create_argument_simplified")
-    logger.info("Début du test simplifié pour StableExtension.")
-    
-    try:
-        # Assurer que integration_jvm (et donc la JVM) est active
-        if not integration_jvm or not integration_jvm.isJVMStarted():
-            pytest.skip("JVM non disponible pour test_create_argument_simplified.")
-            return
-
-        StableExtension = integration_jvm.JClass("org.tweetyproject.arg.dung.semantics.StableExtension")
-        logger.info(f"StableExtension chargée avec succès: {StableExtension}")
-        assert StableExtension is not None
-        # Optionnel: tenter une instanciation si elle a un constructeur simple ou statique
-        # Pour l'instant, se concentrer sur le chargement de la classe.
-        # stable_ext_instance = StableExtension() # Cela pourrait échouer si constructeur non vide
-        # logger.info(f"Instance de StableExtension créée (si constructeur simple): {stable_ext_instance}")
-
-    except jpype.JException as e:
-        logger.error(f"Erreur JPype lors du chargement/test de StableExtension: {e}")
-        if hasattr(e, 'stacktrace'):
-            logger.error(f"Stacktrace Java: {e.stacktrace()}")
-        pytest.fail(f"Erreur JPype: {e}")
-    except Exception as e_py:
-        logger.error(f"Erreur Python lors du test de StableExtension: {e_py}")
-        pytest.fail(f"Erreur Python: {e_py}")
-    finally:
-        logger.info("Fin du test simplifié pour StableExtension.")
-
+def test_create_argument(dung_classes):
+    """Teste la création d'un argument simple."""
+    Argument = dung_classes["Argument"]
+    arg_name = "test_argument"
+    arg = Argument(jpype.JString(arg_name))
+    assert arg is not None
+    assert arg.getName() == arg_name
+    print(f"Argument créé: {arg.toString()}")
 
 def test_create_dung_theory_with_arguments_and_attacks(dung_classes):
     """
     Teste la création d'une théorie de Dung, l'ajout d'arguments et d'attaques,
     en se basant sur l'exemple de la section 4.1.2 de la fiche sujet 1.2.7.
     """
-    import jpype # Import local
     DungTheory = dung_classes["DungTheory"]
     Argument = dung_classes["Argument"]
     Attack = dung_classes["Attack"]
@@ -111,7 +87,6 @@ def test_argument_equality_and_hashcode(dung_classes):
     Teste l'égalité et le hashcode des objets Argument.
     Important pour leur utilisation dans des collections (Set, Map).
     """
-    import jpype # Import local
     Argument = dung_classes["Argument"]
     arg1_a = Argument(jpype.JString("a"))
     arg2_a = Argument(jpype.JString("a"))
@@ -153,7 +128,6 @@ def test_argument_equality_and_hashcode(dung_classes):
 
 def test_attack_equality_and_hashcode(dung_classes):
     """Teste l'égalité et le hashcode des objets Attack."""
-    import jpype # Import local
     Argument = dung_classes["Argument"]
     Attack = dung_classes["Attack"]
 
@@ -320,7 +294,6 @@ def test_parse_dung_theory_from_tgf_string(dung_classes):
     DungTheory = dung_classes["DungTheory"]
     Argument = dung_classes["Argument"] # Pour vérification
     # Supposer l'existence d'un parser TGF
-    import jpype # Import local
     TgfParser = None
     try:
         # Tentative de localisation du parser TGF

==================== COMMIT: c46f1a6fb90a75f906758b159142e462a813c0f0 ====================
commit c46f1a6fb90a75f906758b159142e462a813c0f0
Author: jsboigeEpita <jean-sylvain.boige@epita.fr>
Date:   Mon Jun 16 01:54:40 2025 +0200

    fix(jvm): Résoudre le conflit du stash de débogage ClassNotFoundException
    
    Intègre la version HEAD de jvm_setup.py, plus robuste, tout en conservant la correction du stash qui exclut un JAR problématique (lp.asp) du classpath. Les modifications sur le fichier de test avancé sont écartées car elles concernaient du débogage ponctuel dont la cause première est maintenant résolue.

diff --git a/argumentation_analysis/core/jvm_setup.py b/argumentation_analysis/core/jvm_setup.py
index 818b039c..9900c367 100644
--- a/argumentation_analysis/core/jvm_setup.py
+++ b/argumentation_analysis/core/jvm_setup.py
@@ -185,6 +185,14 @@ def initialize_jvm(lib_dir_path: Optional[str] = None, specific_jar_path: Option
                 return False
             
             jars = [str(f) for f in jar_directory.glob("*.jar")]
+# --- DÉBUT DE L'INTÉGRATION DU CHANGEMENT DU STASH ---
+            # Exclure le JAR problématique identifié dans le stash
+            jar_to_exclude = "org.tweetyproject.lp.asp-1.28-with-dependencies.jar"
+            original_jar_count = len(jars)
+            jars = [jar_path for jar_path in jars if jar_to_exclude not in Path(jar_path).name]
+            if len(jars) < original_jar_count:
+                logger.info(f"Exclusion de débogage: '{jar_to_exclude}' retiré du classpath. Nombre de JARs réduit à {len(jars)}.")
+            # --- FIN DE L'INTÉGRATION DU CHANGEMENT DU STASH ---
             logger.info(f"Classpath construit avec {len(jars)} JAR(s) depuis '{jar_directory}'.")
             logger.info(f"Classpath configuré avec {len(jars)} JARs (JPype {jpype.__version__})")
 

==================== COMMIT: 2db24f6f8298cd4039c353e1f2fb61e5bd887f69 ====================
commit 2db24f6f8298cd4039c353e1f2fb61e5bd887f69
Author: jsboigeEpita <jean-sylvain.boige@epita.fr>
Date:   Mon Jun 16 01:52:13 2025 +0200

    refactor(logic): Intégration du stash 'Initialize Tweety handlers'
    
    Ce commit intègre un refactoring majeur de la logique Tweety. La classe monolithique TweetyBridge a été décomposée en un TweetyInitializer et des handlers dédiés (PL, FOL, Modal). Les conflits avec la branche principale ont été résolus en adoptant cette nouvelle architecture, plus propre et maintenable.

diff --git a/argumentation_analysis/agents/core/logic/tweety_bridge.py b/argumentation_analysis/agents/core/logic/tweety_bridge.py
index 77fe9c07..67928e96 100644
--- a/argumentation_analysis/agents/core/logic/tweety_bridge.py
+++ b/argumentation_analysis/agents/core/logic/tweety_bridge.py
@@ -81,9 +81,9 @@ class TweetyBridge:
 
         # Initialiser les handlers spécifiques
         try:
-            self._pl_handler = PLHandler(self._initializer)
-            self._fol_handler = FOLHandler(self._initializer)
-            self._modal_handler = ModalHandler(self._initializer)
+            self._pl_handler = PLHandler()
+            self._fol_handler = FOLHandler()
+            self._modal_handler = ModalHandler()
             self._jvm_ok = True # Indique que les handlers Python sont prêts
             self._logger.info("TWEETY_BRIDGE: __init__ - Handlers PL, FOL, Modal initialisés avec succès.")
         except RuntimeError as e:
@@ -151,7 +151,7 @@ class TweetyBridge:
 
     # --- Méthodes pour la logique propositionnelle ---
     
-    def validate_formula(self, formula_string: str, constants: Optional[List[str]] = None) -> Tuple[bool, str]:
+    def validate_formula(self, formula_string: str) -> Tuple[bool, str]:
         """
         Valide la syntaxe d'une formule de logique propositionnelle.
         Délègue la validation au PLHandler.
@@ -163,7 +163,7 @@ class TweetyBridge:
         try:
             # PLHandler.parse_pl_formula lève une ValueError en cas d'échec de parsing.
             # Si elle ne lève pas d'exception, la formule est syntaxiquement valide.
-            self._pl_handler.parse_pl_formula(formula_string, constants)
+            self._pl_handler.parse_pl_formula(formula_string)
             self._logger.info(f"Formule PL '{formula_string}' validée avec succès par PLHandler.")
             return True, "Formule valide"
         except ValueError as e_val:
@@ -173,7 +173,7 @@ class TweetyBridge:
             self._logger.error(f"Erreur inattendue lors de la validation PL de '{formula_string}': {e_generic}", exc_info=True)
             return False, f"Erreur inattendue: {str(e_generic)}"
 
-    def validate_belief_set(self, belief_set_string: str, constants: Optional[List[str]] = None) -> Tuple[bool, str]:
+    def validate_belief_set(self, belief_set_string: str) -> Tuple[bool, str]:
         """
         Valide la syntaxe d'un ensemble de croyances en logique propositionnelle.
         Délègue la validation au PLHandler.
@@ -204,7 +204,7 @@ class TweetyBridge:
                 return False, "Ensemble de croyances vide ou ne contenant que des commentaires"
 
             for formula_str in cleaned_formulas:
-                self._pl_handler.parse_pl_formula(formula_str, constants) # Lèvera ValueError si invalide
+                self._pl_handler.parse_pl_formula(formula_str) # Lèvera ValueError si invalide
 
             self._logger.info(f"Ensemble de croyances PL validé avec succès par PLHandler (parsing individuel).")
             return True, "Ensemble de croyances valide"
@@ -220,19 +220,24 @@ class TweetyBridge:
         description="Exécute une requête en Logique Propositionnelle (syntaxe Tweety: !,||,=>,<=>,^^) sur un Belief Set fourni.",
         name="execute_pl_query"
     )
-    def perform_pl_query(self, belief_set_content: str, query_string: str, constants: Optional[List[str]] = None) -> Tuple[Optional[bool], str]:
+    def execute_pl_query(self, belief_set_content: str, query_string: str) -> str:
         """
-        Exécute une requête PL et retourne le résultat booléen brut et une chaîne de sortie.
+        Exécute une requête en logique propositionnelle sur un ensemble de croyances donné.
+        Délègue l'exécution au PLHandler.
         """
-        self._logger.info(f"TweetyBridge.perform_pl_query: Query='{query_string}' sur BS: ('{belief_set_content[:60]}...')")
+        self._logger.info(f"TweetyBridge.execute_pl_query: Query='{query_string}' sur BS: ('{belief_set_content[:60]}...')")
+        
         if not self.is_jvm_ready() or not hasattr(self, '_pl_handler'):
-            self._logger.error("TweetyBridge.perform_pl_query: TweetyBridge ou PLHandler non prêt.")
-            return None, "Erreur: TweetyBridge ou PLHandler non prêt."
-
+            self._logger.error("TweetyBridge.execute_pl_query: TweetyBridge ou PLHandler non prêt.")
+            return "FUNC_ERROR: TweetyBridge ou PLHandler non prêt."
+        
         try:
-            result_bool = self._pl_handler.pl_query(belief_set_content, query_string, constants)
+            # PLHandler.pl_query gère le parsing du BS et de la requête, et l'exécution.
+            # Il devrait retourner True, False, ou lever une exception.
+            result_bool = self._pl_handler.pl_query(belief_set_content, query_string)
             
-            if result_bool is None:
+            # Formater le résultat comme attendu par l'ancienne interface
+            if result_bool is None: # Cas où le handler pourrait retourner None (même si non prévu actuellement)
                 result_str = f"Tweety Result: Unknown for query '{query_string}'."
                 self._logger.warning(f"Requête PL '{query_string}' -> indéterminé (None) via PLHandler.")
             else:
@@ -240,38 +245,16 @@ class TweetyBridge:
                 result_str = f"Tweety Result: Query '{query_string}' is {result_label}."
                 self._logger.info(f"Résultat formaté requête PL '{query_string}' via PLHandler: {result_label}")
             
-            return result_bool, result_str
-
-        except ValueError as e_val:
+            return result_str
+        
+        except ValueError as e_val: # Erreurs de parsing ou autres erreurs logiques du handler
             error_msg = f"Erreur lors de l'exécution de la requête PL via PLHandler: {str(e_val)}"
             self._logger.error(error_msg, exc_info=True)
-            return None, f"ERREUR: {error_msg}"
-        except Exception as e_generic:
+            return f"FUNC_ERROR: {error_msg}"
+        except Exception as e_generic: # Autres erreurs inattendues
             error_msg = f"Erreur inattendue lors de l'exécution de la requête PL: {str(e_generic)}"
             self._logger.error(error_msg, exc_info=True)
-            return None, f"ERREUR: {error_msg}"
-
-    def execute_pl_query(self, belief_set_content: str, query_string: str, constants: Optional[List[str]] = None) -> Tuple[Optional[bool], str]:
-        """
-        Exécute une requête en logique propositionnelle sur un ensemble de croyances donné.
-        Délègue l'exécution au PLHandler et retourne un tuple (résultat_bool, chaîne_formatée).
-        """
-        self._logger.info(f"TweetyBridge.execute_pl_query: Query='{query_string}' sur BS: ('{belief_set_content[:60]}...')")
-        # La méthode perform_pl_query est maintenant la méthode publique principale
-        return self.perform_pl_query(belief_set_content, query_string, constants)
-
-    def is_pl_kb_consistent(self, belief_set_content: str) -> Tuple[bool, str]:
-        """Vérifie la cohérence d'une base de connaissances propositionnelle."""
-        self._logger.info(f"TweetyBridge.is_pl_kb_consistent sur BS: ('{belief_set_content[:60]}...')")
-        if not self.is_jvm_ready() or not hasattr(self, '_pl_handler'):
-            return False, "TweetyBridge ou PLHandler non prêt."
-        try:
-            is_consistent = self._pl_handler.pl_check_consistency(belief_set_content)
-            return is_consistent, f"Consistency check returned: {is_consistent}"
-        except Exception as e:
-            self._logger.error(f"Erreur lors de la vérification de cohérence PL: {e}", exc_info=True)
-            return False, f"Error during PL consistency check: {e}"
-
+            return f"FUNC_ERROR: {error_msg}"
     # Les méthodes _parse_pl_formula, _parse_pl_belief_set, _execute_pl_query_internal
     # sont maintenant encapsulées dans PLHandler et peuvent être supprimées ici.
             
@@ -299,25 +282,33 @@ class TweetyBridge:
             self._logger.error(f"Erreur inattendue lors de la validation FOL de '{formula_string}': {e_generic}", exc_info=True)
             return False, f"Erreur FOL inattendue: {str(e_generic)}"
 
-    def validate_fol_belief_set(self, belief_set_string: str) -> Tuple[bool, str]:
+    def validate_fol_belief_set(self, belief_set_string: str, signature_declarations_str: Optional[str] = None) -> Tuple[bool, str]:
         """
-        Valide la syntaxe d'un ensemble de croyances FOL, qui doit inclure une ligne de signature.
-        Délègue le parsing complet au FOLHandler.
+        Valide la syntaxe d'un ensemble de croyances en logique du premier ordre (FOL).
+        Délègue la validation au FOLHandler.
         """
         if not self.is_jvm_ready() or not hasattr(self, '_fol_handler'):
             return False, "TweetyBridge ou FOLHandler non prêt."
 
         self._logger.debug(f"TweetyBridge.validate_fol_belief_set appelée pour BS: '{belief_set_string[:100]}...'")
         try:
-            # Délègue le parsing complet du belief set (signature + formules) au handler.
-            # La méthode parse_fol_belief_set lèvera une exception si le format est invalide.
-            self._fol_handler.parse_fol_belief_set(belief_set_string)
+            # Similaire à PL, FOLHandler devrait avoir une méthode pour valider la syntaxe du BS.
+            # En attendant, on parse chaque formule individuellement après nettoyage.
+            # FOLHandler.fol_check_consistency parse aussi, mais son but est la cohérence.
             
-            self._logger.info("Ensemble de croyances FOL validé avec succès par FOLHandler.")
+            # Solution temporaire : utiliser _remove_comments_and_empty_lines ici
+            # et parser chaque formule via le handler.
+            cleaned_formulas = self._remove_comments_and_empty_lines(belief_set_string)
+            if not cleaned_formulas:
+                return False, "Ensemble de croyances FOL vide ou ne contenant que des commentaires"
+
+            for formula_str in cleaned_formulas:
+                self._fol_handler.parse_fol_formula(formula_str, signature_declarations_str) # Lèvera ValueError
+
+            self._logger.info(f"Ensemble de croyances FOL validé avec succès par FOLHandler (parsing individuel).")
             return True, "Ensemble de croyances FOL valide"
             
-        except (ValueError, RuntimeError) as e_val:
-            # Attrape les erreurs de parsing spécifiques du handler.
+        except ValueError as e_val:
             self._logger.warning(f"Erreur de syntaxe dans le BS FOL détectée par FOLHandler: {e_val}")
             return False, f"Erreur de syntaxe FOL: {str(e_val)}"
         except Exception as e_generic:
@@ -361,36 +352,9 @@ class TweetyBridge:
             self._logger.error(error_msg, exc_info=True)
             return f"FUNC_ERROR: {error_msg}"
 
-    def is_fol_kb_consistent(self, belief_set_content: str, signature_declarations_str: Optional[str] = None) -> Tuple[bool, str]:
-        """Vérifie la cohérence d'une base de connaissances FOL."""
-        self._logger.info(f"TweetyBridge.is_fol_kb_consistent sur BS: ('{belief_set_content[:60]}...')")
-        if not self.is_jvm_ready() or not hasattr(self, '_fol_handler'):
-            return False, "TweetyBridge ou FOLHandler non prêt."
-        try:
-            is_consistent = self._fol_handler.fol_check_consistency(belief_set_content, signature_declarations_str)
-            return is_consistent, f"Consistency check returned: {is_consistent}"
-        except Exception as e:
-            self._logger.error(f"Erreur lors de la vérification de cohérence FOL: {e}", exc_info=True)
-            return False, f"Error during FOL consistency check: {e}"
-
     # Les méthodes _parse_fol_formula, _parse_fol_belief_set, _execute_fol_query_internal
     # sont maintenant encapsulées dans FOLHandler et peuvent être supprimées ici.
     
-    def validate_fol_query_with_context(self, belief_set_str: str, query_str: str) -> Tuple[bool, str]:
-        """
-        Valide une requête FOL en utilisant le contexte (signature) d'une base de connaissances.
-        Délègue la validation au FOLHandler.
-        """
-        if not self.is_jvm_ready() or not hasattr(self, '_fol_handler'):
-            return False, "TweetyBridge ou FOLHandler non prêt."
-
-        self._logger.debug(f"TweetyBridge.validate_fol_query_with_context appelée pour query: '{query_str}'")
-        try:
-            return self._fol_handler.validate_fol_query_with_context(belief_set_str, query_str)
-        except (ValueError, RuntimeError) as e:
-            self._logger.error(f"Erreur lors de la validation contextuelle de la requête FOL '{query_str}': {e}", exc_info=True)
-            return False, str(e)
-
     # --- Méthodes pour la logique modale ---
 
     def validate_modal_formula(self, formula_string: str, modal_logic_str: str = "S4", signature_declarations_str: Optional[str] = None) -> Tuple[bool, str]:
@@ -480,35 +444,6 @@ class TweetyBridge:
             self._logger.error(error_msg, exc_info=True)
             return f"FUNC_ERROR: {error_msg}"
 
-    def is_modal_kb_consistent(self, belief_set_content: str, modal_logic_str: str = "S4", signature_declarations_str: Optional[str] = None) -> Tuple[bool, str]:
-        """Vérifie la cohérence d'une base de connaissances modale."""
-        self._logger.info(f"TweetyBridge.is_modal_kb_consistent sur BS: ('{belief_set_content[:60]}...')")
-        if not self.is_jvm_ready() or not hasattr(self, '_modal_handler'):
-            return False, "TweetyBridge ou ModalHandler non prêt."
-        try:
-            is_consistent = self._modal_handler.modal_check_consistency(belief_set_content, modal_logic_str, signature_declarations_str)
-            return is_consistent, f"Consistency check returned: {is_consistent}"
-        except Exception as e:
-            self._logger.error(f"Erreur lors de la vérification de cohérence modale: {e}", exc_info=True)
-            return False, f"Error during Modal consistency check: {e}"
-
-    def validate_modal_query_with_context(self, belief_set_str: str, query_str: str, modal_logic_str: str = "S4") -> Tuple[bool, str]:
-        """
-        Valide une requête modale en utilisant le contexte d'une base de connaissances.
-        Délègue la validation au ModalHandler.
-        """
-        if not self.is_jvm_ready() or not hasattr(self, '_modal_handler'):
-            return False, "TweetyBridge ou ModalHandler non prêt."
-
-        self._logger.debug(f"TweetyBridge.validate_modal_query_with_context appelée pour query: '{query_str}', Logic: {modal_logic_str}")
-        try:
-            # Valider la requête en essayant de la parser avec le même contexte que le belief set
-            self._modal_handler.parse_modal_formula(query_str, modal_logic_str)
-            return True, "Requête modale valide dans le contexte"
-        except (ValueError, RuntimeError) as e:
-            self._logger.error(f"Erreur lors de la validation contextuelle de la requête modale '{query_str}': {e}", exc_info=True)
-            return False, str(e)
-
     # Les méthodes _parse_modal_formula, _parse_modal_belief_set, _execute_modal_query_internal
     # sont maintenant encapsulées dans ModalHandler et peuvent être supprimées ici.
 
diff --git a/argumentation_analysis/agents/core/logic/tweety_initializer.py b/argumentation_analysis/agents/core/logic/tweety_initializer.py
index ecc1f858..015f4e26 100644
--- a/argumentation_analysis/agents/core/logic/tweety_initializer.py
+++ b/argumentation_analysis/agents/core/logic/tweety_initializer.py
@@ -8,12 +8,15 @@ except ImportError:
 import jpype
 import logging
 from argumentation_analysis.utils.core_utils.logging_utils import setup_logging
+# from argumentation_analysis.utils.core_utils.path_operations import get_project_root # Différé
 from pathlib import Path
-import os
-import subprocess
+import os # Ajout de l'import os
 
-setup_logging("INFO")
-logger = logging.getLogger(__name__)
+# Initialisation du logger pour ce module.
+# setup_logging() est appelé pour configurer le logging global.
+# Il est important que setup_logging soit idempotent ou gère les appels multiples (ce qu'il fait avec force=True).
+setup_logging("INFO")  # Appel avec un niveau de log valide comme "INFO" ou selon la config souhaitée.
+logger = logging.getLogger(__name__) # Obtention correcte du logger pour ce module.
 
 class TweetyInitializer:
     """
@@ -48,87 +51,45 @@ class TweetyInitializer:
 
     def _start_jvm(self):
         """Starts the JVM and sets up the classpath."""
-        global logger
+        global logger # Assurer qu'on référence le logger du module
+        # Le logger devrait maintenant être initialisé correctement au niveau du module.
+        # Ce bloc if logger is None peut rester comme une double sécurité, mais ne devrait idéalement pas être atteint.
         if logger is None:
+            # Cela ne devrait plus se produire si l'initialisation au niveau du module est correcte.
             setup_logging("INFO")
             logger = logging.getLogger(__name__)
-            logger.error("CRITICAL: TweetyInitializer module logger was None and had to be re-initialized in _start_jvm.")
+            logger.error("CRITICAL: TweetyInitializer module logger was None and had to be re-initialized in _start_jvm. This indicates an issue in module loading or initial logger setup.")
 
         if TweetyInitializer._jvm_started:
             logger.info("JVM already started.")
             return
 
         try:
-            project_root = Path(__file__).resolve().parents[4]
+            # Importation dynamique de get_project_root UNIQUEMENT si on doit démarrer la JVM
+            from argumentation_analysis.utils.system_utils import get_project_root # Chemin corrigé
+            project_root = get_project_root()
             tweety_lib_path = project_root / "libs" / "tweety"
-            logger.info(f"Contenu de tweety_lib_path ({tweety_lib_path}):")
-            try:
-                for item in os.listdir(tweety_lib_path):
-                    logger.info(f"  - {item}")
-            except Exception as e_ls:
-                logger.error(f"    Impossible de lister {tweety_lib_path}: {e_ls}")
-
-            tweety_jar_file = tweety_lib_path / "org.tweetyproject.tweety-full-1.28-with-dependencies.jar"
-
-            classpath_entries = [tweety_jar_file]
+            
+            # Updated classpath based on previous successful runs
+            classpath_entries = [
+                tweety_lib_path / "tweety.jar",
+                # tweety_lib_path / "lib" / "*", # General libs - Répertoire vide, donc inutile pour l'instant
+            ]
+            
+            # Convert Path objects to strings for jpype
             classpath = [str(p) for p in classpath_entries]
             logger.info(f"Calculated Classpath: {classpath}")
 
             if not jpype.isJVMStarted():
                 logger.info("Starting JVM...")
-                try:
-                    java_home = os.environ.get('JAVA_HOME')
-                    jvm_path = None
-                    if java_home:
-                        java_home_path = Path(java_home)
-                        if not java_home_path.is_absolute():
-                            java_home_path = (project_root / java_home.lstrip('./')).resolve()
-                        
-                        logger.info(f"Chemin JAVA_HOME résolu: {java_home_path}")
-
-                        if os.name == 'nt':  # Windows
-                            possible_jvm_paths = [
-                                java_home_path / 'bin' / 'server' / 'jvm.dll',
-                                java_home_path / 'bin' / 'client' / 'jvm.dll',
-                                java_home_path / 'bin' / 'jvm.dll'
-                            ]
-                            for path in possible_jvm_paths:
-                                if path.exists():
-                                    jvm_path = str(path)
-                                    logger.info(f"Utilisation du JVM depuis JAVA_HOME: {jvm_path}")
-                                    break
-                            if not jvm_path:
-                                logger.warning(f"JVM introuvable dans les chemins standards de JAVA_HOME: {java_home_path}")
-                        else:  # Unix/Linux
-                            path = java_home_path / 'lib' / 'server' / 'libjvm.so'
-                            if path.exists():
-                                jvm_path = str(path)
-                                logger.info(f"Utilisation du JVM depuis JAVA_HOME: {jvm_path}")
-                            else:
-                                logger.warning(f"JVM introuvable dans JAVA_HOME: {path}")
-
-                    if not jvm_path:
-                        logger.info("Utilisation du JVM par défaut du système")
-                        jvm_path = jpype.getDefaultJVMPath()
-
-                    logger.info(f"Using JVM Path: {jvm_path}")
-                    
-                    jpype.startJVM(
-                        jvm_path,
-                        f"-Djava.class.path={os.pathsep.join(classpath)}",
-                        "-Xmx1g",
-                        "-Xms256m",
-                        "-XX:+UseG1GC",
-                        "-XX:MaxMetaspaceSize=256m",
-                        "-Dfile.encoding=UTF-8",
-                        convertStrings=False,
-                        ignoreUnrecognized=False
-                    )
-                    TweetyInitializer._jvm_started = True
-                    logger.info("JVM started successfully.")
-                except Exception as e:
-                    logger.error(f"Échec d'initialisation JVM: {e}")
-                    raise RuntimeError(f"Impossible d'initialiser la JVM: {e}") from e
+                jpype.startJVM(
+                    jpype.getDefaultJVMPath(),
+                    "-ea",
+                    f"-Djava.class.path={os.pathsep.join(classpath)}", # Utilisation de os.pathsep
+                    convertStrings=False
+                )
+                TweetyInitializer._jvm_started = True
+                logger.info("JVM started successfully.")
             else:
                 logger.info("JVM was already started by another component.")
                 TweetyInitializer._jvm_started = True
diff --git a/tests/agents/core/logic/test_tweety_bridge.py b/tests/agents/core/logic/test_tweety_bridge.py
index c48878c1..575fe08a 100644
--- a/tests/agents/core/logic/test_tweety_bridge.py
+++ b/tests/agents/core/logic/test_tweety_bridge.py
@@ -1,210 +1,767 @@
 ﻿# -*- coding: utf-8 -*-
 # tests/agents/core/logic/test_tweety_bridge.py
 """
-Tests pour la classe TweetyBridge, refactorisés pour pytest-asyncio.
+Tests unitaires pour la classe TweetyBridge.
 """
 
+import sys # Ajout de sys
 import os
-import pytest
-from unittest.mock import MagicMock, ANY
+from pathlib import Path # Ajout de Path
 
-# Imports authentiques (remplaçant les mocks)
-from config.unified_config import UnifiedConfig
+# Ajout pour forcer la reconnaissance du package principal
+current_script_path = Path(__file__).resolve()
+project_root_for_test = current_script_path.parents[4] # Remonter de tests/agents/core/logic à la racine du projet
+sys.path.insert(0, str(project_root_for_test))
+print(f"DEBUG: sys.path[0] in test_tweety_bridge.py set to: {str(project_root_for_test)}")
+
+import unittest
+from unittest.mock import MagicMock, patch, PropertyMock
+from tests.mocks.jpype_mock import JException as MockedJException
 
 from argumentation_analysis.agents.core.logic.tweety_bridge import TweetyBridge
-# L'import de MockedJException est supprimé car le mock jpype est maintenant géré par fixture.
-
-# --- Configuration des tests ---
-
-# Condition pour exécuter les tests nécessitant la vraie JVM
-REAL_JPYPE = os.environ.get('USE_REAL_JPYPE', 'false').lower() in ('true', '1')
-
-# --- Fonctions d'aide asynchrones ---
-
-async def _create_authentic_gpt4o_mini_instance():
-    """Crée une instance authentique de gpt-4o-mini."""
-    config = UnifiedConfig()
-    # Assurez-vous que cette méthode est asynchrone si elle effectue des I/O
-    return await config.get_kernel_with_gpt4o_mini()
-
-async def _make_authentic_llm_call(prompt: str) -> str:
-    """Fait un appel authentique à gpt-4o-mini."""
-    try:
-        kernel = await _create_authentic_gpt4o_mini_instance()
-        result = await kernel.invoke("chat", input=prompt)
-        return str(result)
-    except Exception as e:
-        print(f"Avertissement: Appel LLM authentique échoué: {e}")
-        return "Authentic LLM call failed"
-
-# --- Fixtures Pytest ---
-
-@pytest.fixture
-def mock_jpype_modules(mocker):
-    """Fixture pour mocker les modules jpype dans leurs contextes respectifs."""
-    if REAL_JPYPE:
-        yield None
-    else:
-        mock_jpype_tweety = mocker.patch('argumentation_analysis.agents.core.logic.tweety_bridge.jpype')
-        mock_jpype_jvm_setup = mocker.patch('argumentation_analysis.core.jvm_setup.jpype')
-
-        # Assurer la cohérence entre les mocks
-        for attr in ['isJVMStarted', 'JClass', 'startJVM', 'shutdownJVM']:
-            setattr(mock_jpype_jvm_setup, attr, getattr(mock_jpype_tweety, attr))
-        
-        # Définir JException directement sur le mock
-        mock_jpype_tweety.JException = Exception
-        setattr(mock_jpype_jvm_setup, 'JException', Exception)
-
-        yield mock_jpype_tweety
-
-
-@pytest.fixture
-def tweety_bridge_mocked(mock_jpype_modules):
-    """Fixture pour une instance de TweetyBridge avec mocks (cas non-REAL_JPYPE)."""
-    if REAL_JPYPE:
-        pytest.skip("Test spécifique aux mocks.")
-
-    mock_jpype_modules.isJVMStarted.return_value = True
-
-    # Mocks pour les classes Java
-    jclass_map = {
-        "org.tweetyproject.logics.pl.parser.PlParser": MagicMock(name="PlParser_class_mock"),
-        "org.tweetyproject.logics.pl.reasoner.SatReasoner": MagicMock(name="SatReasoner_class_mock"),
-        "org.tweetyproject.logics.pl.syntax.PlFormula": MagicMock(name="PlFormula_class_mock"),
-        "org.tweetyproject.logics.fol.parser.FolParser": MagicMock(name="FolParser_class_mock"),
-        "org.tweetyproject.logics.fol.reasoner.SimpleFolReasoner": MagicMock(name="SimpleFolReasoner_class_mock"),
-        "org.tweetyproject.logics.fol.syntax.FolFormula": MagicMock(name="FolFormula_class_mock"),
-        "org.tweetyproject.logics.ml.parser.MlParser": MagicMock(name="MlParser_class_mock"),
-        "org.tweetyproject.logics.ml.reasoner.SimpleMlReasoner": MagicMock(name="SimpleMlReasoner_class_mock"),
-        "org.tweetyproject.logics.ml.syntax.MlFormula": MagicMock(name="ModalFormula_class_mock")
-    }
-
-    def jclass_side_effect(class_name):
-        return jclass_map.get(class_name, MagicMock(name=f"Unknown_Class_{class_name}"))
+
+class TestTweetyBridge(unittest.TestCase):
+    """Tests pour la classe TweetyBridge."""
     
-    mock_jpype_modules.JClass.side_effect = jclass_side_effect
+    def setUp(self):
+        """Initialisation avant chaque test."""
+        # FORCER PROVISOIREMENT L'UTILISATION DES MOCKS pour débogage
+        # self.use_real_jpype = False # Commenté pour utiliser la variable d'environnement
+        self.use_real_jpype = os.environ.get('USE_REAL_JPYPE') == 'true' # Ligne originale décommentée
+        
+        self.jpype_patcher = None
+        self.mock_jpype = None # Mock pour jpype dans tweety_bridge lui-même
 
-    # Mocks pour les instances de classes
-    mock_pl_parser_instance = MagicMock(name="PlParser_instance_mock")
-    mock_sat_reasoner_instance = MagicMock(name="SatReasoner_instance_mock")
+        # Patchers pour TweetyInitializer et Handlers
+        self.tweety_initializer_patcher = None
+        self.mock_tweety_initializer_class = None # Mock de la classe TweetyInitializer
+        self.mock_tweety_initializer_instance = None # Mock de l'instance retournée par get_instance()
 
-    jclass_map["org.tweetyproject.logics.pl.parser.PlParser"].return_value = mock_pl_parser_instance
-    jclass_map["org.tweetyproject.logics.pl.reasoner.SatReasoner"].return_value = mock_sat_reasoner_instance
+        self.pl_handler_patcher = None
+        self.mock_pl_handler_class = None
+        self.mock_pl_handler_instance = None # Mock de l'instance de PLHandler
 
-    bridge = TweetyBridge()
+        self.fol_handler_patcher = None
+        self.mock_fol_handler_class = None
+        self.mock_fol_handler_instance = None # Mock de l'instance de FOLHandler
 
-    # Attacher les mocks à l'instance pour les assertions
-    bridge.mock_pl_parser_instance = mock_pl_parser_instance
-    bridge.mock_sat_reasoner_instance = mock_sat_reasoner_instance
-    bridge.mock_jpype = mock_jpype_modules
-    bridge.jclass_map = jclass_map
-    
-    return bridge
-
-@pytest.fixture
-async def tweety_bridge_real():
-    """Fixture pour une instance réelle de TweetyBridge (cas REAL_JPYPE)."""
-    if not REAL_JPYPE:
-        pytest.skip("Test nécessitant une vraie JVM.")
-    bridge = TweetyBridge()
-    return bridge
-
-# --- Tests ---
-
-@pytest.mark.skip(reason="Disabling flaky mock-based tweety tests to fix suite.")
-@pytest.mark.asyncio
-async def test_initialization_jvm_ready_mocked(tweety_bridge_mocked):
-    """Test de l'initialisation quand la JVM est prête (mock)."""
-    bridge = tweety_bridge_mocked
-    assert bridge.is_jvm_ready()
-    bridge.mock_jpype.JClass.assert_any_call("org.tweetyproject.logics.pl.parser.PlParser")
-    bridge.jclass_map["org.tweetyproject.logics.pl.parser.PlParser"].assert_called_once()
-
-def test_validate_formula_valid_mocked(tweety_bridge_mocked):
-    """Test de validation d'une formule propositionnelle valide (mock)."""
-    bridge = tweety_bridge_mocked
-    bridge._pl_handler._pl_parser.parseFormula.return_value = MagicMock()
-    
-    is_valid, message = bridge.validate_formula("a => b")
+        self.modal_handler_patcher = None
+        self.mock_modal_handler_class = None
+        self.mock_modal_handler_instance = None # Mock de l'instance de ModalHandler
+        
+        # Mocks pour les composants Java (parsers, reasoners) retournés par TweetyInitializer statique
+        self.mock_java_pl_parser = MagicMock(name="MockJavaPlParser")
+        self.mock_java_pl_reasoner = MagicMock(name="MockJavaPlReasoner")
+        self.mock_java_fol_parser = MagicMock(name="MockJavaFolParser")
+        # self.mock_java_fol_reasoner = MagicMock(name="MockJavaFolReasoner") # Si FOLHandler l'utilise
+        self.mock_java_modal_parser = MagicMock(name="MockJavaModalParser")
+        # self.mock_java_modal_reasoner = MagicMock(name="MockJavaModalReasoner") # Si ModalHandler l'utilise
+
+        if not self.use_real_jpype:
+            # 1. Patcher jpype dans tweety_bridge (principalement pour JException)
+            self.jpype_patcher = patch('argumentation_analysis.agents.core.logic.tweety_bridge.jpype')
+            self.mock_jpype = self.jpype_patcher.start()
+            self.mock_jpype.JException = MockedJException
+
+            # 2. Patcher TweetyInitializer
+            #    Patch la classe TweetyInitializer là où elle est importée par TweetyBridge et les Handlers.
+            #    Nous devons patcher `get_instance` et les méthodes statiques `get_..._parser/reasoner`.
+            self.tweety_initializer_patcher = patch('argumentation_analysis.agents.core.logic.tweety_bridge.TweetyInitializer')
+            self.mock_tweety_initializer_class = self.tweety_initializer_patcher.start()
+
+            # Configurer le mock de la classe TweetyInitializer
+            self.mock_tweety_initializer_instance = MagicMock(name="MockTweetyInitializerInstance")
+            # self.mock_tweety_initializer_class.get_instance.return_value = self.mock_tweety_initializer_instance # Supprimé car get_instance n'est plus utilisé
+            self.mock_tweety_initializer_class.return_value = self.mock_tweety_initializer_instance # Assure que l'appel de la classe mockée retourne notre instance mockée
+            
+            # Configurer les méthodes de l'instance mockée de TweetyInitializer
+            self.mock_tweety_initializer_instance.is_jvm_started.return_value = True # Corrigé pour is_jvm_started
+            self.mock_tweety_initializer_instance.initialize_pl_components = MagicMock()
+            self.mock_tweety_initializer_instance.initialize_fol_components = MagicMock()
+            self.mock_tweety_initializer_instance.initialize_modal_components = MagicMock()
+            self.mock_tweety_initializer_instance.start_jvm_and_initialize = MagicMock()
+
+
+            # Configurer les méthodes statiques mockées de TweetyInitializer (utilisées par les Handlers)
+            # Ces mocks seront utilisés lorsque les vrais Handlers (ou leurs mocks) appellent TweetyInitializer.get_...
+            # Pour que cela fonctionne, il faut aussi patcher TweetyInitializer dans pl_handler.py, fol_handler.py, modal_handler.py
+            # ou s'assurer que le patch ici est global. Pour l'instant, on se concentre sur le patch dans tweety_bridge.
+            # Si les Handlers sont aussi mockés (voir ci-dessous), leurs __init__ ne seront pas appelés par défaut,
+            # sauf si on configure le mock de la classe Handler pour appeler l'original ou si on teste les vrais Handlers.
+            
+            # Pour simplifier, nous allons patcher les méthodes statiques directement sur self.mock_tweety_initializer_class
+            # car c'est ce que les handlers vont référencer s'ils importent TweetyInitializer du même module.
+            # Si les handlers importent `from .tweety_initializer import TweetyInitializer`, il faudra patcher là-bas aussi.
+            # Pour l'instant, on suppose que le patch dans tweety_bridge est suffisant ou que les handlers sont mockés.
+            
+            self.mock_tweety_initializer_class.get_pl_parser.return_value = self.mock_java_pl_parser
+            self.mock_tweety_initializer_class.get_pl_reasoner.return_value = self.mock_java_pl_reasoner
+            self.mock_tweety_initializer_class.get_fol_parser.return_value = self.mock_java_fol_parser
+            self.mock_tweety_initializer_class.get_modal_parser.return_value = self.mock_java_modal_parser
+            # Ajouter get_fol_reasoner, get_modal_reasoner si nécessaire
+
+            # 3. Patcher les Handlers
+            self.pl_handler_patcher = patch('argumentation_analysis.agents.core.logic.tweety_bridge.PLHandler')
+            self.mock_pl_handler_class = self.pl_handler_patcher.start()
+            self.mock_pl_handler_instance = MagicMock(name="MockPLHandlerInstance")
+            self.mock_pl_handler_class.return_value = self.mock_pl_handler_instance # TweetyBridge obtiendra ce mock lors de l'instanciation
+
+            self.fol_handler_patcher = patch('argumentation_analysis.agents.core.logic.tweety_bridge.FOLHandler')
+            self.mock_fol_handler_class = self.fol_handler_patcher.start()
+            self.mock_fol_handler_instance = MagicMock(name="MockFOLHandlerInstance")
+            self.mock_fol_handler_class.return_value = self.mock_fol_handler_instance
+
+            self.modal_handler_patcher = patch('argumentation_analysis.agents.core.logic.tweety_bridge.ModalHandler')
+            self.mock_modal_handler_class = self.modal_handler_patcher.start()
+            self.mock_modal_handler_instance = MagicMock(name="MockModalHandlerInstance")
+            self.mock_modal_handler_class.return_value = self.mock_modal_handler_instance
+            
+            # Initialiser TweetyBridge APRÈS que tous les patchs sont en place
+            self.tweety_bridge = TweetyBridge()
+            
+        else: # self.use_real_jpype is True
+            self.tweety_bridge = TweetyBridge()
+
+    def tearDown(self):
+        """Nettoyage après chaque test."""
+        if self.jpype_patcher:
+            self.jpype_patcher.stop()
+        if self.tweety_initializer_patcher:
+            self.tweety_initializer_patcher.stop()
+        if self.pl_handler_patcher:
+            self.pl_handler_patcher.stop()
+        if self.fol_handler_patcher:
+            self.fol_handler_patcher.stop()
+        if self.modal_handler_patcher:
+            self.modal_handler_patcher.stop()
     
-    bridge._pl_handler._pl_parser.parseFormula.assert_called_once_with(ANY)
-    assert is_valid
-    assert message == "Formule valide"
-
-def test_validate_formula_invalid_mocked(tweety_bridge_mocked):
-    """Test de validation d'une formule propositionnelle invalide (mock)."""
-    bridge = tweety_bridge_mocked
-    # Utilise JException depuis le mock jpype fourni par la fixture
-    java_exception_instance = bridge.mock_jpype.JException("Erreur de syntaxe")
-    bridge._pl_handler._pl_parser.parseFormula.side_effect = java_exception_instance
+    def test_initialization_jvm_ready(self):
+        """Test de l'initialisation lorsque la JVM est prête."""
+        if not self.use_real_jpype:
+            # TweetyBridge instancie TweetyInitializer directement dans son __init__
+            # donc on vérifie que la classe mockée TweetyInitializer a été appelée pour créer une instance.
+            # TweetyBridge passe `self` (l'instance de TweetyBridge) à TweetyInitializer.
+            # L'instance de TweetyBridge est self.tweety_bridge, créée dans setUp.
+            self.mock_tweety_initializer_class.assert_called_once_with(self.tweety_bridge)
+            
+            # Vérifier que is_jvm_started a été appelé sur l'instance retournée
+            # L'instance est self.mock_tweety_initializer_instance car mock_tweety_initializer_class.return_value est configuré ainsi.
+            self.assertIs(self.tweety_bridge._initializer, self.mock_tweety_initializer_instance, "TweetyBridge._initializer n'est pas l'instance mockée attendue.")
+            self.mock_tweety_initializer_instance.is_jvm_started.assert_called_once()
+            
+            # Vérifier que start_jvm_and_initialize n'a PAS été appelé car is_jvm_started est mocké pour retourner True
+            self.mock_tweety_initializer_instance.start_jvm_and_initialize.assert_not_called()
+
+            # Vérifier que les méthodes d'initialisation des composants ont été appelées sur l'instance mockée
+            self.mock_tweety_initializer_instance.initialize_pl_components.assert_called_once()
+            self.mock_tweety_initializer_instance.initialize_fol_components.assert_called_once()
+            self.mock_tweety_initializer_instance.initialize_modal_components.assert_called_once()
+            
+            # Vérifier que les constructeurs des Handlers ont été appelés
+            self.mock_pl_handler_class.assert_called_once()
+            self.mock_fol_handler_class.assert_called_once()
+            self.mock_modal_handler_class.assert_called_once()
+            
+            # Vérifier que TweetyBridge considère la JVM comme prête
+            self.assertTrue(self.tweety_bridge.is_jvm_ready())
+        else:
+            # Pour le cas réel, on vérifie juste que l'initialisation ne lève pas d'erreur
+            # et que is_jvm_ready retourne True.
+            self.assertTrue(self.tweety_bridge.is_jvm_ready(), "TweetyBridge devrait être prêt avec la vraie JVM.")
+            # Des assertions plus spécifiques sur les instances réelles pourraient être ajoutées si nécessaire,
+            # mais cela relèverait plus de tests d'intégration pour les handlers/initializer.
+            self.assertIsNotNone(self.tweety_bridge._initializer)
+            self.assertIsNotNone(self.tweety_bridge._pl_handler)
+            self.assertIsNotNone(self.tweety_bridge._fol_handler)
+            self.assertIsNotNone(self.tweety_bridge._modal_handler)
+
+    def test_initialization_jvm_not_ready(self):
+        """Test de l'initialisation lorsque la JVM n'est pas prête."""
+        if not self.use_real_jpype:
+            # Configurer le mock de TweetyInitializer pour simuler un échec d'initialisation de la JVM
+            # 1. is_jvm_ready() retourne False initialement
+            # 2. start_jvm_and_initialize() est appelée
+            # 3. is_jvm_ready() retourne toujours False après l'appel à start_jvm_and_initialize
+            
+            # Réinitialiser les mocks pour ce test spécifique
+            self.mock_tweety_initializer_class.reset_mock()
+            self.mock_pl_handler_class.reset_mock()
+            self.mock_fol_handler_class.reset_mock()
+            self.mock_modal_handler_class.reset_mock()
+            # Ne pas réinitialiser self.mock_tweety_initializer_instance ici, car nous allons en créer un local.
+ 
+            # Créer et configurer une instance de mock locale pour TweetyInitializer
+            local_mock_initializer_instance = MagicMock(name="LocalMockTweetyInitializerInstance")
+            # is_jvm_started est appelée une première fois. Si False, TweetyBridge loggue.
+            # Puis is_jvm_started est appelée une seconde fois. Si toujours False, TweetyBridge lève une exception.
+            local_mock_initializer_instance.is_jvm_started.side_effect = [False, False]
+            
+            # La méthode start_jvm_and_initialize n'est PAS appelée par TweetyBridge sur l'instance.
+            # Le démarrage de la JVM est géré DANS TweetyInitializer.__init__().
+            # Si TweetyInitializer.__init__() échoue à démarrer la JVM, is_jvm_started() devrait refléter cela.
+            local_mock_initializer_instance.start_jvm_and_initialize = MagicMock(name="MockStartJVMAndInitialize_SHOULD_NOT_BE_CALLED_BY_BRIDGE")
+
+            # Assurer que l'instanciation de TweetyInitializer dans TweetyBridge utilise cette instance mockée locale
+            self.mock_tweety_initializer_class.return_value = local_mock_initializer_instance
     
-    is_valid, message = bridge.validate_formula("a ==> b")
+            # S'attendre à une RuntimeError lors de l'instanciation de TweetyBridge
+            with self.assertRaisesRegex(RuntimeError, "TweetyBridge ne peut pas fonctionner sans une JVM initialisée."):
+                TweetyBridge()
     
-    bridge._pl_handler._pl_parser.parseFormula.assert_called_once_with(ANY)
-    assert not is_valid
-    # Le message peut varier un peu, on vérifie la sous-chaine
-    assert "Erreur de syntaxe" in message
-
-def test_execute_pl_query_accepted_mocked(tweety_bridge_mocked):
-    """Test d'exécution d'une requête PL acceptée (mock)."""
-    bridge = tweety_bridge_mocked
+            # Vérifications des appels
+            # TweetyInitializer est instancié une fois.
+            self.mock_tweety_initializer_class.assert_called_once_with(unittest.mock.ANY) # ANY car l'instance de TweetyBridge est créée dans le with block
     
-    mock_kb_formula = MagicMock(name="mock_kb_formula")
-    mock_query_formula = MagicMock(name="mock_query_formula")
-
-    def parse_formula_side_effect(formula_str):
-        if "=>" in formula_str:
-            return mock_kb_formula
+            # is_jvm_started devrait être appelée deux fois par TweetyBridge.__init__
+            self.assertEqual(local_mock_initializer_instance.is_jvm_started.call_count, 2, "is_jvm_started devrait être appelée deux fois")
+            
+            # start_jvm_and_initialize ne devrait PAS être appelée par TweetyBridge sur l'instance d'initializer
+            local_mock_initializer_instance.start_jvm_and_initialize.assert_not_called()
+            
+            # Les méthodes d'initialisation des composants ne devraient pas être appelées si la JVM est considérée comme non démarrée
+            local_mock_initializer_instance.initialize_pl_components.assert_not_called()
+            local_mock_initializer_instance.initialize_fol_components.assert_not_called()
+            local_mock_initializer_instance.initialize_modal_components.assert_not_called()
+
+            # Les handlers ne devraient pas être initialisés si la JVM échoue
+            self.mock_pl_handler_class.assert_not_called()
+            self.mock_fol_handler_class.assert_not_called()
+            self.mock_modal_handler_class.assert_not_called()
+        else:
+            self.skipTest("Ce test est spécifique au cas mocké où le démarrage de la JVM peut être simulé comme échoué.")
+
+    def test_validate_formula_valid(self):
+        """Test de la validation d'une formule propositionnelle valide."""
+        formula_str = "a => b"
+        if not self.use_real_jpype:
+            # Configurer le mock de PLHandler pour ne pas lever d'exception lors du parsing
+            self.mock_pl_handler_instance.parse_pl_formula.return_value = MagicMock(name="ParsedPlFormulaMock") # Simule un objet formule parsé
+
+            is_valid, message = self.tweety_bridge.validate_formula(formula_str)
+            
+            # Vérifier que la méthode du handler a été appelée
+            self.mock_pl_handler_instance.parse_pl_formula.assert_called_once_with(formula_str)
+            self.assertTrue(is_valid)
+            self.assertEqual(message, "Formule valide")
         else:
-            return mock_query_formula
+            is_valid, message = self.tweety_bridge.validate_formula(formula_str)
+            self.assertTrue(is_valid, f"La formule '{formula_str}' devrait être valide avec la vraie JVM. Message: {message}")
+            self.assertEqual(message, "Formule valide", f"Message inattendu pour '{formula_str}' avec la vraie JVM. Reçu: {message}")
     
-    bridge._pl_handler._pl_parser.parseFormula.side_effect = parse_formula_side_effect
-    bridge._pl_handler._pl_reasoner.query.return_value = True
-    bridge.mock_jpype.JObject = lambda x, target: target(x) # Simule la conversion de type
+    def test_validate_formula_invalid(self):
+        """Test de la validation d'une formule propositionnelle invalide."""
+        formula_str = "a ==> b"
+        error_detail = "Détail de l'erreur de syntaxe du handler"
+        if not self.use_real_jpype:
+            # Configurer le mock de PLHandler pour lever une ValueError lors du parsing
+            self.mock_pl_handler_instance.parse_pl_formula.side_effect = ValueError(error_detail)
+            
+            is_valid, message = self.tweety_bridge.validate_formula(formula_str)
+            
+            # Vérifier que la méthode du handler a été appelée
+            self.mock_pl_handler_instance.parse_pl_formula.assert_called_once_with(formula_str)
+            
+            self.assertFalse(is_valid)
+            self.assertEqual(message, f"Erreur de syntaxe: {error_detail}")
+        else:
+            # Valider une formule invalide avec la vraie JVM
+            is_valid, message = self.tweety_bridge.validate_formula(formula_str)
+            
+            self.assertFalse(is_valid, f"La formule '{formula_str}' devrait être invalide avec la vraie JVM.")
+            self.assertTrue(message, "Le message d'erreur ne devrait pas être vide pour une formule invalide.")
+            # Le message exact peut varier, mais il devrait indiquer une erreur de syntaxe.
+            self.assertIn("syntax", message.lower(), f"Le message d'erreur '{message}' devrait contenir 'syntax' pour '{formula_str}'.")
+            
+            # Test avec une autre formule invalide
+            formula_complex = "p1 & (p2 | )"
+            is_valid_complex, message_complex = self.tweety_bridge.validate_formula(formula_complex) # Erreur de syntaxe
+            self.assertFalse(is_valid_complex, f"La formule '{formula_complex}' devrait être invalide.")
+            self.assertIn("syntax", message_complex.lower(), f"Le message d'erreur '{message_complex}' devrait contenir 'syntax' pour '{formula_complex}'.")
+
+    def test_validate_belief_set_valid(self):
+        """Test de la validation d'un ensemble de croyances propositionnelles valide."""
+        belief_set_str = "a => b; c" # TweetyBridge._remove_comments_and_empty_lines enlèvera le point final s'il y en a un.
+        cleaned_formulas = ["a => b", "c"] # Ce que _remove_comments_and_empty_lines devrait retourner
+
+        if not self.use_real_jpype:
+            # Configurer le mock de PLHandler pour ne pas lever d'exception lors du parsing
+            self.mock_pl_handler_instance.parse_pl_formula.return_value = MagicMock(name="ParsedPlFormulaMock")
+            
+            # On peut aussi mocker _remove_comments_and_empty_lines pour contrôler son output
+            with patch.object(self.tweety_bridge, '_remove_comments_and_empty_lines', return_value=cleaned_formulas) as mock_remove_lines:
+                is_valid, message = self.tweety_bridge.validate_belief_set(belief_set_str)
+            
+            mock_remove_lines.assert_called_once_with(belief_set_str)
+            
+            # Vérifier que la méthode du handler a été appelée pour chaque formule nettoyée
+            self.assertEqual(self.mock_pl_handler_instance.parse_pl_formula.call_count, len(cleaned_formulas))
+            for formula in cleaned_formulas:
+                self.mock_pl_handler_instance.parse_pl_formula.assert_any_call(formula)
+            
+            self.assertTrue(is_valid)
+            self.assertEqual(message, "Ensemble de croyances valide")
+        else:
+            # Valider un ensemble de croyances valide avec la vraie JVM
+            is_valid, message = self.tweety_bridge.validate_belief_set("a => b; c.") # Avec un point final
+            self.assertTrue(is_valid, f"L'ensemble 'a => b; c.' devrait être valide. Message: {message}")
+            self.assertEqual(message, "Ensemble de croyances valide", f"Message inattendu. Reçu: {message}")
+
+            is_valid_nocomment, message_nocomment = self.tweety_bridge.validate_belief_set("p1; p2 || p3")
+            self.assertTrue(is_valid_nocomment, f"L'ensemble 'p1; p2 || p3' devrait être valide. Message: {message_nocomment}")
+            self.assertEqual(message_nocomment, "Ensemble de croyances valide")
+
+    def test_validate_belief_set_empty(self):
+        """Test de la validation d'un ensemble de croyances propositionnelles vide."""
+        belief_set_empty_str = ""
+        belief_set_comment_str = "% commentaire seul"
+        expected_message = "Ensemble de croyances vide ou ne contenant que des commentaires"
+
+        if not self.use_real_jpype:
+            # Test avec chaîne vide
+            with patch.object(self.tweety_bridge, '_remove_comments_and_empty_lines', return_value=[]) as mock_remove_lines_empty:
+                is_valid_empty, message_empty = self.tweety_bridge.validate_belief_set(belief_set_empty_str)
+            
+            mock_remove_lines_empty.assert_called_once_with(belief_set_empty_str)
+            self.mock_pl_handler_instance.parse_pl_formula.assert_not_called() # Ne devrait pas être appelé si la liste est vide
+            self.assertFalse(is_valid_empty)
+            self.assertEqual(message_empty, expected_message)
+            
+            self.mock_pl_handler_instance.parse_pl_formula.reset_mock() # Réinitialiser pour le prochain test
+
+            # Test avec seulement des commentaires
+            with patch.object(self.tweety_bridge, '_remove_comments_and_empty_lines', return_value=[]) as mock_remove_lines_comment:
+                is_valid_comment, message_comment = self.tweety_bridge.validate_belief_set(belief_set_comment_str)
+
+            mock_remove_lines_comment.assert_called_once_with(belief_set_comment_str)
+            self.mock_pl_handler_instance.parse_pl_formula.assert_not_called()
+            self.assertFalse(is_valid_comment)
+            self.assertEqual(message_comment, expected_message)
+        else:
+            # Valider un ensemble de croyances vide avec la vraie JVM
+            is_valid, message = self.tweety_bridge.validate_belief_set(belief_set_empty_str)
+            self.assertFalse(is_valid, "Un ensemble vide devrait retourner False.")
+            self.assertEqual(message, expected_message)
+            
+            is_valid_comment, message_comment = self.tweety_bridge.validate_belief_set(belief_set_comment_str)
+            self.assertFalse(is_valid_comment, "Un ensemble avec seulement des commentaires devrait retourner False.")
+            self.assertEqual(message_comment, expected_message)
+
+    def test_validate_belief_set_invalid(self):
+        """Test de la validation d'un ensemble de croyances propositionnelles invalide."""
+        belief_set_str = "a => b; a ==> c" # La deuxième formule est invalide
+        cleaned_formulas = ["a => b", "a ==> c"]
+        error_detail = "Erreur sur a ==> c"
+
+        if not self.use_real_jpype:
+            # Configurer le mock de PLHandler pour lever une ValueError sur la deuxième formule
+            self.mock_pl_handler_instance.parse_pl_formula.side_effect = [
+                MagicMock(name="ParsedOkFormula"), # Pour "a => b"
+                ValueError(error_detail)          # Pour "a ==> c"
+            ]
+            
+            with patch.object(self.tweety_bridge, '_remove_comments_and_empty_lines', return_value=cleaned_formulas) as mock_remove_lines:
+                is_valid, message = self.tweety_bridge.validate_belief_set(belief_set_str)
+            
+            mock_remove_lines.assert_called_once_with(belief_set_str)
+            
+            # parse_pl_formula devrait être appelée pour "a => b" et "a ==> c"
+            self.assertEqual(self.mock_pl_handler_instance.parse_pl_formula.call_count, 2)
+            self.mock_pl_handler_instance.parse_pl_formula.assert_any_call("a => b")
+            self.mock_pl_handler_instance.parse_pl_formula.assert_any_call("a ==> c")
+            
+            self.assertFalse(is_valid)
+            self.assertEqual(message, f"Erreur de syntaxe: {error_detail}")
+        else:
+            # Valider un ensemble de croyances invalide avec la vraie JVM
+            is_valid, message = self.tweety_bridge.validate_belief_set("a ==>; c.")
+            self.assertFalse(is_valid, "L'ensemble 'a ==>; c.' devrait être invalide.")
+            self.assertTrue(message, "Le message d'erreur pour un ensemble invalide ne devrait pas être vide.")
+            self.assertIn("syntax", message.lower(), f"Le message d'erreur '{message}' devrait contenir 'syntax'.")
+
+    def test_execute_pl_query_accepted(self):
+        """Test de l'exécution d'une requête propositionnelle acceptée."""
+        belief_set_content = "a; a=>b"
+        query_string = "b"
+        
+        if not self.use_real_jpype:
+            # Configurer le mock de PLHandler pour retourner True
+            self.mock_pl_handler_instance.pl_query.return_value = True
+            
+            result = self.tweety_bridge.execute_pl_query(belief_set_content, query_string)
+            
+            # Vérifier que la méthode du handler a été appelée
+            self.mock_pl_handler_instance.pl_query.assert_called_once_with(belief_set_content, query_string)
+            
+            self.assertIn(f"Query '{query_string}' is ACCEPTED (True)", result)
+        else:
+            # Exécuter une requête acceptée avec la vraie JVM
+            result = self.tweety_bridge.execute_pl_query(belief_set_content, query_string) # KB: a, a implies b. Query: b.
+            self.assertIn("ACCEPTED (True)", result, f"Query '{query_string}' from '{belief_set_content}' should be ACCEPTED (True). Result: {result}")
+            
+            belief_set_complex = "p1; p2; (p1 && p2) => q"
+            query_complex = "q"
+            result_complex = self.tweety_bridge.execute_pl_query(belief_set_complex, query_complex)
+            self.assertIn("ACCEPTED (True)", result_complex, f"Query '{query_complex}' from '{belief_set_complex}' should be ACCEPTED (True). Result: {result_complex}")
+
+    def test_execute_pl_query_rejected(self):
+        """Test de l'exécution d'une requête propositionnelle rejetée."""
+        belief_set_content = "a; a=>b"
+        query_string = "c"
+
+        if not self.use_real_jpype:
+            # Configurer le mock de PLHandler pour retourner False
+            self.mock_pl_handler_instance.pl_query.return_value = False
+            
+            result = self.tweety_bridge.execute_pl_query(belief_set_content, query_string)
+            
+            # Vérifier que la méthode du handler a été appelée
+            self.mock_pl_handler_instance.pl_query.assert_called_once_with(belief_set_content, query_string)
+            
+            self.assertIn(f"Query '{query_string}' is REJECTED (False)", result)
+        else:
+            # Exécuter une requête rejetée avec la vraie JVM
+            result = self.tweety_bridge.execute_pl_query(belief_set_content, query_string) # KB: a, a implies b. Query: c.
+            self.assertIn("REJECTED (False)", result, f"Query '{query_string}' from '{belief_set_content}' should be REJECTED (False). Result: {result}")
+
+    def test_execute_pl_query_error(self):
+        """Test de l'exécution d'une requête propositionnelle avec erreur."""
+        belief_set_content_invalid = "a ==> b" # Erreur de syntaxe dans le BS
+        query_string = "a"
+        error_detail_handler = "Erreur de parsing du handler pour le BS"
+
+        if not self.use_real_jpype:
+            # Configurer le mock de PLHandler pour lever une ValueError
+            self.mock_pl_handler_instance.pl_query.side_effect = ValueError(error_detail_handler)
+            
+            result = self.tweety_bridge.execute_pl_query(belief_set_content_invalid, query_string)
+            
+            # Vérifier que la méthode du handler a été appelée
+            self.mock_pl_handler_instance.pl_query.assert_called_once_with(belief_set_content_invalid, query_string)
+            
+            self.assertIn("FUNC_ERROR", result)
+            # Le message d'erreur de TweetyBridge inclut le message de l'exception du handler
+            expected_error_message_part = f"Erreur lors de l'exécution de la requête PL via PLHandler: {error_detail_handler}"
+            self.assertIn(expected_error_message_part, result)
+        else:
+            # Exécuter une requête avec erreur de syntaxe dans la base avec la vraie JVM
+            result = self.tweety_bridge.execute_pl_query(belief_set_content_invalid, query_string)
+            self.assertIn("FUNC_ERROR", result, f"Query with syntax error in KB '{belief_set_content_invalid}' should be FUNC_ERROR. Result: {result}")
+            self.assertTrue(result) # S'assurer que le message d'erreur n'est pas vide
+            # Le message exact peut varier, mais il devrait indiquer une erreur.
+            self.assertTrue("error" in result.lower() or "exception" in result.lower() or "parsing" in result.lower() or "syntaxe" in result.lower(), f"Error message '{result}' should contain relevant keywords.")
+
+    def test_validate_fol_formula(self):
+        """Test de la validation d'une formule du premier ordre."""
+        formula_valid_str = "forall X: (p(X) => q(X))"
+        formula_invalid_str = "forall X: p(X) &"
+        signature_str = "sort person; predicate p(person); predicate q(person);" # Exemple de signature
+        error_detail_handler = "Détail erreur syntaxe FOL handler"
+
+        if not self.use_real_jpype:
+            # Test cas valide
+            self.mock_fol_handler_instance.parse_fol_formula.return_value = MagicMock(name="ParsedFolFormulaMock")
+            is_valid, message = self.tweety_bridge.validate_fol_formula(formula_valid_str, signature_str)
+            self.mock_fol_handler_instance.parse_fol_formula.assert_called_once_with(formula_valid_str, signature_str)
+            self.assertTrue(is_valid)
+            self.assertEqual(message, "Formule FOL valide")
+            self.mock_fol_handler_instance.parse_fol_formula.reset_mock()
+
+            # Test cas invalide
+            self.mock_fol_handler_instance.parse_fol_formula.side_effect = ValueError(error_detail_handler)
+            is_valid_invalid, message_invalid = self.tweety_bridge.validate_fol_formula(formula_invalid_str, signature_str)
+            self.mock_fol_handler_instance.parse_fol_formula.assert_called_once_with(formula_invalid_str, signature_str)
+            self.assertFalse(is_valid_invalid)
+            self.assertEqual(message_invalid, f"Erreur de syntaxe FOL: {error_detail_handler}")
+        else:
+            # Valider une formule FOL valide avec la vraie JVM
+            # Note: Pour FOL, la validité peut dépendre de la déclaration des prédicats/sorts.
+            # TweetyBridge.validate_fol_formula ne prend pas de signature pour l'instant,
+            # mais le handler oui. Les tests ici doivent refléter cela.
+            # Pour un test simple, on utilise une formule qui ne dépend pas de prédicats complexes.
+            is_valid_real, message_real = self.tweety_bridge.validate_fol_formula("forall X: (p(X) => p(X))") # Pas de signature passée ici
+            self.assertTrue(is_valid_real, f"FOL Formula 'forall X: (p(X) => p(X))' should be valid. Message: {message_real}")
+            self.assertEqual(message_real, "Formule FOL valide", f"Message inattendu. Reçu: {message_real}")
+            
+            # Valider une formule FOL invalide avec la vraie JVM
+            is_valid_invalid_real, message_invalid_real = self.tweety_bridge.validate_fol_formula(formula_invalid_str)
+            self.assertFalse(is_valid_invalid_real, f"FOL Formula '{formula_invalid_str}' should be invalid. Message: {message_invalid_real}")
+            self.assertTrue(message_invalid_real)
+            self.assertTrue("syntax" in message_invalid_real.lower() or "error" in message_invalid_real.lower(), f"Message d'erreur '{message_invalid_real}' devrait contenir 'syntax' or 'error'.")
+
+    def test_validate_modal_formula(self):
+        """Test de la validation d'une formule modale."""
+        formula_valid_str = "[]p => <>q"
+        formula_invalid_str = "[]p => <>" # Erreur de syntaxe
+        modal_logic = "S4"
+        error_detail_handler = "Détail erreur syntaxe Modale handler"
+
+        if not self.use_real_jpype:
+            # Test cas valide
+            self.mock_modal_handler_instance.parse_modal_formula.return_value = MagicMock(name="ParsedModalFormulaMock")
+            is_valid, message = self.tweety_bridge.validate_modal_formula(formula_valid_str, modal_logic)
+            self.mock_modal_handler_instance.parse_modal_formula.assert_called_once_with(formula_valid_str, modal_logic)
+            self.assertTrue(is_valid)
+            self.assertEqual(message, "Formule Modale valide")
+            self.mock_modal_handler_instance.parse_modal_formula.reset_mock()
+
+            # Test cas invalide
+            self.mock_modal_handler_instance.parse_modal_formula.side_effect = ValueError(error_detail_handler)
+            is_valid_invalid, message_invalid = self.tweety_bridge.validate_modal_formula(formula_invalid_str, modal_logic)
+            self.mock_modal_handler_instance.parse_modal_formula.assert_called_once_with(formula_invalid_str, modal_logic)
+            self.assertFalse(is_valid_invalid)
+            self.assertEqual(message_invalid, f"Erreur de syntaxe Modale: {error_detail_handler}")
+        else:
+            # Valider une formule modale valide avec la vraie JVM
+            is_valid_real, message_real = self.tweety_bridge.validate_modal_formula("[] (prop1) => <> (prop1)", modal_logic)
+            self.assertTrue(is_valid_real, f"Modal formula '[] (prop1) => <> (prop1)' (Logic: {modal_logic}) should be valid. Message: {message_real}")
+            self.assertEqual(message_real, "Formule Modale valide", f"Message inattendu. Reçu: {message_real}")
+
+            # Valider une formule modale invalide avec la vraie JVM
+            is_valid_invalid_real, message_invalid_real = self.tweety_bridge.validate_modal_formula(formula_invalid_str, modal_logic)
+            self.assertFalse(is_valid_invalid_real, f"Modal formula '{formula_invalid_str}' (Logic: {modal_logic}) should be invalid. Message: {message_invalid_real}")
+            self.assertTrue(message_invalid_real)
+            self.assertTrue("syntax" in message_invalid_real.lower() or "error" in message_invalid_real.lower(), f"Message d'erreur '{message_invalid_real}' devrait contenir 'syntax' or 'error'.")
+
+    # --- Tests pour FOL Belief Set ---
+    def test_validate_fol_belief_set_valid(self):
+        """Test de la validation d'un ensemble de croyances FOL valide."""
+        belief_set_str = "forall X: p(X); exists Y: q(Y)"
+        cleaned_formulas = ["forall X: p(X)", "exists Y: q(Y)"]
+        signature_str = "sort T; predicate p(T); predicate q(T);"
+
+        if not self.use_real_jpype:
+            self.mock_fol_handler_instance.parse_fol_formula.return_value = MagicMock(name="ParsedFolFormulaMock")
+            with patch.object(self.tweety_bridge, '_remove_comments_and_empty_lines', return_value=cleaned_formulas) as mock_remove_lines:
+                is_valid, message = self.tweety_bridge.validate_fol_belief_set(belief_set_str, signature_str)
+            
+            mock_remove_lines.assert_called_once_with(belief_set_str)
+            self.assertEqual(self.mock_fol_handler_instance.parse_fol_formula.call_count, len(cleaned_formulas))
+            for formula in cleaned_formulas:
+                self.mock_fol_handler_instance.parse_fol_formula.assert_any_call(formula, signature_str)
+            self.assertTrue(is_valid)
+            self.assertEqual(message, "Ensemble de croyances FOL valide")
+        else:
+            is_valid, message = self.tweety_bridge.validate_fol_belief_set("forall X: p(X).", "sort T; predicate p(T).")
+            self.assertTrue(is_valid, f"L'ensemble FOL devrait être valide. Message: {message}")
+            self.assertEqual(message, "Ensemble de croyances FOL valide")
+
+    def test_validate_fol_belief_set_empty(self):
+        """Test de la validation d'un ensemble de croyances FOL vide."""
+        expected_message = "Ensemble de croyances FOL vide ou ne contenant que des commentaires"
+        if not self.use_real_jpype:
+            with patch.object(self.tweety_bridge, '_remove_comments_and_empty_lines', return_value=[]) as mock_remove_lines:
+                is_valid, message = self.tweety_bridge.validate_fol_belief_set("", None)
+            mock_remove_lines.assert_called_once_with("")
+            self.mock_fol_handler_instance.parse_fol_formula.assert_not_called()
+            self.assertFalse(is_valid)
+            self.assertEqual(message, expected_message)
+        else:
+            is_valid, message = self.tweety_bridge.validate_fol_belief_set("% only comment", None)
+            self.assertFalse(is_valid)
+            self.assertEqual(message, expected_message)
+
+    def test_validate_fol_belief_set_invalid(self):
+        """Test de la validation d'un ensemble de croyances FOL invalide."""
+        belief_set_str = "forall X: p(X); forall Y: q(Y) &" # Dernière formule invalide
+        cleaned_formulas = ["forall X: p(X)", "forall Y: q(Y) &"]
+        signature_str = "sort T; predicate p(T); predicate q(T);"
+        error_detail = "Erreur sur q(Y) &"
+        if not self.use_real_jpype:
+            self.mock_fol_handler_instance.parse_fol_formula.side_effect = [MagicMock(), ValueError(error_detail)]
+            with patch.object(self.tweety_bridge, '_remove_comments_and_empty_lines', return_value=cleaned_formulas) as mock_remove_lines:
+                is_valid, message = self.tweety_bridge.validate_fol_belief_set(belief_set_str, signature_str)
+            mock_remove_lines.assert_called_once_with(belief_set_str)
+            self.assertEqual(self.mock_fol_handler_instance.parse_fol_formula.call_count, 2)
+            self.assertFalse(is_valid)
+            self.assertEqual(message, f"Erreur de syntaxe FOL: {error_detail}")
+        else:
+            is_valid, message = self.tweety_bridge.validate_fol_belief_set("forall X: p(X); forall Y: q(Y) &", "sort T; predicate p(T); predicate q(T).")
+            self.assertFalse(is_valid)
+            self.assertIn("syntaxe FOL", message)
+
+
+    # --- Tests pour Modal Belief Set ---
+    def test_validate_modal_belief_set_valid(self):
+        """Test de la validation d'un ensemble de croyances modales valide."""
+        belief_set_str = "[]p; <>q"
+        cleaned_formulas = ["[]p", "<>q"]
+        modal_logic = "S4"
+        if not self.use_real_jpype:
+            self.mock_modal_handler_instance.parse_modal_formula.return_value = MagicMock(name="ParsedModalFormulaMock")
+            with patch.object(self.tweety_bridge, '_remove_comments_and_empty_lines', return_value=cleaned_formulas) as mock_remove_lines:
+                is_valid, message = self.tweety_bridge.validate_modal_belief_set(belief_set_str, modal_logic)
+            mock_remove_lines.assert_called_once_with(belief_set_str)
+            self.assertEqual(self.mock_modal_handler_instance.parse_modal_formula.call_count, len(cleaned_formulas))
+            for formula in cleaned_formulas:
+                self.mock_modal_handler_instance.parse_modal_formula.assert_any_call(formula, modal_logic)
+            self.assertTrue(is_valid)
+            self.assertEqual(message, "Ensemble de croyances Modal valide")
+        else:
+            is_valid, message = self.tweety_bridge.validate_modal_belief_set("[]p.", modal_logic)
+            self.assertTrue(is_valid, f"L'ensemble Modal devrait être valide. Message: {message}")
+            self.assertEqual(message, "Ensemble de croyances Modal valide")
+
+    def test_validate_modal_belief_set_empty(self):
+        """Test de la validation d'un ensemble de croyances modales vide."""
+        expected_message = "Ensemble de croyances Modal vide ou ne contenant que des commentaires"
+        if not self.use_real_jpype:
+            with patch.object(self.tweety_bridge, '_remove_comments_and_empty_lines', return_value=[]) as mock_remove_lines:
+                is_valid, message = self.tweety_bridge.validate_modal_belief_set("", "S4")
+            mock_remove_lines.assert_called_once_with("")
+            self.mock_modal_handler_instance.parse_modal_formula.assert_not_called()
+            self.assertFalse(is_valid)
+            self.assertEqual(message, expected_message)
+        else:
+            is_valid, message = self.tweety_bridge.validate_modal_belief_set("% only comment", "S4")
+            self.assertFalse(is_valid)
+            self.assertEqual(message, expected_message)
+
+    def test_validate_modal_belief_set_invalid(self):
+        """Test de la validation d'un ensemble de croyances modales invalide."""
+        belief_set_str = "[]p; <>q &" # Dernière formule invalide
+        cleaned_formulas = ["[]p", "<>q &"]
+        modal_logic = "S4"
+        error_detail = "Erreur sur <>q &"
+        if not self.use_real_jpype:
+            self.mock_modal_handler_instance.parse_modal_formula.side_effect = [MagicMock(), ValueError(error_detail)]
+            with patch.object(self.tweety_bridge, '_remove_comments_and_empty_lines', return_value=cleaned_formulas) as mock_remove_lines:
+                is_valid, message = self.tweety_bridge.validate_modal_belief_set(belief_set_str, modal_logic)
+            mock_remove_lines.assert_called_once_with(belief_set_str)
+            self.assertEqual(self.mock_modal_handler_instance.parse_modal_formula.call_count, 2)
+            self.assertFalse(is_valid)
+            self.assertEqual(message, f"Erreur de syntaxe Modale: {error_detail}")
+        else:
+            is_valid, message = self.tweety_bridge.validate_modal_belief_set("[]p; <>q &", modal_logic)
+            self.assertFalse(is_valid)
+            self.assertIn("syntaxe Modale", message)
+
+    # --- Tests pour execute_fol_query ---
+    def test_execute_fol_query_accepted(self):
+        """Test de l'exécution d'une requête FOL acceptée."""
+        bs = "forall X: p(X)."
+        query = "p(a)."
+        sig = "sort T; constant a:T; predicate p(T)."
+        if not self.use_real_jpype:
+            self.mock_fol_handler_instance.fol_query.return_value = True
+            result = self.tweety_bridge.execute_fol_query(bs, query, sig)
+            self.mock_fol_handler_instance.fol_query.assert_called_once_with(bs, query, sig)
+            self.assertIn(f"FOL Query '{query}' is ACCEPTED (True)", result)
+        else:
+            # Ce test nécessite une vraie logique FOL et un moteur de raisonnement.
+            # Pour l'instant, on s'attend à ce que le handler placeholder retourne True.
+            # Ou on peut skipper si le handler n'est pas pleinement implémenté.
+            result = self.tweety_bridge.execute_fol_query(bs, query, sig)
+            self.assertIn("ACCEPTED (True)", result) # Ajuster si le vrai handler a un comportement différent
+
+    def test_execute_fol_query_rejected(self):
+        """Test de l'exécution d'une requête FOL rejetée."""
+        bs = "forall X: p(X)."
+        query = "q(a)." # q n'est pas dans le BS
+        sig = "sort T; constant a:T; predicate p(T); predicate q(T)."
+        if not self.use_real_jpype:
+            self.mock_fol_handler_instance.fol_query.return_value = False
+            result = self.tweety_bridge.execute_fol_query(bs, query, sig)
+            self.mock_fol_handler_instance.fol_query.assert_called_once_with(bs, query, sig)
+            self.assertIn(f"FOL Query '{query}' is REJECTED (False)", result)
+        else:
+            result = self.tweety_bridge.execute_fol_query(bs, query, sig)
+            self.assertIn("REJECTED (False)", result) # Ajuster si le vrai handler a un comportement différent
+
+    def test_execute_fol_query_unknown(self):
+        """Test de l'exécution d'une requête FOL avec résultat inconnu."""
+        bs = "exists X: p(X)."
+        query = "p(a)."
+        sig = "sort T; constant a:T; predicate p(T)."
+        if not self.use_real_jpype:
+            self.mock_fol_handler_instance.fol_query.return_value = None # Simule un résultat inconnu
+            result = self.tweety_bridge.execute_fol_query(bs, query, sig)
+            self.mock_fol_handler_instance.fol_query.assert_called_once_with(bs, query, sig)
+            self.assertIn(f"Unknown for FOL query '{query}'", result)
+        else:
+            # Le comportement "Unknown" dépend du vrai reasoner FOL.
+            # Pour l'instant, le handler placeholder ne retourne pas None.
+            self.skipTest("Le vrai FOLHandler ne retourne pas 'None' pour l'instant.")
+
+    def test_execute_fol_query_error(self):
+        """Test de l'exécution d'une requête FOL avec erreur."""
+        bs_invalid = "forall X: p(X) &." # Erreur de syntaxe
+        query = "p(a)."
+        sig = "sort T; constant a:T; predicate p(T)."
+        error_detail = "Erreur de parsing FOL dans le handler"
+        if not self.use_real_jpype:
+            self.mock_fol_handler_instance.fol_query.side_effect = ValueError(error_detail)
+            result = self.tweety_bridge.execute_fol_query(bs_invalid, query, sig)
+            self.mock_fol_handler_instance.fol_query.assert_called_once_with(bs_invalid, query, sig)
+            self.assertIn("FUNC_ERROR", result)
+            self.assertIn(f"Erreur lors de l'exécution de la requête FOL via FOLHandler: {error_detail}", result)
+        else:
+            result = self.tweety_bridge.execute_fol_query(bs_invalid, query, sig)
+            self.assertIn("FUNC_ERROR", result)
+            self.assertIn("syntaxe", result.lower())
+
+
+    # --- Tests pour execute_modal_query ---
+    def test_execute_modal_query_accepted(self):
+        """Test de l'exécution d'une requête modale acceptée."""
+        bs = "[]p"
+        query = "p"
+        logic = "S4"
+        if not self.use_real_jpype:
+            self.mock_modal_handler_instance.modal_query.return_value = True
+            result = self.tweety_bridge.execute_modal_query(bs, query, logic)
+            self.mock_modal_handler_instance.modal_query.assert_called_once_with(bs, query, logic, None) # None pour signature
+            self.assertIn(f"Modal Query '{query}' (Logic: {logic}) is ACCEPTED (True)", result)
+        else:
+            result = self.tweety_bridge.execute_modal_query(bs, query, logic)
+            self.assertIn("ACCEPTED (True)", result) # Ajuster si le vrai handler a un comportement différent
+
+    def test_execute_modal_query_rejected(self):
+        """Test de l'exécution d'une requête modale rejetée."""
+        bs = "[]p"
+        query = "<>q" # q n'est pas dans le BS
+        logic = "S4"
+        if not self.use_real_jpype:
+            self.mock_modal_handler_instance.modal_query.return_value = False
+            result = self.tweety_bridge.execute_modal_query(bs, query, logic)
+            self.mock_modal_handler_instance.modal_query.assert_called_once_with(bs, query, logic, None)
+            self.assertIn(f"Modal Query '{query}' (Logic: {logic}) is REJECTED (False)", result)
+        else:
+            result = self.tweety_bridge.execute_modal_query(bs, query, logic)
+            self.assertIn("REJECTED (False)", result) # Ajuster si le vrai handler a un comportement différent
+
+    def test_execute_modal_query_unknown(self):
+        """Test de l'exécution d'une requête modale avec résultat inconnu."""
+        bs = "<>p"
+        query = "p" # Peut être vrai ou faux selon le modèle
+        logic = "K"
+        if not self.use_real_jpype:
+            self.mock_modal_handler_instance.modal_query.return_value = None # Simule un résultat inconnu
+            result = self.tweety_bridge.execute_modal_query(bs, query, logic)
+            self.mock_modal_handler_instance.modal_query.assert_called_once_with(bs, query, logic, None)
+            self.assertIn(f"Unknown for Modal query '{query}'", result)
+        else:
+            # Le comportement "Unknown" dépend du vrai reasoner Modal.
+            # Le handler placeholder actuel retourne False.
+            self.skipTest("Le vrai ModalHandler ne retourne pas 'None' pour l'instant.")
+
+
+    def test_execute_modal_query_error(self):
+        """Test de l'exécution d'une requête modale avec erreur."""
+        bs_invalid = "[]p &." # Erreur de syntaxe
+        query = "p"
+        logic = "S4"
+        error_detail = "Erreur de parsing Modale dans le handler"
+        if not self.use_real_jpype:
+            self.mock_modal_handler_instance.modal_query.side_effect = ValueError(error_detail)
+            result = self.tweety_bridge.execute_modal_query(bs_invalid, query, logic)
+            self.mock_modal_handler_instance.modal_query.assert_called_once_with(bs_invalid, query, logic, None)
+            self.assertIn("FUNC_ERROR", result)
+            self.assertIn(f"Erreur lors de l'exécution de la requête Modale via ModalHandler: {error_detail}", result)
+        else:
+            result = self.tweety_bridge.execute_modal_query(bs_invalid, query, logic)
+            self.assertIn("FUNC_ERROR", result)
+            self.assertIn("syntaxe", result.lower())
 
-    status, result_msg = bridge.execute_pl_query("a => b", "a")
-    
-    assert bridge._pl_handler._pl_parser.parseFormula.call_count == 2
-    bridge._pl_handler._pl_reasoner.query.assert_called_once_with(ANY, mock_query_formula)
-    assert status is True
-    assert "ACCEPTED" in result_msg
-
-# --- Tests avec la vraie JVM ---
-
-@pytest.mark.skipif(not REAL_JPYPE, reason="Nécessite une JVM réelle.")
-@pytest.mark.asyncio
-async def test_validate_formula_real(tweety_bridge_real):
-    """Test de validation avec la vraie JVM."""
-    bridge = await tweety_bridge_real
-    
-    # Valide
-    is_valid, message = bridge.validate_formula("a => b")
-    assert is_valid
-    assert message == "Formule valide"
-
-    # Invalide
-    is_valid_inv, message_inv = bridge.validate_formula("a ==> b")
-    assert not is_valid_inv
-    assert "syntax" in message_inv.lower()
-
-@pytest.mark.skipif(not REAL_JPYPE, reason="Nécessite une JVM réelle.")
-@pytest.mark.asyncio
-async def test_execute_pl_query_real(tweety_bridge_real):
-    """Test d'exécution d'une requête PL avec la vraie JVM."""
-    bridge = await tweety_bridge_real
-
-    # Acceptée
-    status, result = bridge.execute_pl_query("a; a=>b", "b")
-    assert status == "ACCEPTED"
-    assert "ACCEPTED (True)" in result
-
-    # Rejetée
-    status_rej, result_rej = bridge.execute_pl_query("a; a=>b", "c")
-    assert status_rej == "REJECTED"
-    assert "REJECTED (False)" in result_rej
-
-    # Erreur
-    status_err, result_err = bridge.execute_pl_query("a ==>; b", "c")
-    assert status_err == "ERREUR"
-    assert "error" in result_err.lower() or "exception" in result_err.lower()
+
+if __name__ == "__main__":
+    unittest.main()

==================== COMMIT: 6864cb742715701e04234e200367960c577c526c ====================
commit 6864cb742715701e04234e200367960c577c526c
Author: jsboigeEpita <jean-sylvain.boige@epita.fr>
Date:   Mon Jun 16 01:48:48 2025 +0200

    refactor: Intégration du stash SK v1.32.2 après résolution conflits
    
    Ce commit intègre un stash lié à une mise à jour de Semantic Kernel. Les conflits ont été résolus en supprimant des fichiers de log et de statut temporaires (backend_info.json, webapp_orchestrator.log) et un script d'activation obsolète. Le script principal 'setup_project_env.ps1' a été fusionné et temporairement patché pour refléter ces suppressions.

diff --git a/setup_project_env.ps1 b/setup_project_env.ps1
index 5a22e6d4..263b5852 100644
--- a/setup_project_env.ps1
+++ b/setup_project_env.ps1
@@ -61,16 +61,26 @@ Write-Host "[INFO] Environnement cible: conda 'projet-is'" -ForegroundColor Cyan
 Write-Host "[INFO] [COMMANDE] $CommandToRun" -ForegroundColor Cyan
 
 # Raccourci vers le script de setup principal
-$realScriptPath = Join-Path $PSScriptRoot "activate_project_env.ps1"
-
-if (!(Test-Path $realScriptPath)) {
-    Write-Host "[ERREUR] Script d'activation non trouvé: $realScriptPath" -ForegroundColor Red
-    Write-Host "[INFO] Vérifiez l'intégrité du projet" -ForegroundColor Yellow
-    exit 1
+# $realScriptPath = Join-Path $PSScriptRoot "activate_project_env.ps1"
+#
+# if (!(Test-Path $realScriptPath)) {
+#     Write-Host "[ERREUR] Script d'activation non trouvé: $realScriptPath" -ForegroundColor Red
+#     Write-Host "[INFO] Vérifiez l'intégrité du projet" -ForegroundColor Yellow
+#     exit 1
+# }
+#
+# & $realScriptPath -CommandToRun $CommandToRun
+# $exitCode = $LASTEXITCODE
+Write-Host "[AVERTISSEMENT] Le mécanisme d'appel au script d'activation a été désactivé temporairement suite à un refactoring." -ForegroundColor Yellow
+Write-Host "[AVERTISSEMENT] Le script ne fait qu'exécuter la commande directement. Pour une activation complète, utilisez le terminal." -ForegroundColor Yellow
+$exitCode = 0 # Placeholder
+
+# Exécution directe de la commande pour maintenir une fonctionnalité minimale
+Invoke-Expression $CommandToRun
+if ($LASTEXITCODE -ne $null) {
+    $exitCode = $LASTEXITCODE
 }
 
-& $realScriptPath -CommandToRun $CommandToRun
-$exitCode = $LASTEXITCODE
 
 # Message final informatif
 Write-Host ""
@@ -81,6 +91,5 @@ if ($exitCode -eq 0) {
 } else {
     Write-Host "[ECHEC] Vérifiez l'environnement avec: .\setup_project_env.ps1 -Status" -ForegroundColor Red
 }
-# Write-Host "EXECUTION TERMINEE CHECKPOINT"
 
 exit $exitCode
\ No newline at end of file

